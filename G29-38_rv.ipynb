{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to do the radial velocity for the other white dwarfs\n",
    "\n",
    "#mike folder for G29-38 /data/wdplanetary/laura/MIKE/Data/G29-38/blue/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import numpy as np\n",
    "import astropy.io.fits as fits\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.modeling import models,fitting\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.stats import norm\n",
    "from scipy.special import voigt_profile\n",
    "from scipy import signal\n",
    "from scipy.signal import correlate\n",
    "from PyAstronomy import pyasl\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "from astropy.constants import c\n",
    "import astropy.io.fits as fits\n",
    "from astropy.units import dimensionless_unscaled\n",
    "from lmfit.models import LinearModel, GaussianModel, VoigtModel, PolynomialModel\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "import fnmatch\n",
    "import os\n",
    "from astropy.timeseries import LombScargle\n",
    "from plotnine import *\n",
    "import traceback\n",
    "import sys\n",
    "import linecache\n",
    "import textwrap\n",
    "import re\n",
    "import emcee\n",
    "import corner\n",
    "import tqdm\n",
    "\n",
    "\"\"\" \n",
    "plt.rcParams['axes.linewidth'] = 1.25\n",
    "plt.rcParams['xtick.major.size'] = 5\n",
    "plt.rcParams['xtick.major.width'] = 1.25\n",
    "plt.rcParams['ytick.major.size'] = 5\n",
    "plt.rcParams['ytick.major.width'] = 1.25\n",
    " \"\"\"\n",
    "default_settings = {\n",
    "    'font.size': 16,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'xtick.major.size': 3.5,\n",
    "    'xtick.major.width': 1,\n",
    "    'ytick.major.size': 3.5,\n",
    "    'ytick.major.width': 1\n",
    "}\n",
    "\n",
    "\n",
    "initial_settings = {\n",
    "    'font.size': 22,\n",
    "    'axes.linewidth': 1.25,\n",
    "    'xtick.major.size': 5,\n",
    "    'xtick.major.width': 1.25,\n",
    "    'ytick.major.size': 5,\n",
    "    'ytick.major.width': 1.25\n",
    "}\n",
    "plt.rcParams.update(initial_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Mike data function\n",
    "\n",
    "\n",
    "def read_mike_spec(filename):\n",
    "\n",
    "    hdulist = fits.open(filename)\n",
    "    obj_fits = hdulist[0]\n",
    "    header = obj_fits.header\n",
    "    date_obs = header.get('UT-DATE', None)\n",
    "    time_obs = header.get('UT-START',None)\n",
    "    \n",
    "    OBJECT = header.get(\"OBJECT\", None)\n",
    "   \n",
    "    sky_spec = obj_fits.data[0,:,:]\n",
    "    obj_spec = obj_fits.data[1,:,:]\n",
    "    noi_spec = obj_fits.data[2,:,:]\n",
    "    snr_spec = obj_fits.data[3,:,:]\n",
    "    lamp_spec = obj_fits.data[4,:,:]\n",
    "    flat_spec = obj_fits.data[5,:,:]\n",
    "    nobj_spec = obj_fits.data[6,:,:]\n",
    "    hdulist.close()\n",
    "\n",
    "    # Ca H and K are found in orders 18-22 which are indexed 17-21. Over these\n",
    "    # 4 order, the extracted Ca H and K can be combined to further reduce noise\n",
    "   \n",
    "    i = 0\n",
    "    found = -1\n",
    "   \n",
    "    while (found == -1) and (i < len(header)):\n",
    "        found = str(header[i]).find('spec1')\n",
    "        i += 1\n",
    "   \n",
    "    all_order_wav = str(header[(i-1)::])\n",
    "    all_order_wav = '\\''+all_order_wav\n",
    "   \n",
    "    #The following loop can be used to remove all the header keys from the string\n",
    "    i = 1\n",
    "    match = True\n",
    "    while match:\n",
    "        string = '\\'WAT2_%03d= \\'' % i\n",
    "        wat = re.compile(string)\n",
    "        match = wat.search(all_order_wav)\n",
    "        all_order_wav = wat.sub('',all_order_wav)\n",
    "        i +=1\n",
    "   \n",
    "    wavelength_soln_data = []\n",
    "   \n",
    "    #This loop parses the wavelength solns by the spec keyword and stores the entire\n",
    "    #solution as a list of lists, each entry containing one place for each solved\n",
    "    #parameter.\n",
    "    i = 1\n",
    "    match = True\n",
    "    while match:\n",
    "        string = 'spec%d' %i\n",
    "        spec_key = re.compile(string)\n",
    "        match = spec_key.search(all_order_wav)\n",
    "        if match:\n",
    "            wavelength_soln_data.append((all_order_wav[(match.start()+10):match.start()+91]).split())\n",
    "        i+=1\n",
    "   \n",
    "    #This loop generates an array of arrays with each array containing the wavelength\n",
    "    #soln.\n",
    "    wav_solns = []\n",
    "    for i in range(len(wavelength_soln_data)):\n",
    "        wav_start = float(wavelength_soln_data[i][3])\n",
    "        wav_delta = float(wavelength_soln_data[i][4])\n",
    "        wav_end = wav_start + wav_delta*float(wavelength_soln_data[i][5])\n",
    "        wav_solns.append(np.arange(wav_start,wav_end,wav_delta))\n",
    "        i+=1\n",
    "    #    \n",
    "    #    plt.figure(1)\n",
    "    #    plt.clf()\n",
    "    #    for i in range(len(wav_solns)):\n",
    "    #        plt.plot(wav_solns[i],nobj_spec[i,:])\n",
    "       \n",
    "    all_data = np.zeros([len(wav_solns),9],dtype=object)\n",
    "    for i in range(len(wav_solns)):\n",
    "        all_data[i,0] = i+1\n",
    "        all_data[i,1] = wav_solns[i]\n",
    "        all_data[i,2] = sky_spec[i,:]\n",
    "        all_data[i,3] = obj_spec[i,:]\n",
    "        all_data[i,4] = noi_spec[i,:]\n",
    "        all_data[i,5] = snr_spec[i,:]\n",
    "        all_data[i,6] = lamp_spec[i,:]\n",
    "        all_data[i,7] = flat_spec[i,:]\n",
    "        all_data[i,8] = nobj_spec[i,:]\n",
    "    keywords = ['Order','Wavelength Soln', 'Sky Spectrum', 'Object Spectrum',\n",
    "                'Noise Spectrum', 'SNR Spectrum', 'Lamp Spectrum', 'Flat Spectrum',\n",
    "                'Normalized Object Spectrum']\n",
    "\n",
    "\n",
    "    #First, calculating the heliocentric correction\n",
    "\n",
    "    site_lat = header.get(\"SITELAT\",None)\n",
    "    site_long = header.get(\"SITELONG\",None)\n",
    "    site_alt = header.get(\"SITEALT\",None)\n",
    "    ra = header.get(\"RA-D\",None)  # in degrees\n",
    "    dec = header.get(\"DEC-D\", None)  # in degrees\n",
    "    epoch = str(header.get(\"EPOCH\", None))  # e.g., 'J2000'\n",
    "    time = Time(str(date_obs) + 'T' + str(time_obs), format='fits',scale = \"utc\")\n",
    "    \n",
    "    #print(time)\n",
    "    # Create EarthLocation object\n",
    "    location = EarthLocation.from_geodetic(lat=site_lat*u.deg, lon=site_long*u.deg, height=site_alt*u.m)\n",
    "    sc = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)\n",
    "    heliocorr = sc.radial_velocity_correction('heliocentric', obstime=time, location=location) \n",
    "    #print(heliocorr.to(u.km/u.s)) \n",
    "    heliocorr = (heliocorr.to(u.m/u.s) /c)\n",
    "\n",
    "\n",
    "    \n",
    "    #Now making easy arrays to use:\n",
    "    wav = []\n",
    "    order= []\n",
    "    flux = []\n",
    "    snr = []\n",
    "    for i in range(len(all_data)):\n",
    "        wav.append(all_data[i,1])\n",
    "        order.append(all_data[i,0])\n",
    "        flux.append(all_data[i,8])\n",
    "        snr.append(all_data[i,5])\n",
    "\n",
    "\n",
    "    #Now apply the heliocentric correction to the wavelength data:\n",
    "    for i in range(len(wav)):\n",
    "        for jk in range(len(wav[i])):\n",
    "            wav[i][jk] *= (1 + heliocorr)\n",
    "\n",
    "        #heliocentric correction\n",
    "     #   wavelength[i] = (CRVAL1 + CDELT1*i) + (HEL_COR/299792 )*(CRVAL1 + CDELT1*i)\n",
    "        #wavelength[i] = (CRVAL1 + CDELT1*i)\n",
    "    print(OBJECT)\n",
    "    return order,wav,flux,snr, OBJECT,time\n",
    "\n",
    "\n",
    "\n",
    "#filename = \"/data/wdplanetary/laura/MIKE/Data/WD1929+011/blue/galex1931_blue_2011-06-09.fits\"\n",
    "\"\"\" order,wav,flux,snr,OBJECT,time = read_mike_spec(filename)\n",
    "long_string = str(header)\n",
    "wrapped_text = textwrap.wrap(long_string, width=80)\n",
    "for line in wrapped_text:\n",
    "    print(line)\n",
    "plt.figure()\n",
    "plt.plot(wav[8],flux[8])\n",
    "plt.xlim(4475,4485)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wav[8],snr[8])\n",
    "plt.xlim(4475,4485)\n",
    "plt.show() \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line dictionaries: Name, position(A), present?, log gf\n",
    "p_lines = []\n",
    "spec_lines = []\n",
    "\n",
    "#pollutant lines\n",
    "p_lines.append((\"CaII_3933\", 3933.663, True)) #only use for MIKE data\n",
    "p_lines.append((\"CaII_4226\",4226.727, False)) #good\n",
    "p_lines.append((\"FeII_4923\",4923.927, False)) #can't use, crosses over with interorder\n",
    "p_lines.append((\"FeII_5018\",5018.440, False))#not there\n",
    "p_lines.append((\"FeII_5169\",5169.033, False)) #this one gives an error when trying to fit it\n",
    "p_lines.append((\"SiII_5041\",5041.024, False)) #don't use this\n",
    "p_lines.append((\"SiII_5055\",5055.984, False)) #This one is good use it\n",
    "p_lines.append((\"SiII_5957\",5957.560, False))\n",
    "p_lines.append((\"SiII_5978\",5978.930, False))\n",
    "p_lines.append((\"SiII_6347\",6347.100, False)) #these two are quite strong in WD1929+012\n",
    "p_lines.append((\"SiII_6371\",6371.360, False)) #\n",
    "p_lines.append((\"MgI_5172\",5172.683, False))\n",
    "p_lines.append((\"MgI_5183\",5183.602, False))\n",
    "p_lines.append((\"MgII_4481\",4481.130,False)) #strong magnesium line\n",
    "p_lines.append((\"MgII_4481_2\",4481.327, False))\n",
    "p_lines.append((\"MgII_4481\",4481.180,False)) #weighted combination of mg_4481 lines\n",
    "\n",
    "p_lines.append((\"MgII_7877\",7877.054, False)) \n",
    "p_lines.append((\"MgII_7896\",7896.366, False)) #definitely not present\n",
    "p_lines.append((\"OI_7771\",7771.944, False)) #definitely not present\n",
    "#hydrogen lines\n",
    "p_lines.append((\"H_4860\",4860.680, False))\n",
    "p_lines.append((\"H_4860_2\",4860.968, False))#This one gives better values\n",
    "p_lines.append((\"H_4860_2\",4860.968, False))#Weighted combo\n",
    "p_lines.append((\"H_4340\",4340.472,False)) #present\n",
    "p_lines.append((\"H_6563\",6562.79 ,False)) \n",
    "#pick the spectral lines present in this white dwarf\n",
    "\n",
    "for i in p_lines:\n",
    "    if i[2] == True:\n",
    "        spec_lines.append(i)\n",
    "\n",
    "b_lines = []\n",
    "r_lines = []\n",
    "for i in spec_lines:\n",
    "    if i[1] <= 5550:\n",
    "        #370 - 555 nm\n",
    "        b_lines.append(i)\n",
    "    else:\n",
    "        #555 - 890 nm\n",
    "        r_lines.append(i)\n",
    "\n",
    "#Now define the sky lines that we will use to find the stability corrections from the instrument variability\n",
    "\n",
    "sky_lines = []\n",
    "sky_lines.append((\"OI_5577\",5577.340))\n",
    "sky_lines.append((\"OI_6300\",6300.304))\n",
    "sky_lines.append((\"OI_6364\",6363.776))\n",
    "#sky_lines.append((\"H2O_7392\"))\n",
    "#sky_lines.append((\"H2O_8365\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_mike_gaussian(wav,flux,c_lines):\n",
    "    #make a new wavelength filter that is accurate to 3dp\n",
    "    min = wav[np.argmin(wav)]\n",
    "    max = wav[np.argmax(wav)]\n",
    "    xwav = np.arange(min,max,0.001)\n",
    "\n",
    "    minlen = np.minimum(len(wav),len(flux))\n",
    "\n",
    "    # Adjust the size of the flux data to fit the mask\n",
    "    interp_func = interp1d(wav[:minlen], flux[:minlen], bounds_error=False, fill_value=np.nan)\n",
    "    padded_flux = interp_func(xwav)\n",
    "\n",
    "    #movingavg\n",
    "    window_size = 3000\n",
    "    n_moving_avg = uniform_filter1d(padded_flux, size=window_size)\n",
    "    return xwav, padded_flux, n_moving_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(data):\n",
    "    window_size = int(500)\n",
    "    # Use a rolling window approach for efficient calculation of standard deviation\n",
    "    #least mean squared method\n",
    "    rolling_std = np.sqrt(np.convolve(data**2, np.ones(window_size) / window_size, mode='valid') - np.convolve(data, np.ones(window_size) / window_size, mode='valid')**2)\n",
    "    \n",
    "    # Extend the result to match the original length of the data\n",
    "    pad_width = window_size // 2\n",
    "    errors = np.pad(rolling_std, (pad_width, pad_width), mode='edge')\n",
    "    #print(len(errors),len(data))\n",
    "    errorsv = np.nan_to_num(errors)\n",
    "    #print(np.mean(errorsv))\n",
    "    return errorsv[:len(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(gwav,gdata):\n",
    "    poly_model = PolynomialModel(degree=7)\n",
    "    params = poly_model.guess(gdata, x=gwav)\n",
    "    poly_result = poly_model.fit(gdata, params, x=gwav)\n",
    "    print(poly_result.params['c0'],poly_result.params['c1'],poly_result.params['c2'],poly_result.params['c3'],poly_result.params['c4'])\n",
    "    \n",
    "    #find the individual polynomial best fit\n",
    "    c0 = poly_result.params['c0'].value\n",
    "    c1 = poly_result.params['c1'].value\n",
    "    c2 = poly_result.params['c2'].value\n",
    "    c3 = poly_result.params[\"c3\"].value\n",
    "    c4 = poly_result.params[\"c4\"].value\n",
    "    c5 = poly_result.params[\"c5\"].value\n",
    "    c6 = poly_result.params[\"c6\"].value\n",
    "    c7 = poly_result.params[\"c7\"].value\n",
    "    \n",
    "    p_result = c0 + c1 * gwav + c2 * gwav**2 + c3 * gwav**3 + c4 * gwav**4 + c5 * gwav**5 + c6 * gwav**6 + c7 * gwav**7\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(gwav, p_result, color='green', linewidth=1)\n",
    "    plt.plot(gwav, gdata, color='blue', linewidth=0.5)\n",
    "    \n",
    "    plt.show()\n",
    "    return p_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pick the files that we want\n",
    "\n",
    "def pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "    \"\"\"\n",
    "    Pick files in a folder based on given start and end patterns in their filename.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files.\n",
    "    - start_patterns (list): List of patterns to match at the start of filenames.\n",
    "    - end_patterns (list): List of patterns to match at the end of filenames.\n",
    "\n",
    "    Returns:\n",
    "    - List of filenames matching the specified start and end patterns.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        fp = os.path.join(folder_path, folder_name,\"product/\")\n",
    "\n",
    "        # List all files in the folder\n",
    "        try:\n",
    "            all_files = os.listdir(fp)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "\n",
    "        # Filter files based on start patterns\n",
    "        #for start_pattern in start_patterns:\n",
    "        #   matching_files.extend(fnmatch.filter(all_files, start_pattern + '*'))\n",
    "\n",
    "        # Filter files based on end patterns\n",
    "        #for end_pattern in end_patterns:\n",
    "         #   matching_files.extend(fnmatch.filter(all_files, '*' + end_pattern))\n",
    "        for file in all_files:\n",
    "            if file.startswith(start_patterns) and file.endswith(end_patterns):\n",
    "                matching_files.append(os.path.join(fp, file))\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "\n",
    "def mike_pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "    matching_files = []\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "    try:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            matching_files.append(os.path.join(folder_path, file_name))\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the folder doesn't exist\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Return the list of matching files\n",
    "    return matching_files\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#This is where we pick the folder path and the name of the file that we want\n",
    "#Need to adjust this so that it is more clear\n",
    "star = \"G29-38\"\n",
    "MIKE_blue_folder_path = \"/data/wdplanetary/laura/MIKE/Data/G29-38/blue/\"\n",
    "MIKE_red_folder_path = \"/data/wdplanetary/laura/MIKE/Data/G29-38/red/\"\n",
    "\n",
    "mike_start = (\"gal\")\n",
    "mike_end = (\".fits\")\n",
    "\n",
    "#Creating file directory for the blue and red channels separately\n",
    "\n",
    "mike_b_files = mike_pick_files_by_patterns(MIKE_blue_folder_path, mike_start,mike_end)\n",
    "mike_r_files = mike_pick_files_by_patterns(MIKE_red_folder_path, mike_start,mike_end)\n",
    "\n",
    "mike_b_files.sort()\n",
    "mike_r_files.sort()\n",
    "\n",
    "print(mike_b_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voigt fitting for the MIKE Spectrum\n",
    " \n",
    "def Mike_Gaussian(wav_list,flux_list,moving_avg_list,errors_list,snr_list,c_lines,time,n,plot_dir,OBJECT):\n",
    "    \n",
    "    v_precision = 0. #kneeed to find the stability for MIKE\n",
    "    w_size = 30\n",
    "    snr_cutoff = 13\n",
    "    line_depth_cutoff= 3\n",
    "    #initialise result arrays\n",
    "    RV = np.array([])\n",
    "    RV_weight = np.array([])\n",
    "    RV_err = np.array([])\n",
    "\n",
    "    #initialise the arrays for plotting\n",
    "    D = np.array([])\n",
    "    SNR = np.array([])\n",
    "\n",
    "    \n",
    "    for i in c_lines:\n",
    "        print(i)\n",
    "        try: \n",
    "            for j, wav_value in enumerate(wav_list):\n",
    "                \"\"\" \n",
    "                plt.figure()\n",
    "                plt.plot(wav_value,flux_list[j])\n",
    "                plt.show()\n",
    "                \"\"\"\n",
    "                if float(np.min(wav_value)) <= float(i[1]) <= float(np.max(wav_value)):\n",
    "                    print(f\" For order {j} the line is in the order\")\n",
    "                    xwav = wav_value\n",
    "                    xflux = flux_list[j]\n",
    "                    xmoving_avg = moving_avg_list[j]\n",
    "                    xerrors = errors_list[j]\n",
    "                    #xsnr = snr_list[j]\n",
    "\n",
    "                    #change plot directory\n",
    "                    wd = os.path.join(plot_dir, str(i[0])+\"/\")\n",
    "                    # Create the directory if it doesn't exist\n",
    "                    if not os.path.exists(wd):\n",
    "                        os.makedirs(wd)\n",
    "\n",
    "                    \n",
    "                    #gwav,gdata,gerrors,gweights,D,snr = make_window(wav,flux,moving_avg,i,w_size):\n",
    "\n",
    "                    #find the upper bound of the window\n",
    "                    u_loc = np.searchsorted(xwav, (i[1]+(w_size/2)))\n",
    "                    closest_value = xwav[max(0, u_loc-1)]\n",
    "                    u_bound = np.where(xwav == closest_value)\n",
    "                    u_bound = int(u_bound[0])\n",
    "                    #find the lower bound of the window\n",
    "                    l_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "                    closest_value = xwav[max(0, l_loc-1)]\n",
    "                    l_bound = np.where(xwav == closest_value)\n",
    "                    l_bound = int(l_bound[0])\n",
    "                    #make small datasets around the line\n",
    "                    #if l_bound >= 0 and u_bound <= len(flux):\n",
    "                        # Slice the array using integer indices\n",
    "    \n",
    "                    gdata = xflux[l_bound:u_bound]\n",
    "                    gwav = xwav[l_bound:u_bound]\n",
    "                    gavg = xmoving_avg[l_bound:u_bound]\n",
    "                    gavg = np.where(gavg != 0, gavg, 1)\n",
    "                    #gsnr = xsnr[l_bound:u_bound]\n",
    "                    \"\"\" else:\n",
    "                        raise Exception(\"The window falls outside the limit of the order\") \"\"\"\n",
    "                    #make a new window for errors which is just outside the window - basically the next window down instead\n",
    "                    #Instead we want to make a small window around the absorption line \n",
    "                    #and then take the errors from the points around it \n",
    "                    #and then take the average value of that \n",
    "                    #and set the errors of the line to it\n",
    "                    el_loc = np.searchsorted(xwav, (i[1]-(1/4)*(w_size)))\n",
    "                    closest_value = xwav[max(0, el_loc-1)]\n",
    "                    el_bound = np.where(xwav == closest_value)\n",
    "                    el_bound = int(el_bound[0])\n",
    "                    eh_loc = np.searchsorted(xwav, (i[1]+(1/4)*(w_size)))\n",
    "                    closest_value = xwav[max(0, eh_loc-1)]\n",
    "                    eh_bound = np.where(xwav == closest_value)\n",
    "                    eh_bound = int(eh_bound[0])\n",
    "\n",
    "                    firsterrorbox = (xerrors[l_bound:el_bound])\n",
    "                    seconderrorbox = (xerrors[eh_bound:u_bound])\n",
    "                    err_avg = (np.mean(firsterrorbox) + np.mean(seconderrorbox) )/2\n",
    "                    lineerroravg= []*(eh_bound-el_bound)\n",
    "                    lineerroravg[el_bound:eh_bound] = [err_avg] * (eh_bound - el_bound)\n",
    "\n",
    "                    gerrors = np.concatenate((firsterrorbox, lineerroravg, seconderrorbox))\n",
    "                    #gerrors[np.isnan(gerrors)] = 0.001\n",
    "                    gerrors[:len(gdata)]\n",
    "                    \n",
    "                    gweights = np.where(gerrors != 0, 1 / gerrors, 0.001)\n",
    "                    \n",
    "                    \n",
    "                    # Calculate the mean signal\n",
    "                    signal_mean = ((np.mean(xflux[l_bound:el_bound]) + np.mean(xflux[eh_bound:u_bound])) / 2) - np.min(xflux[el_bound:eh_bound])\n",
    "                    # Calculate the signal-to-noise ratio (SNR)\n",
    "                    depth = np.abs(signal_mean / err_avg)\n",
    "                    D = np.append(D,depth)\n",
    "\n",
    "                    #now calculate snr\n",
    "                    snr = np.median(xflux[eh_bound:u_bound])/(np.std(xflux[eh_bound:u_bound]))\n",
    "                    \n",
    "\n",
    "                    #instead do this with the snr dataset - take mean of snr in bounds\n",
    "                    #snr = np.mean(gsnr)\n",
    "\n",
    "                    SNR = np.append(SNR,snr)\n",
    "\n",
    "                    #set the initial guess of the mean value of the gaussian to the wavelength in air\n",
    "                    line = i[1]\n",
    "                    \n",
    "                \n",
    "                    #p_result = poly_fit(xwav,gdata)\n",
    "                    poly_model = PolynomialModel(degree=5)\n",
    "                    params = poly_model.guess(gdata, x=gwav)\n",
    "                    poly_result = poly_model.fit(gdata, params, x=gwav, weights = gweights )\n",
    "                    print(poly_result.params['c0'],poly_result.params['c1'],poly_result.params['c2'],poly_result.params['c3'],poly_result.params['c4'])\n",
    "                    \n",
    "                    #find the individual polynomial best fit\n",
    "                    c0 = poly_result.params['c0'].value\n",
    "                    c1 = poly_result.params['c1'].value\n",
    "                    c2 = poly_result.params['c2'].value\n",
    "                    c3 = poly_result.params[\"c3\"].value\n",
    "                    c4 = poly_result.params[\"c4\"].value\n",
    "                    c5 = poly_result.params[\"c5\"].value\n",
    "                    p_result = c0 + c1 * gwav + c2 * gwav**2 + c3 * gwav**3 + c4 * gwav**4 + c5 * gwav**5\n",
    " \n",
    "                    \"\"\"     plt.figure()\n",
    "                    plt.plot(gwav, p_result, color='green', linewidth=1)\n",
    "                    plt.plot(gwav, gdata, color='blue', linewidth=0.5)\n",
    "                    \n",
    "                    #plt.show()\n",
    "                    plt.close()\n",
    "                    \"\"\"\n",
    "\n",
    "                    n_flux = gdata/p_result\n",
    "                    n_errors = gerrors/p_result\n",
    "                    n_weights = np.where(n_errors != 0, 1 / n_errors, 0.001)\n",
    "                    #CHANGE THE SNR VALUE TO FILTER DATASETS THAT ARE NOT FITTING WELL\n",
    "                    #We want this code to try the next line in the dataset \n",
    "                    if snr <= snr_cutoff:\n",
    "                        raise Exception(f\"SNR too low: SNR {snr} <{snr_cutoff}\")\n",
    "                    if depth <= line_depth_cutoff:\n",
    "                        raise Exception(f\"Line not significant: Line depth {depth} < {line_depth_cutoff}\")\n",
    "                    ####\n",
    "                    \n",
    "                    print(f\"SNR is {snr:.3g}\")\n",
    "\n",
    "\n",
    "                    voigt_model = VoigtModel(prefix='voigt_')\n",
    "                    linear_model = LinearModel(prefix='linear_')\n",
    "                    composite_model = voigt_model + linear_model\n",
    "                    #When these parameters fit right for the Ca 3xxx fit the amp is -200, the sigma and gamma are 0.03\n",
    "                    params = voigt_model.make_params(voigt_amplitude=-0.6, voigt_center=(line + 0.6), voigt_sigma=0.03, voight_gamma = 0.03)\n",
    "                    params += linear_model.make_params(slope=0, intercept=np.median(n_flux))\n",
    "                    params['voigt_amplitude'].min = -1\n",
    "                    params['voigt_amplitude'].max = -0.02\n",
    "                    params['voigt_center'].min = (line - 2)\n",
    "                    params['voigt_center'].max = (line + 2) \n",
    "                    #params['voigt_sigma'].max = 0.05\n",
    "                    print(\"absorption params made\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    result = composite_model.fit(n_flux, params, x=gwav, weights = n_weights )#,sigma = wav_errors)\n",
    "                    print(result.values['voigt_center'],result.params['voigt_center'],result.params[\"voigt_amplitude\"])\n",
    "                    #print(\"The results of the fitting report are: \")\n",
    "                    #print(result.fit_report())\n",
    "                    \n",
    "                   \n",
    "                    t = time.datetime\n",
    "                    #plt.show()\n",
    "                    #print(result.values['voigt_center'])\n",
    "                    #print(result.fit_report())\n",
    "                    Line_Offset = result.values['voigt_center'] - line\n",
    "                    rv = (Line_Offset/line) * 299792\n",
    "                    #print(f\"The voigt sigma is {(result.params['voigt_sigma'])} and the center error is{ result.params['voigt_center'].stderr}\")\n",
    "                    if result.params['voigt_center'].stderr is not None:\n",
    "                        err = (((result.params['voigt_center'].stderr * 3 )/result.values['voigt_center']) * 299792)\n",
    "                    \n",
    "                    else:\n",
    "                        err = 0.2\n",
    "                    #err = (((result.params['voigt_sigma'].value)/result.values['voigt_center']) * 299792)\n",
    "\n",
    "                    #Append values to RV\n",
    "                    #if 35 < rv < 40:\n",
    "                    RV = np.append(RV,rv)\n",
    "                    RV_err = np.append(RV_err,(err)) #+v_precision\n",
    "                    RV_weight = np.append(RV_weight,(1/(err)))\n",
    "                    #else:\n",
    "                    #    raise Exception(\"The rv shift is not in the bounds\")\n",
    "\n",
    "                    rv_significant_figures = max(3, -int(np.floor(np.log10(err))) + 2)\n",
    "\n",
    "                    # Format the radial velocity and its error using the determined significant figures\n",
    "                    rv_formatted = f\"{rv:.{4}g}\"\n",
    "                    err_formatted = f\"{err:.{2}g}\"\n",
    "                    \n",
    "                    #Plotting results\n",
    "                    if c_lines is not sky_lines:\n",
    "\n",
    "                        #Add a simple plot to the stacked plot\n",
    "                        \"\"\" \n",
    "                        ax.plot(gwav, gdata/p_result  + n/3 , color='black', linewidth=0.5)\n",
    "                        ax.plot(gwav, result.best_fit/p_result  + n/3, color='red')\n",
    "                        ax.set_xlim(line-10,line+10)\n",
    "                        x_text = gwav[-1] + 0.2  # Add an offset for spacing\n",
    "                        ax.text(line+11, (1 + n/3), f\"SNR:{snr:.3g}\" , verticalalignment='center')\n",
    "                        n = n+1\n",
    "                        \"\"\"\n",
    "                        print(\"stacked plot made\")\n",
    "                        \n",
    "                        \n",
    "                        #We can add a label next to the line by using \n",
    "                             \n",
    "                        #Now save the line plot with the best fit and the shaded errors \n",
    "                        plt.figure(facecolor='white',figsize = (10,8))\n",
    "                        #add spectrum\n",
    "                        plt.plot(gwav, n_flux, color='black', linewidth=0.5)\n",
    "                        #add best fit line\n",
    "                        plt.plot(gwav, result.best_fit, color='red')\n",
    "                        #shade standard deviation of data\n",
    "                        plt.fill_between(gwav, (result.best_fit - n_errors), (result.best_fit + n_errors), color='gray', alpha=0.3)\n",
    "                        \n",
    "                        plt.title(f\"{t.year}/{t.month}/{t.day}, Order: {j}, Object: {OBJECT}\")\n",
    "                        plt.text(0.95, 0.10, f\"SNR = {snr:.3g}\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                        plt.text(0.95, 0.05, f\" RV = {rv:.{4}g} ± {err:.{2}g} km/s\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                        plt.ylim(0.5,1.3)\n",
    "                        plt.xlim(line-6,line+6)\n",
    "                        plt.xlabel(\"Wavelength (Å)\")\n",
    "                        plt.ylabel(\"Normalized Flux\")\n",
    "                        title = str(i[0]) + \"_\" + str(t) + '.pdf'\n",
    "                        title_without_spaces = title.replace(\" \", \"\")\n",
    "                        plt.savefig(os.path.join(wd, title_without_spaces))\n",
    "                        plt.show()\n",
    "                        plt.close() \n",
    "\n",
    "                        #save the datafiles instead of stacking the plot here\n",
    "                        data = pd.DataFrame({\"Wavelength\":(gwav),\"Normalized Data\": (n_flux), \"Voigt fit\": (result.best_fit), \"Time\": t, \"SNR\": snr, \"Depth\": depth, \"RV\": rv, \"Error\": err, \"Order\": j}) #[t],[snr], [depth], [rv], [err]])\n",
    "                        dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/G29-38/MIKE_Voigt_fitting/all_lines/{t}/\"\n",
    "                        dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "                        os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "                        file_end = f\"order{j}_{str(line)}.txt\"\n",
    "                        file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "                        file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "                        #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "                        data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "                        \n",
    "                    else:  \n",
    "                        raise Exception(\"Line not in this order\") \n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            \n",
    "            RV = np.append(RV,np.nan)\n",
    "            RV_err = np.append(RV_err,np.nan)\n",
    "            RV_weight = np.append(RV_weight,np.nan)\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    #weighted mean and standard deviation error calculations\n",
    "    if np.any(~np.isnan(RV)) :\n",
    "         #Weighted mean and standard deviation error calculations\n",
    "        mean = (np.sum((RV[~np.isnan(RV)] * RV_weight[~np.isnan(RV_weight)])) / (np.sum(RV_weight[~np.isnan(RV_weight)])))\n",
    "        mean_error = np.sqrt(np.sum(RV_weight[~np.isnan(RV_weight)] * RV_err[~np.isnan(RV_err)]**2) / np.sum(RV_weight[~np.isnan(RV_weight)])+ v_precision**2) \n",
    "\n",
    "    else:\n",
    "        mean = np.nan\n",
    "        mean_error = np.nan\n",
    "    \n",
    "    print(\"The rv mean and error is\", mean,mean_error)\n",
    "\n",
    "        \n",
    "    return mean, mean_error,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voigt fitting runfile for MIKE\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "\n",
    "n = 0\n",
    "star = \"\"\n",
    "\n",
    "mike_blue_plot_dir = \"/data/wdplanetary/omri/Output/G29-38/MIKE/Voigtfits\"\n",
    "#fig, ax = plt.subplots(figsize=(6, 20))\n",
    "\n",
    "\n",
    "for j in (mike_b_files):\n",
    "    order,wav,flux,snr,OBJECT,time =read_mike_spec(j)\n",
    "    print(\"------------NEW--FILE----------\", str(time))\n",
    "    xwav_list = np.empty(len(order), dtype=object)\n",
    "    padded_flux_list = np.empty(len(order), dtype=object)\n",
    "    moving_avg_list = np.empty(len(order), dtype=object)\n",
    "    errors_list = np.empty(len(order), dtype=object)\n",
    "    snr_list = np.empty(len(order), dtype=object)\n",
    "\n",
    "    print(\"imported files\")\n",
    "    #if OBJECT.startswith(star):\n",
    "        #then call data processing\n",
    "    for o in order:\n",
    "        xwav,padded_flux,moving_avg = process_data_mike_gaussian(wav[o-1],flux[o-1],b_lines)\n",
    "        errors = calculate_error(padded_flux)  \n",
    "\n",
    "        xwav_list[o-1] = xwav\n",
    "        padded_flux_list[o-1] = padded_flux\n",
    "        moving_avg_list[o-1] = moving_avg\n",
    "        errors_list[o-1] = errors\n",
    "        snr_list[o-1] = snr[o-1]\n",
    "    \n",
    "    #print(xwav_list)\n",
    "    #print(snr)\n",
    "\n",
    "    print(\"Smoothed flux and calculated errors\") \n",
    "    #print(errors,padded_flux,moving_avg)      \n",
    "    #then do Gaussian\n",
    "    rv,rv_err,n = Mike_Gaussian(xwav_list,padded_flux_list,moving_avg_list,errors_list,snr_list,b_lines,time,n,mike_blue_plot_dir,OBJECT)\n",
    "    #now need to add to directories of wavelengths, flux_results, best_fits,snrs, line depths\n",
    "    \n",
    "    #Then find the atmospheric correction- don't need to do this as we have the stabilities\n",
    "    #rv_corr,corr_err,ax = Gaussian(xskywav,sky_padded_flux,sky_moving_avg,sky_errors,sky_lines,skytime,n,ax)\n",
    "    if np.isnan(rv) : #or np.isnan(rv_corr):\n",
    "        print(\"Fit could not be calculated\")\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        #rv_corrs.append((rv_corr))\n",
    "        rvs.append((rv)) #-rv_corr\n",
    "        rverrs.append((rv_err )) \n",
    "        time_strings.append((time))\n",
    "        print(\"The fit has been calculated successfully\")\n",
    "    #else:\n",
    "    #    print(\"This is a file for \" + str(OBJECT) + \" instead of \" + str(star))\n",
    "    #    continue\n",
    "\n",
    "print(np.nanmean(rvs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining data points that are the same times\n",
    "t_list = Time(time_strings, scale='utc')\n",
    "times = t_list.datetime \n",
    "t0 = times[0]\n",
    "tdays = [(dt - t0).days for dt in times]\n",
    "mean = np.nanmean(rvs)\n",
    "print(mean)\n",
    "\n",
    "\n",
    "tarray = np.array(tdays)\n",
    "rvarray = np.array(rvs)\n",
    "rverrsarray = np.array(rverrs)\n",
    "#rvcorrsarray = np.array(rv_corrs)\n",
    "#rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "filtered_indices = np.where((rverrsarray < 20) & ( rvarray > 30 ) & ( rvarray < 45 ))[0] #& (rvarray > 10)\n",
    "filtered_rvarray = rvarray[filtered_indices]\n",
    "filtered_tarray = tarray[filtered_indices]\n",
    "filtered_rverrsarray = rverrsarray[filtered_indices]\n",
    "#filtered_rvcorrsarray = rvcorrsarray[filtered_indices]\n",
    "\n",
    "filtered_mean = np.nanmean(filtered_rvarray)\n",
    "print(filtered_mean)\n",
    "unique_times = np.unique(filtered_tarray)\n",
    "averages_unfiltered = np.array([tarray, rvarray, rverrsarray])\n",
    "\n",
    "averages = np.array([[t, np.nanmean(filtered_rvarray[filtered_tarray == t]), np.nanmean(filtered_rverrsarray[filtered_tarray == t])] for t in unique_times])\n",
    "#averages = averages[np.isnan(averages[:,1])]\n",
    "\n",
    "print(averages)#\n",
    "\n",
    "np.savetxt(\"/data/wdplanetary/omri/Output/resultfiles/G29-38/MIKE_Voigt_Results/ca3933.txt\",averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/data/wdplanetary/omri/Output/resultfiles/G29-38/MIKE_Voigt_Results/ca3933.txt\"\n",
    "times = []\n",
    "delta_rvs = []\n",
    "errors = []\n",
    "frequencies = []\n",
    "powers = []\n",
    "\n",
    "data = np.loadtxt(file_name, delimiter = \" \")\n",
    "print(data)\n",
    "\n",
    "\n",
    "t = data[:, 0]  # First column\n",
    "mean = np.nanmean(data[:, 1])\n",
    "v = data[:, 1] - mean  # Second column\n",
    "errs = data[:, 2]   # Third column\n",
    "variance = np.var(v) #+ np.mean(errs)**2\n",
    "stdev = np.sqrt(variance)\n",
    "#frequencies = np.linspace(1.95,2.05,3000)\n",
    "frequencies = np.linspace(0.001,1,1000)\n",
    "powers = LombScargle(t, v,errs).power(frequencies)\n",
    "\n",
    "\n",
    "fig, (ax_t, ax_w) = plt.subplots(2, 1, facecolor=\"white\", figsize=(12,14), constrained_layout=True, dpi = 200)\n",
    "\n",
    "ax_t.errorbar(t, v, yerr=errs,fmt = '.k',capsize=5,lw = 1.5)\n",
    "ax_t.fill_between(t, -stdev, stdev, color='gray',label = \"1 sigma\", alpha=0.4)\n",
    "ax_t.fill_between(t, -3* stdev, 3* stdev, color='gray',label = \"3 sigma\", alpha=0.2)\n",
    "ax_t.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax_t.legend()\n",
    "ax_t.text(np.max(t)/3, 2.5* stdev, f\"RV mean = {mean:.3g} km/s\")\n",
    "ax_t.text(np.max(t)/3, 2* stdev, f'sigma = {stdev:.3g} km/s')\n",
    "ax_t.set_xlabel(\"T - Days\")\n",
    "ax_t.set_ylabel(\"ΔRV - km/s\")\n",
    "\"\"\" \n",
    "#insrerting a planet\n",
    "periodtime = np.linspace(np.min(t), np.max(t), 10000) \n",
    "ax_t.plot(periodtime, np.abs(v).max() * np.sin(2*np.pi* 2*periodtime))\n",
    "ax_t.set_xlim(10,50)\n",
    " \"\"\"\n",
    "#plt.title(\"WD1929+012 Radial Velocity variations using Voigt fits - SALT data\")\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(-20,20)\n",
    "#plt.xlim(0,112)\n",
    "\n",
    "normalized_powers = powers/(np.max(np.abs(powers)))\n",
    "ax_w.plot(frequencies, normalized_powers )\n",
    "ax_w.set_xlabel('Angular frequency [Periods/days]')\n",
    "ax_w.set_ylabel('Normalized Power')\n",
    "ax_w.tick_params(axis='x')\n",
    "ax_w.tick_params(axis='y')\n",
    "\n",
    "\n",
    "# Identify significant peaks (you can set your own threshold here)\n",
    "threshold = 0.6  # Adjust as needed\n",
    "significant_peak_indices = np.where(normalized_powers >= threshold)[0]\n",
    "print(significant_peak_indices)\n",
    "print(frequencies[1])\n",
    "significant_peaks = np.array(frequencies[significant_peak_indices])\n",
    "\n",
    "# Estimate false alarm rate for each peak\n",
    "false_alarm_rates = []\n",
    "for peak_freq in significant_peaks:\n",
    "    # Use your preferred method to estimate false alarm rate here\n",
    "    # Example: Monte Carlo simulations\n",
    "    num_simulations = 500  # Adjust as needed\n",
    "    peak_heights_simulated = []\n",
    "    for _ in range(num_simulations):\n",
    "        simulated_data = np.random.normal(0, 1, len(t))  # Generate random noise\n",
    "        simulated_power = LombScargle(t, simulated_data, errs).power([peak_freq])\n",
    "        peak_heights_simulated.append(simulated_power[0])\n",
    "    false_alarm_rate = np.sum(peak_heights_simulated >= normalized_powers[np.where(frequencies == peak_freq)]) / num_simulations\n",
    "    false_alarm_rates.append(false_alarm_rate)\n",
    "\n",
    "# Print significant peaks and their corresponding false alarm rates\n",
    "print(\"Significant Peaks:\")\n",
    "for i, freq in enumerate(significant_peaks):\n",
    "    print(f\"Frequency: {freq}, False Alarm Rate: {false_alarm_rates[i]}\")\n",
    " \n",
    "os.makedirs(\"/data/wdplanetary/omri/Output/DeltaRV_files/SALT/Self_crosscorr/\", exist_ok=True)\n",
    "#plt.savefig(\"/data/wdplanetary/omri/Output/DeltaRV_files/SALT/Self_crosscorr/firstrun.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvshift = (0.001/3933) * 299792000\n",
    "print(f\"rv = {rvshift} (m/s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
