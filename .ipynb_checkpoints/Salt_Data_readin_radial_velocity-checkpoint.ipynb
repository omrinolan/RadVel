{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radial velocity shifts from SALT spectral data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import numpy as np\n",
    "import astropy.io.fits as fits\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.modeling import models,fitting\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy import signal\n",
    "from scipy.signal import correlate\n",
    "from PyAstronomy import pyasl\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.time import Time\n",
    "from lmfit import Model\n",
    "from lmfit.models import LinearModel, GaussianModel, VoigtModel\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "import fnmatch\n",
    "import os\n",
    "from astropy.timeseries import LombScargle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdulist = fits.open(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "header = hdulist[0].header\n",
    "date_obs = header.get('DATE-OBS', None)\n",
    "time = Time(date_obs, format='fits')\n",
    "print(time)\n",
    "hdulist.close()\n",
    "\n",
    "# Print the UTC representation of the time\n",
    "print(f\"Observation Date and Time (UTC): {time.utc.iso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Wavelength_Flux_File(filename) :\n",
    "    # Get the wavelength and flux from a MIDAS pipeline file\n",
    "    hdulist = fits.open(filename)\n",
    "    header = hdulist[0].header\n",
    "    date_obs = header.get('DATE-OBS', None)\n",
    "    time = Time(date_obs, format='fits')    \n",
    "    \n",
    "    flux = hdulist[0].data # flux in counts \n",
    "    \n",
    "    CRVAL1 = hdulist[0].header['CRVAL1'] # Coordinate at reference pixel\n",
    "    CRPIX1 = hdulist[0].header['CRPIX1'] # Reference pixel\n",
    "    CDELT1 = hdulist[0].header['CDELT1'] # Coordinate increase per pixel\n",
    "    HEL_COR = hdulist[0].header['HEL_COR'] # Heliocentric correction\n",
    "    \n",
    "    # Need to use the info above to make the wavelength axis\n",
    "    wavelength = np.zeros(len(flux))\n",
    "    \n",
    "    for i in range(len(wavelength)) :\n",
    "        wavelength[i] = (CRVAL1 + CDELT1*i) + (HEL_COR/299792 )*(CRVAL1 + CDELT1*i)\n",
    "        #wavelength[i] = (CRVAL1 + CDELT1*i)\n",
    "    \n",
    "    hdulist.close()\t\n",
    "\n",
    "    return wavelength, flux, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_noise = []\n",
    "\n",
    "#dictionary for all the inter-order noise areas\n",
    "io_noise.append((3890,3910))\n",
    "io_noise.append((3890,3910))\n",
    "io_noise.append((3934,3945))\n",
    "io_noise.append((3960,3972))\n",
    "io_noise.append((3995,4010))\n",
    "io_noise.append((4030,4040))\n",
    "io_noise.append((4063,4075))\n",
    "io_noise.append((4103,4115))\n",
    "io_noise.append((4140,4150))\n",
    "io_noise.append((4175,4185))\n",
    "io_noise.append((4175,4185))\n",
    "io_noise.append((4215,4225))\n",
    "io_noise.append((4175,4185))\n",
    "io_noise.append((4255,4265))\n",
    "io_noise.append((4290,4305))\n",
    "io_noise.append((4330,4340))\n",
    "io_noise.append((4372,4380))\n",
    "io_noise.append((4415,4428))\n",
    "io_noise.append((4455,4470))\n",
    "io_noise.append((4455,4470))\n",
    "io_noise.append((4502,4510))\n",
    "io_noise.append((4545,4555))\n",
    "io_noise.append((4455,4470))\n",
    "io_noise.append((4585,4600))\n",
    "io_noise.append((4633,4650))\n",
    "io_noise.append((4680,4690))\n",
    "io_noise.append((4728,4740))\n",
    "io_noise.append((4777,4790))\n",
    "io_noise.append((4826,4835))\n",
    "io_noise.append((4878,4888))\n",
    "io_noise.append((4928,4938))\n",
    "io_noise.append((4980,4988))\n",
    "io_noise.append((5032,5050))\n",
    "io_noise.append((5080,5100))\n",
    "io_noise.append((5148,5158))\n",
    "io_noise.append((5208,5215))\n",
    "io_noise.append((5265,5275))\n",
    "io_noise.append((5325,5335))\n",
    "io_noise.append((5325,5335))\n",
    "io_noise.append((5385,5395))\n",
    "io_noise.append((5400,5600)) #big cut off for second channel (red)\n",
    "io_noise.append((5625,5635))\n",
    "io_noise.append((5695,5705))\n",
    "io_noise.append((5765,5779))\n",
    "#io_noise.append((5765,5779))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdulist = fits.open(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "header = hdulist[0].header\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lines = []\n",
    "spec_lines = []\n",
    "\n",
    "#pollutant lines\n",
    "p_lines.append((\"Ca II\",4226.727, False))\n",
    "p_lines.append((\"Fe II\",4923.927, False))\n",
    "p_lines.append((\"Fe II\",5018.440, False))\n",
    "p_lines.append((\"Fe II\",5169.033, False)) #this one gives an error when trying to fit it\n",
    "p_lines.append((\"Si II\",5041.024, False))\n",
    "p_lines.append((\"Si II\",5055.984, False))\n",
    "p_lines.append((\"Si II\",5957.560, False))\n",
    "p_lines.append((\"Si II\",5978.930, False))\n",
    "p_lines.append((\"Si II\",6347.100, True)) #these two are quite strong in WD1929+012\n",
    "p_lines.append((\"Si II\",6371.360, True)) #\n",
    "p_lines.append((\"Mg I\",5172.683, False))\n",
    "p_lines.append((\"Mg I\",5183.602, False))\n",
    "p_lines.append((\"Mg II\",4481.130, True)) #strong magnesium line\n",
    "p_lines.append((\"Mg II\",4481.327, False))\n",
    "p_lines.append((\"Mg II\",7877.054, False)) \n",
    "p_lines.append((\"Mg II\",7896.366, False)) \n",
    "p_lines.append((\"O I\",7771.944, False)) \n",
    "#hydrogen lines\n",
    "p_lines.append((\"H\",4860.680, True))\n",
    "p_lines.append((\"H\",4860.968, False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick the spectral lines present in this white dwarf\n",
    "\n",
    "for i in p_lines:\n",
    "    if i[2] == True:\n",
    "        spec_lines.append(i)\n",
    "\n",
    "b_lines = []\n",
    "r_lines = []\n",
    "for i in spec_lines:\n",
    "    if i[1] <= 5000:\n",
    "        b_lines.append(i)\n",
    "    else:\n",
    "        r_lines.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process the inputted wavelength and flux data, \n",
    "#making binary masks, moving averagses, normalising the continuum\n",
    "\n",
    "def process_data(wav,flux):\n",
    "    \n",
    "    #interpolating for the interorder noise\n",
    "    #for i in io_noise:\n",
    "    #    a = np.searchsorted(wav, i[0])\n",
    "    #    b = np.searchsorted(wav, i[1])\n",
    "    #    flux[a:b] = np.nan\n",
    "\n",
    "    #not_nan = np.where(~np.isnan(flux))[0]\n",
    "    #x = np.arange(flux.shape[0])\n",
    "    #flux[np.isnan(flux)] = np.interp(x[np.isnan(flux)], not_nan, flux[not_nan])\n",
    "\n",
    "    #make a new binary mask that is accurate to 3dp\n",
    "    xwav = np.arange(3857.2,8871.2,0.001)\n",
    "    #mask = np.zeros(len(xwav))\n",
    "    #mask[:]=1\n",
    "\n",
    "    #adjust the size of the flux data to fit the mask\n",
    "    #padded_flux = np.interp(xwav, wav, flux)\n",
    "    \n",
    "    #normalise the continuum\n",
    "    #smoothed_continuum = savgol_filter(padded_flux, window_length=1001, polyorder=3)\n",
    "    #n_p_flux = padded_flux / smoothed_continuum\n",
    "\n",
    "    # Adjust the size of the flux data to fit the mask\n",
    "    interp_func = interp1d(wav, flux, bounds_error=False, fill_value=np.nan)\n",
    "    padded_flux = interp_func(xwav)\n",
    "\n",
    "    # Normalise the continuum using Savitzky-Golay filter\n",
    "    #smoothed_continuum = savgol_filter(padded_flux, window_length=2001, polyorder=3)\n",
    "    #n_p_flux = np.nan_to_num(padded_flux/smoothed_continuum, nan=1, posinf=1, neginf=1)\n",
    "\n",
    "    # Clip values to the bound\n",
    "    #bound = 2\n",
    "    #n_p_flux = np.clip(n_p_flux, -bound, bound)\n",
    "\n",
    "    #also maybe we could make a mask that is basically a moving average of the spectrum \n",
    "    #over a very wide moving average?\n",
    "    #window_size = 100000\n",
    "    #n_moving_avg = uniform_filter1d(n_p_flux, size=window_size)\n",
    "\n",
    "    #Add a binary point for each of the spectral lines\n",
    "    #for i in spec_lines:\n",
    "    #   loc = np.searchsorted(xwav, i[1])\n",
    "    #    closest_value = xwav[max(0, loc-1)]\n",
    "    #    index = np.where(xwav == closest_value)\n",
    "    #    mask[index] = 0 #binary value - later on will add strength to this depending on the log (gf)\n",
    "    #    moving_avg[index] = 0\n",
    "    \n",
    "    return xwav,padded_flux#n_mask\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#calculate errors - first loop - maybe easier to change\n",
    "def calculate_error(data):\n",
    "    errors = np.zeros_like(data, dtype=float)\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # Determine window boundaries\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(len(data), i + half_window + 1)\n",
    "        \n",
    "        # Compute standard deviation within the window\n",
    "        window_std = np.std(data[start:end])\n",
    "        \n",
    "        # Assign error to the data point\n",
    "        errors[i] = window_std\n",
    "    \n",
    "    return errors\n",
    "\n",
    "#size of window where errors are calculated\n",
    "window_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate errors - faster\n",
    "def calculate_error(data):\n",
    "    # Use a rolling window approach for efficient calculation of standard deviation\n",
    "    rolling_std = np.sqrt(np.convolve(data**2, np.ones(window_size) / window_size, mode='valid') - np.convolve(data, np.ones(window_size) / window_size, mode='valid')**2)\n",
    "    \n",
    "    # Extend the result to match the original length of the data\n",
    "    pad_width = window_size // 2\n",
    "    errors = np.pad(rolling_std, (pad_width, pad_width), mode='edge')\n",
    "    #print(len(errors),len(data))\n",
    "    errorsv = np.nan_to_num(5*errors)\n",
    "    #e = np.full(len(data),0.001)\n",
    "    #return e\n",
    "    print(np.mean(errorsv))\n",
    "    return errorsv[:len(data)]\n",
    "window_size = 50\n",
    "\n",
    "\n",
    "#Currently using a window size of 100 - better representing the individual variation\n",
    "#Also currently multiplying the raw error figure in the data by 5 to try and get a more representative result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#plotting the mask on the spectrum\n",
    "fig, ax = plt.subplots(facecolor='white')\n",
    "#ax.plot(wav,flux)\n",
    "ax.plot(xwav,n_p_flux)\n",
    "ax.set_xlim(4800,4900)\n",
    "ax.set_ylim(0, 1.5E-2)\n",
    "ax.set_xlabel(\"Wavelength(Amstrong)\")\n",
    "ax.set_ylabel(\"Normalised Flux\")\n",
    "plt.show()\n",
    "\n",
    "ax.plot(xwav,n_p_flux)\n",
    "ax.set_xlabel(\"Wavelength(Amstrong)\")\n",
    "ax.set_ylabel(\"Normalised Flux\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laura's Gaussian fitting\n",
    "def Gaussian(wavelength,flux,errors,c_lines,time):\n",
    "    RV = np.array([])\n",
    "    RV_weight = np.array([])\n",
    "    RV_err = np.array([])\n",
    "    w_size = 30\n",
    "    for i in c_lines:\n",
    "        #find the upper bound of the window\n",
    "        u_loc = np.searchsorted(xwav, (i[1]+(w_size/2)))\n",
    "        closest_value = xwav[max(0, u_loc-1)]\n",
    "        u_bound = np.where(xwav == closest_value)\n",
    "        u_bound = int(u_bound[0])\n",
    "        #find the lower bound of the window\n",
    "        l_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "        closest_value = xwav[max(0, l_loc-1)]\n",
    "        l_bound = np.where(xwav == closest_value)\n",
    "        l_bound = int(l_bound[0])\n",
    "        #make small datasets around the line\n",
    "        gdata = n_p_flux[l_bound:u_bound]\n",
    "        gwav = xwav[l_bound:u_bound]\n",
    "        #make a new window for errors which is just outside the window - basically the next window down instead\n",
    "        el_loc = np.searchsorted(xwav, (i[1]-3*(w_size/2)))\n",
    "        closest_value = xwav[max(0, el_loc-1)]\n",
    "        el_bound = np.where(xwav == closest_value)\n",
    "        el_bound = int(el_bound[0])\n",
    "        eh_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "        closest_value = xwav[max(0, eh_loc-1)]\n",
    "        eh_bound = np.where(xwav == closest_value)\n",
    "        eh_bound = int(eh_bound[0])\n",
    "\n",
    "        gerrors = errors[el_bound:eh_bound]\n",
    "        gerrors[np.isnan(gerrors)] = 0.001\n",
    "        gweights = np.where(gerrors != 0, 1 / gerrors, 0.0001)\n",
    "        \n",
    "        \n",
    "        #set the initial guess of the mean value of the gaussian to the wavelength in air\n",
    "        line = i[1]\n",
    "        voigt_model = VoigtModel(prefix='voigt_')\n",
    "        linear_model = LinearModel(prefix='linear_')\n",
    "        composite_model = voigt_model + linear_model\n",
    "        params = voigt_model.make_params(voigt_amplitude=-0.1, voigt_center=line, voigt_sigma=0.1)\n",
    "        params += linear_model.make_params(slope=0, intercept=np.median(gdata))\n",
    "        result = composite_model.fit(gdata, params, x=gwav, weights = gweights)\n",
    "        plt.plot(gwav,gdata)\n",
    "        plt.plot(gwav, result.best_fit)\n",
    "        plt.title(time.datetime)\n",
    "        plt.show()\n",
    "        #print(result.values['voigt_center'])\n",
    "        #print(result.fit_report())\n",
    "        Line_Offset = result.values['voigt_center'] - line\n",
    "        RV= np.append(RV,(Line_Offset/line) * 299792)\n",
    "        err = (((result.params['voigt_center'].stderr) / line) * 299792)\n",
    "        RV_err = np.append(RV_err,(err))\n",
    "        RV_weight = np.append(RV_weight,(1/(err**2)))\n",
    "        #print(line,RV,RV_err)\n",
    "        #shifts.append((RV,RV_err,RV_weight))\n",
    "                              \n",
    "    #weighted mean and standard deviation calculations\n",
    "    mean = (np.sum(RV * RV_weight)) / (np.sum(RV_weight))\n",
    "    mean_error = np.sqrt(np.sum(RV_weight * (RV - mean)**2) / np.sum(RV_weight))\n",
    "    return mean, mean_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rvs = []\n",
    "time_strings = []\n",
    "\n",
    "#Now we try and iterate over all of the files in the folder\n",
    " \n",
    "wav_b,flux_b,time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "wav_r,flux_r,time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphR201707060019_u2wm.fits\")\n",
    "wav = np.concatenate((wav_b,wav_r))\n",
    "flux = np.concatenate((flux_b,flux_r))\n",
    "\n",
    "#then call data processing\n",
    "xwav,n_p_flux,n_mask = process_data(wav,flux)\n",
    "#then do gaussian\n",
    "rv = Gaussian(xwav,n_p_flux,n_mask)\n",
    "rvs.append((rv))\n",
    "time_strings.append((time))\n",
    "#then do cross-correlation\n",
    "#First need to have that whole thing as a function, of just a filename input and outputting time and rv\n",
    "#then make a graph of the radial velocity vs time\n",
    "times = Time(time_strings, scale='utc')\n",
    "print(rvs,times)\n",
    "plt.plot(times.datetime, rvs,marker='o', linestyle='-')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#attempt to do gaussian fits on each spectral line\n",
    "#need to do a separate gaussian fit for each line and then average them(possibly weighted with stddev later on)\n",
    "from specutils.fitting import fit_lines\n",
    "\n",
    "shifts = []\n",
    "\n",
    "#\n",
    "#def Gaussian(gwav,gdata,line):\n",
    "    #g_init = models.Gaussian1D(amplitude=-0.01, mean=(line), stddev=0.6)\n",
    "    #fit_g = fitting.LevMarLSQFitter()\n",
    "    #g = fit_lines(g_init, gwav, gdata)\n",
    "    #return g,g_init\n",
    "\n",
    "#def spec_gaussian(gwav,gdata,line):\n",
    "    #g_init = models.Gaussian1D(amplitude=-0.01, mean=line, stddev=0.6)\n",
    "    #g_fit = fit_lines(gdata, g_init,get_fit_info=True)\n",
    "    #y_fit = g_fit(gwav)\n",
    "    #return #y_fit\n",
    "\n",
    "#manual gaussian\n",
    "def g(gwav,amplitude, mean, stddev, offset,slope):\n",
    "    g = amplitude * np.exp(-(gwav - line)**2 / (stddev * 2**2)) + (offset + gwav*slope) #calculate the gaussian\n",
    "    return g\n",
    "\n",
    "def composite(gwav,gdata,line): #manual gaussian\n",
    "    p0 = [-0.01, line, 2, 0.02,0.000001]  # Initial guess for parameters (amplitude, mean, stddev, offset)\n",
    "   #gauss = g(gwav,p0)\n",
    "    params, covariance = curve_fit(g, gwav, gdata,p0) #fit gaussian to data\n",
    "    return params,covariance\n",
    "    \n",
    "\n",
    "for i in spec_lines:\n",
    "    w_size = 10\n",
    "    #find the upper bound of the window\n",
    "    u_loc = np.searchsorted(xwav, (i[1]+(w_size/2)))\n",
    "    closest_value = xwav[max(0, u_loc-1)]\n",
    "    u_bound = np.where(xwav == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "    closest_value = xwav[max(0, l_loc-1)]\n",
    "    l_bound = np.where(xwav == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    #make small datasets around the line\n",
    "    gdata = padded_flux[l_bound:u_bound]\n",
    "    gwav = xwav[l_bound:u_bound]\n",
    "    #set the initial guess of the mean value of the gaussian to the wavelength in air\n",
    "    line = i[1]\n",
    "    #call the gaussian funtcion\n",
    "    params,covariance = composite(gwav,gdata,line)\n",
    "    result\n",
    "    print (result[0])\n",
    "    #add the resultsof the fitted gaussian to the dictionary, if the shift comes out with a sufficiently low value\n",
    "    if abs(result[1] - line) <= 10000: #method for cutting out outlying shift values\n",
    "        shifts.append((result[1],line))\n",
    "        plt.plot(gwav,gdata,label = \"data\")\n",
    "        #plt.plot(gwav,g[1](gwav),label = \"gaussian fit unshifted\")\n",
    "        plt.plot(gwav,result[0],label = \"gaussian fit\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "print(shifts)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the cross coreelation of the entire spectrum as created by Lalitha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlation(reference_flux, observed_flux):\n",
    "    # Perform cross-correlation using NumPy's correlate function. The reference spectra here is any one spectra. \n",
    "    # Ideally I would take the first epoch spectra as the reference spectra and measure the radial velocity \n",
    "    # shift with respect to the first epoch spectra.\n",
    "    cross_correlation_result = correlate(observed_flux, reference_flux, mode='full')\n",
    "\n",
    "    \n",
    "    # Determine the velocity axis corresponding to the cross-correlation result\n",
    "    velocity_axis = (np.arange(len(cross_correlation_result)) - (len(cross_correlation_result) - 1) / 2) * \\\n",
    "                     (wavelength_observed[1] - wavelength_observed[0])\n",
    "\n",
    "    # Plot the cross-correlation function\n",
    "    #plt.plot(velocity_axis, cross_correlation_result)\n",
    "    #plt.xlabel('Velocity (km/s)')\n",
    "    #plt.ylabel('Arbitrary CCF value')\n",
    "    #plt.xlim(-300,300)\n",
    "    \n",
    "\n",
    "    # Find the velocity shift corresponding to the maximum cross-correlation value\n",
    "    max_correlation_index = np.argmax(cross_correlation_result)\n",
    "    velocity_shift = velocity_axis[max_correlation_index]\n",
    "    #print('Velocity Shift:', velocity_shift, 'km/s')\n",
    "    return velocity_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def crosscorrRV(reference_wav,reference_flux,observed_flux,observed_wav):\n",
    "    # Perform cross-correlation using NumPy's correlate function. The reference spectra here is any one spectra. \n",
    "    # Ideally I would take the first epoch spectra as the reference spectra and measure the radial velocity \n",
    "    # shift with respect to the first epoch spectra.\n",
    "    rv,cc = pyasl.crosscorrRV(reference_wav,reference_flux, observed_flux,observed_wav,-50.,50.,1,mode = \"lin\",skipedge = 2000)\n",
    "    \n",
    "    maxind = np.argmax(cc)\n",
    "    velocity_shift = rv[maxind]\n",
    "    #print(maxind,rv[maxind])\n",
    "    #print(rv,cc)\n",
    "    #plt.plot(rv, cc)\n",
    "    #plt.xlabel(\"Radial Velocity shift - km/s\")\n",
    "    #plt.ylabel(\"Cross-correlation\")\n",
    "    #plt.show()\n",
    "\n",
    "    return velocity_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #make a dictionary for all the files we want in the folder - need to automate this\n",
    "current_spectrum_filenames = []\n",
    "current_spectrum_filenames.append((\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pick the files that we want\n",
    "\n",
    "def pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "    \"\"\"\n",
    "    Pick files in a folder based on given start and end patterns in their filename.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files.\n",
    "    - start_patterns (list): List of patterns to match at the start of filenames.\n",
    "    - end_patterns (list): List of patterns to match at the end of filenames.\n",
    "\n",
    "    Returns:\n",
    "    - List of filenames matching the specified start and end patterns.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        fp = os.path.join(folder_path, folder_name,\"product/\")\n",
    "\n",
    "        # List all files in the folder\n",
    "        all_files = os.listdir(fp)\n",
    "\n",
    "        # Filter files based on start patterns\n",
    "        #for start_pattern in start_patterns:\n",
    "        #   matching_files.extend(fnmatch.filter(all_files, start_pattern + '*'))\n",
    "\n",
    "        # Filter files based on end patterns\n",
    "        #for end_pattern in end_patterns:\n",
    "         #   matching_files.extend(fnmatch.filter(all_files, '*' + end_pattern))\n",
    "        for file in all_files:\n",
    "            if file.startswith(start_patterns) and file.endswith(end_patterns):\n",
    "                matching_files.append(os.path.join(fp, file))\n",
    "    return matching_files\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"/data/wdplanetary/omri/Data/WD1929+012/\"\n",
    "blue_start = (\"mbgphH\")\n",
    "red_start =  (\"mbgphR\")\n",
    "end_patterns = (\"u2wm.fits\")  # Example end patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating file directory for the blue and red channels separately\n",
    "b_files = pick_files_by_patterns(folder_path, blue_start, end_patterns)\n",
    "r_files = pick_files_by_patterns(folder_path, red_start, end_patterns)\n",
    "\n",
    "bad_date = ['/data/wdplanetary/omri/Data/WD1929+012/2019-1-SCI-008.20190513','/data/wdplanetary/omri/Data/WD1929+012/2020-1-SCI-043.20200531','/data/wdplanetary/omri/Data/WD1929+012/2018-1-SCI-043.20180605','/data/wdplanetary/omri/Data/WD1929+012/2019-2-SCI-049.20200824']\n",
    "b_files = [f for f in b_files if not any(f.startswith(start) for start in bad_date)]\n",
    "r_files = [f for f in r_files if not any(f.startswith(start) for start in bad_date)]\n",
    "#for i in (b_files):\n",
    "   #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runfile for the Cross-correlation code\n",
    "\n",
    "#for the blue files\n",
    "btime_strings = []\n",
    "bvels = []\n",
    "#\"/home/omn24/Documents/Documents/product/mbgphH201707060019_u2wm.fits\"\n",
    "wav_reference,flux_reference,ref_time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "btime_strings.append((ref_time))\n",
    "bvels.append(0)\n",
    "for i in b_files:\n",
    "    wavelength_observed, flux_observed,time = Get_Wavelength_Flux_File(i)\n",
    "    # Perform cross-correlation\n",
    "    vel = cross_correlation(flux_reference, flux_observed)\n",
    "    btime_strings.append(time)\n",
    "    bvels.append(vel)\n",
    "\n",
    "\n",
    "\n",
    "#for the red files\n",
    "rtime_strings = []\n",
    "rvels = []\n",
    "wav_reference,flux_reference,ref_time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphR201707060019_u2wm.fits\")\n",
    "rtime_strings.append((ref_time))\n",
    "rvels.append(0)\n",
    "for i in r_files:\n",
    "    wavelength_observed, flux_observed,time = Get_Wavelength_Flux_File(i)\n",
    "    # Perform cross-correlation\n",
    "    vel = cross_correlation(flux_reference, flux_observed)\n",
    "    rtime_strings.append(time)\n",
    "    rvels.append(vel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Runfile for the PyAstronomy CrosscorrRV code\n",
    "\n",
    "#for the blue files\n",
    "btime_strings = []\n",
    "bvels = []\n",
    "#\"/home/omn24/Documents/Documents/product/mbgphH201707060019_u2wm.fits\"\n",
    "wav_reference,flux_reference,ref_time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "btime_strings.append((ref_time))\n",
    "bvels.append(0)\n",
    "for i in b_files:\n",
    "    wavelength_observed, flux_observed,time = Get_Wavelength_Flux_File(i)\n",
    "    # Perform cross-correlation\n",
    "    vel = crosscorrRV(wav_reference,flux_reference,wavelength_observed, flux_observed)\n",
    "    btime_strings.append(time)\n",
    "    bvels.append(vel)\n",
    "\n",
    "\n",
    "\n",
    "#for the red files\n",
    "rtime_strings = []\n",
    "rvels = []\n",
    "wav_reference,flux_reference,ref_time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphR201707060019_u2wm.fits\")\n",
    "rtime_strings.append((ref_time))\n",
    "rvels.append(0)\n",
    "for i in r_files:\n",
    "    wavelength_observed, flux_observed,time = Get_Wavelength_Flux_File(i)\n",
    "    # Perform cross-correlation\n",
    "    vel = crosscorrRV(wav_reference,flux_reference,wavelength_observed, flux_observed)\n",
    "    rtime_strings.append(time)\n",
    "    rvels.append(vel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btimes = Time(btime_strings, scale='utc')\n",
    "plt.scatter(btimes.datetime, bvels,marker='x')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Radial Velocity km/s\")\n",
    "#plt.ylim(-25,-22)\n",
    "plt.title('WD1929+012 Radial Velocity variations \\n using spectral cross-correlations \\n in the \"blue\" channel ')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtimes = Time(rtime_strings, scale='utc')\n",
    "plt.scatter(rtimes.datetime, rvels,marker='x')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Radial Velocity km/s\")\n",
    "plt.ylim(-20,200)\n",
    "plt.title(\"WD1929+012 Radial Velocity variations \\n using spectral cross-correlations \\n in the red channel \")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian fits runfile for the blue channel\n",
    "#using code from above and iterating over the filenames\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "for i in b_files:\n",
    "    wav, flux ,time = Get_Wavelength_Flux_File(i)\n",
    "#then call data processing\n",
    "    xwav,n_p_flux = process_data(wav,flux)\n",
    "    errors = calculate_error(n_p_flux)\n",
    "#then do gaussian\n",
    "    rv,rv_err = Gaussian(xwav,n_p_flux,errors,b_lines,time)\n",
    "    rvs.append((rv))\n",
    "    rverrs.append((rv_err))\n",
    "    time_strings.append((time))\n",
    "    print(time)\n",
    "#then do cross-correlation\n",
    "#First need to have that whole thing as a function, of just a filename input and outputting time and rv\n",
    "#then make a graph of the radial velocity vs time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian fits runfile for the red channel\n",
    "#using code from above and iterating over the filenames\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "for i in r_files:\n",
    "    wav, flux ,time = Get_Wavelength_Flux_File(i)\n",
    "#then call data processing\n",
    "    xwav,n_p_flux = process_data(wav,flux)\n",
    "    errors = calculate_error(n_p_flux)\n",
    "#then do gaussian\n",
    "    rv,rv_err = Gaussian(xwav,n_p_flux,errors,r_lines,time)\n",
    "    rvs.append((rv))\n",
    "    rverrs.append((rv_err))\n",
    "    time_strings.append((time))\n",
    "    print(rv,rv_err,time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining data points that are the same times\n",
    "t_list = Time(time_strings, scale='utc')\n",
    "times = t_list.datetime \n",
    "\n",
    "print(np.mean(rvs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarray = np.array(times)\n",
    "rvarray = np.array(rvs)\n",
    "#rverrsarray = np.array(rverrs)\n",
    "rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "#unique_times = np.unique(tarray)\n",
    "#averages = np.array([[t, np.mean(rvarray[tarray == t]), np.mean(rverrsarray[tarray == t])] for t in unique_times])\n",
    "#print(averages)\n",
    "\n",
    "d = pd.DataFrame(data=[tarray, rvarray, rverrsarray], columns=[\"Time\", \"RV\", \"Errors\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data = pd.DataFrame({'Time':times,'RV':rvs, \"RV errors\":rverrs})\n",
    "#d = data.groupby('Time').mean().reset_index()\n",
    "#f = d[(d['RV'] >= 30) & (d['RV'] <= 50)]\n",
    "#times = [d['Time']]\n",
    "#rvs = [d[\"RV\"]]\n",
    "#rverrs = [d[\"RV errors\"]]\n",
    "#print((times))\n",
    "#print(d['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = averages[:, 0]\n",
    "rvs = averages[:, 1]\n",
    "#errs = np.full(len(rvs),2)\n",
    "errs = averages[:, 2]\n",
    "t0 = t[0]\n",
    "tdays = [(dt - t0).days for dt in t]\n",
    "\n",
    "plt.errorbar(tdays, rvs, yerr=errs,fmt = '.k')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Radial Velocity km/s\")\n",
    "plt.title(\"WD1929+012 Radial Velocity variations using Gaussian fits\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "plt.ylim(30,50)\n",
    "plt.xlim(0,100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian fitting plot\n",
    "\n",
    "plt.errorbar(times, rvs, yerr=rverrs,fmt = '.k')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Radial Velocity km/s\")\n",
    "plt.title(\"WD1929+012 Radial Velocity variations using Gaussian fits\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(30,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now attempt to make a periodogram of these results\n",
    "\n",
    "#frequency = np.linspace(1,100,1)\n",
    "frequency, power = LombScargle(tdays, rvs, errs).autopower()\n",
    "plt.plot(frequency, power)\n",
    "plt.title(\"Periodogram of the radial velocity measurements of WD1929+012\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
