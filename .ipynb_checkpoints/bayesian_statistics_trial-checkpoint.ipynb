{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian statistics approach for direct fitting\n",
    "#Should be run in our IoA virtual machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import numpy as np\n",
    "import astropy.io.fits as fits\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.modeling import models,fitting\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.stats import norm\n",
    "from scipy.special import voigt_profile\n",
    "from scipy import signal\n",
    "from scipy.signal import correlate\n",
    "from PyAstronomy import pyasl\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "from astropy.constants import c\n",
    "import astropy.io.fits as fits\n",
    "from astropy.units import dimensionless_unscaled\n",
    "from lmfit.models import LinearModel, GaussianModel, VoigtModel, PolynomialModel\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "import fnmatch\n",
    "import os\n",
    "from astropy.timeseries import LombScargle\n",
    "from plotnine import *\n",
    "import traceback\n",
    "import sys\n",
    "import linecache\n",
    "import textwrap\n",
    "import re\n",
    "import emcee\n",
    "import corner\n",
    "import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "plt.rcParams['axes.linewidth'] = 1.25\n",
    "plt.rcParams['xtick.major.size'] = 5\n",
    "plt.rcParams['xtick.major.width'] = 1.25\n",
    "plt.rcParams['ytick.major.size'] = 5\n",
    "plt.rcParams['ytick.major.width'] = 1.25\n",
    "\n",
    "default_settings = {\n",
    "    'font.size': 16,\n",
    "    'axes.linewidth': 0.8,\n",
    "    'xtick.major.size': 3.5,\n",
    "    'xtick.major.width': 1,\n",
    "    'ytick.major.size': 3.5,\n",
    "    'ytick.major.width': 1\n",
    "}\n",
    "\n",
    "\n",
    "initial_settings = {\n",
    "    'font.size': 22,\n",
    "    'axes.linewidth': 1.25,\n",
    "    'xtick.major.size': 5,\n",
    "    'xtick.major.width': 1.25,\n",
    "    'ytick.major.size': 5,\n",
    "    'ytick.major.width': 1.25\n",
    "}\n",
    "plt.rcParams.update(default_settings)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salt read in data\n",
    "def Get_Wavelength_Flux_File(filename) :\n",
    "    # Get the wavelength and flux from a MIDAS pipeline file\n",
    "    hdulist = fits.open(filename)\n",
    "    header = hdulist[0].header\n",
    "    date_obs = header.get('DATE-OBS', None)\n",
    "    time_obs = header.get('TIME-OBS',None)\n",
    "    time = Time(str(date_obs) + 'T' + str(time_obs), format='fits')\n",
    "    #time = Time(date_obs,format = \"fits\")\n",
    "    \n",
    "    flux = hdulist[0].data # flux in counts \n",
    "    \n",
    "    CRVAL1 = hdulist[0].header['CRVAL1'] # Coordinate at reference pixel\n",
    "    CRPIX1 = hdulist[0].header['CRPIX1'] # Reference pixel\n",
    "    CDELT1 = hdulist[0].header['CDELT1'] # Coordinate increase per pixel\n",
    "    HEL_COR = hdulist[0].header['HEL_COR'] # Heliocentric correction\n",
    "    OBJECT = str(header.get(\"OBJECT\", None))\n",
    "    \n",
    "    # Need to use the info above to make the wavelength axis\n",
    "    wavelength = np.zeros(len(flux))\n",
    "    print(f\"The reference pixel in timefile: {time} is {CRPIX1}\")\n",
    "    for i in range(len(wavelength)) :\n",
    "        #heliocentric correction\n",
    "        wavelength[i] = (CRVAL1 + CDELT1*i) + (HEL_COR/299792 )*(CRVAL1 + CDELT1*i) - (CRPIX1*CDELT1)\n",
    "        #wavelength[i] = (CRVAL1 + CDELT1*i)\n",
    "    \n",
    "    hdulist.close()\t\n",
    "\n",
    "    return wavelength, flux, time, OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line dictionaries: Name, position(A), present?, log gf\n",
    "#if you want to add a line to be used in the model, change the corresponding line in the dictionary to True instead of False\n",
    "p_lines = []\n",
    "spec_lines = []\n",
    "\n",
    "#pollutant lines\n",
    "p_lines.append((\"CaII_3933\", 3933.663, False)) #only use for MIKE data\n",
    "p_lines.append((\"CaII_4226\",4226.727, False)) #good\n",
    "p_lines.append((\"FeII_4923\",4923.927, False)) #can't use, crosses over with interorder\n",
    "p_lines.append((\"FeII_5018\",5018.440, False))#not there\n",
    "p_lines.append((\"FeII_5169\",5169.033, False)) #this one gives an error when trying to fit it\n",
    "p_lines.append((\"SiII_5041\",5041.024, False)) #don't use this\n",
    "p_lines.append((\"SiII_5055\",5055.984, True)) #This one is good use it\n",
    "p_lines.append((\"SiII_5957\",5957.560, True))\n",
    "p_lines.append((\"SiII_5978\",5978.930, True))\n",
    "p_lines.append((\"SiII_6347\",6347.100, True)) #these two are quite strong in WD1929+012\n",
    "p_lines.append((\"SiII_6371\",6371.360, True)) #\n",
    "p_lines.append((\"MgI_5172\",5172.683, False))\n",
    "p_lines.append((\"MgI_5183\",5183.602, False))\n",
    "p_lines.append((\"MgII_4481\",4481.130,True)) #strong magnesium line\n",
    "p_lines.append((\"MgII_4481_2\",4481.327, False))\n",
    "p_lines.append((\"MgII_4481\",4481.180,False)) #weighted combination of mg_4481 lines\n",
    "\n",
    "p_lines.append((\"MgII_7877\",7877.054, False)) \n",
    "p_lines.append((\"MgII_7896\",7896.366, False)) #definitely not present\n",
    "p_lines.append((\"OI_7771\",7771.944, False)) #definitely not present\n",
    "#hydrogen lines\n",
    "p_lines.append((\"H_4860\",4860.680, False))\n",
    "p_lines.append((\"H_4860_2\",4860.968, True))#This one gives better values\n",
    "p_lines.append((\"H_4860_2\",4860.968, False))#Weighted combo\n",
    "p_lines.append((\"H_4340\",4340.472,False)) #present\n",
    "p_lines.append((\"H_6563\",6562.79 ,False)) #not present in the 2020 spectra?\n",
    "#pick the spectral lines present in this white dwarf\n",
    "\n",
    "for i in p_lines:\n",
    "    if i[2] == True:\n",
    "        spec_lines.append(i)\n",
    "\n",
    "b_lines = []\n",
    "r_lines = []\n",
    "for i in spec_lines:\n",
    "    if i[1] <= 5550:\n",
    "        #370 - 555 nm\n",
    "        b_lines.append(i)\n",
    "    else:\n",
    "        #555 - 890 nm\n",
    "        r_lines.append(i)\n",
    "\n",
    "#Now define the sky lines that we will use to find the stability corrections from the instrument variability\n",
    "\n",
    "sky_lines = []\n",
    "sky_lines.append((\"OI_5577\",5577.340))\n",
    "sky_lines.append((\"OI_6300\",6300.304))\n",
    "sky_lines.append((\"OI_6364\",6363.776))\n",
    "#sky_lines.append((\"H2O_7392\"))\n",
    "#sky_lines.append((\"H2O_8365\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate errors - faster\n",
    "\n",
    "def calculate_error(data):\n",
    "    window_size = int(500)\n",
    "    # Use a rolling window approach for efficient calculation of standard deviation\n",
    "    #least mean squared method\n",
    "    rolling_std = np.sqrt(np.convolve(data**2, np.ones(window_size) / window_size, mode='valid') - np.convolve(data, np.ones(window_size) / window_size, mode='valid')**2)\n",
    "    \n",
    "    # Extend the result to match the original length of the data\n",
    "    pad_width = window_size // 2\n",
    "    errors = np.pad(rolling_std, (pad_width, pad_width), mode='edge')\n",
    "    #print(len(errors),len(data))\n",
    "    errorsv = np.nan_to_num(errors)\n",
    "    #print(np.mean(errorsv))\n",
    "    return errorsv[:len(data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(gwav,gdata):\n",
    "    poly_model = PolynomialModel(degree=7)\n",
    "    params = poly_model.guess(gdata, x=gwav)\n",
    "    poly_result = poly_model.fit(gdata, params, x=gwav)\n",
    "    print(poly_result.params['c0'],poly_result.params['c1'],poly_result.params['c2'],poly_result.params['c3'],poly_result.params['c4'])\n",
    "    \n",
    "    #find the individual polynomial best fit\n",
    "    c0 = poly_result.params['c0'].value\n",
    "    c1 = poly_result.params['c1'].value\n",
    "    c2 = poly_result.params['c2'].value\n",
    "    c3 = poly_result.params[\"c3\"].value\n",
    "    c4 = poly_result.params[\"c4\"].value\n",
    "    c5 = poly_result.params[\"c5\"].value\n",
    "    c6 = poly_result.params[\"c6\"].value\n",
    "    c7 = poly_result.params[\"c7\"].value\n",
    "    \n",
    "    p_result = c0 + c1 * gwav + c2 * gwav**2 + c3 * gwav**3 + c4 * gwav**4 + c5 * gwav**5 + c6 * gwav**6 + c7 * gwav**7\n",
    "    \"\"\" \n",
    "    plt.figure()\n",
    "    plt.plot(gwav, p_result, color='green', linewidth=1)\n",
    "    plt.plot(gwav, gdata, color='blue', linewidth=0.5)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "    return p_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pick the files that we want and make dictionaries of them\n",
    "\n",
    "def pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "   \n",
    "    matching_files = []\n",
    "\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        fp = os.path.join(folder_path, folder_name,\"product/\")\n",
    "\n",
    "        # List all files in the folder\n",
    "        try:\n",
    "            all_files = os.listdir(fp)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "\n",
    "        # Filter files based on start patterns\n",
    "        #for start_pattern in start_patterns:\n",
    "        #   matching_files.extend(fnmatch.filter(all_files, start_pattern + '*'))\n",
    "\n",
    "        # Filter files based on end patterns\n",
    "        #for end_pattern in end_patterns:\n",
    "         #   matching_files.extend(fnmatch.filter(all_files, '*' + end_pattern))\n",
    "        for file in all_files:\n",
    "            if file.startswith(start_patterns) and file.endswith(end_patterns):\n",
    "                matching_files.append(os.path.join(fp, file))\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "\n",
    "def mike_pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "    matching_files = []\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "    try:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            matching_files.append(os.path.join(folder_path, file_name))\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the folder doesn't exist\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Return the list of matching files\n",
    "    return matching_files\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#This is where we pick the folder path and the name of the file that we want\n",
    "#Need to adjust this so that it is more clear\n",
    "star = \"WD1929+012\"\n",
    "SALT_folder_path = \"/data/wdplanetary/omri/Data/WD1929+012/\"\n",
    "MIKE_blue_folder_path = \"/data/wdplanetary/laura/MIKE/Data/WD1929+011/blue/\"\n",
    "MIKE_red_folder_path = \"/data/wdplanetary/laura/MIKE/Data/WD1929+011/red/\"\n",
    "\n",
    "\n",
    "blue_start = (\"mbgphH\")\n",
    "red_start =  (\"mbgphR\")\n",
    "end_patterns = (\"u2wm.fits\")  #End pattern for the object fiber\n",
    "sky_end_patterns = (\"u1wm.fits\") #End pattern for the sky fiber\n",
    "merged_end_patterns = (\"uwm.fits\") #end pattern for the reduced fiber\n",
    "mike_start = (\"gal\")\n",
    "mike_end = (\".fits\")\n",
    "\n",
    "#Creating file directory for the blue and red channels separately\n",
    "b_files = pick_files_by_patterns(SALT_folder_path, blue_start, end_patterns)\n",
    "merged_b_files = pick_files_by_patterns(SALT_folder_path, blue_start, merged_end_patterns)\n",
    "r_files = pick_files_by_patterns(SALT_folder_path, red_start, end_patterns)\n",
    "sky_r_files = pick_files_by_patterns(SALT_folder_path, red_start, sky_end_patterns)\n",
    "mike_b_files = mike_pick_files_by_patterns(MIKE_blue_folder_path, mike_start,mike_end)\n",
    "mike_r_files = mike_pick_files_by_patterns(MIKE_red_folder_path, mike_start,mike_end)\n",
    "\n",
    "b_files.sort()\n",
    "r_files.sort()\n",
    "sky_r_files.sort()\n",
    "mike_b_files.sort()\n",
    "mike_r_files.sort()\n",
    "bad_date = ['/data/wdplanetary/omri/Data/WD1929+012/2019-1-SCI-008.20190513',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2020-1-SCI-043.20200531',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2018-1-SCI-043.20180605',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2019-2-SCI-049.20200824',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170813/product/mbgphH20170813003'\n",
    "            ]\n",
    "\n",
    "b_files = [f for f in b_files if not any(f.startswith(start) for start in bad_date)]\n",
    "r_files = [f for f in r_files if not any(f.startswith(start) for start in bad_date)] \n",
    "sky_r_files = [f for f in sky_r_files if not any(f.startswith(start) for start in bad_date)] \n",
    "merged_b_files = [f for f in merged_b_files if not any(f.startswith(start) for start in bad_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian statistics model initialization, data calling, and running\n",
    "\n",
    "\n",
    "#importing, cropping, and normalising data\n",
    "wav,flux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170714/product/mbgphH201707140017_u2wm.fits\")\n",
    "#rwav,rflux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphR201707110020_u2wm.fits\")\n",
    "#wav = np.concatenate((bwav,rwav))\n",
    "#flux = np.concatenate((bflux,rflux))\n",
    "#lower bound of window\n",
    "u_loc = np.searchsorted(wav, 5200) #5200\n",
    "closest_value = wav[max(0, u_loc-1)]\n",
    "u_bound = np.where(wav == closest_value)\n",
    "u_bound = int(u_bound[0])\n",
    "#find the lower bound of the window\n",
    "l_loc = np.searchsorted(wav, 4100) #4100\n",
    "closest_value = wav[max(0, l_loc-1)]\n",
    "l_bound = np.where(wav == closest_value)\n",
    "l_bound = int(l_bound[0])\n",
    "y = flux[l_bound:u_bound]\n",
    "l = wav[l_bound:u_bound]\n",
    "p_result = poly_fit(l,y)\n",
    "D = y/p_result\n",
    "sigma = calculate_error(D)/2\n",
    "\n",
    "#initilizing values\n",
    "#priors for the values\n",
    "#gaussian profile for line centre with the x as a function\n",
    "c_vals = []\n",
    "c_priors_list = []\n",
    "lines = ([i[1] for i in b_lines])\n",
    "c1 = np.array(lines)\n",
    "\n",
    "#first attempt to make gaussian priors, now doing inside the logprior function\n",
    "\"\"\" \n",
    "for j in c1:\n",
    "    gaussian = (norm.pdf(D, j, 0.04)) #make a prior function for the line \n",
    "    plt.plot(D,gaussian)\n",
    "    plt.show()\n",
    "    sum = np.sum(gaussian)\n",
    "    c_priors_list.append(sum)\n",
    " \"\"\"\n",
    "\n",
    "n = len(c1)\n",
    "print(f\"number of lines: {n}\")\n",
    "#initial values for each line \n",
    "rv = 36\n",
    "#sigma = 0.03\n",
    "s1 = np.full((n,), 0.3)\n",
    "g1 = np.full((n,), 0.3)\n",
    "d1 = np.full((n,), 0.6)\n",
    "offset = np.full((n,), 0)\n",
    "\n",
    "\n",
    "#make an array for the initial parameters, some of these are lists and some are scalars\n",
    "arrays_theta = [rv, s1, g1, d1, c1,offset]  # Assuming theta is a list containing both scalar values and arrays/lists\n",
    "theta = []  # Initialize an empty list to store the expanded values\n",
    "# Iterate through each element in theta\n",
    "for item in arrays_theta:\n",
    "    # Check if the current item is an array/list\n",
    "    if isinstance(item, (list, np.ndarray)):\n",
    "        # If it is an array/list, extend the expanded_theta list with its elements\n",
    "        theta.extend(item)\n",
    "    else:\n",
    "        # If it's a scalar value, append it directly to the expanded_theta list\n",
    "        theta.append(item)\n",
    "print(\"---------\")\n",
    "print(f\"the starting values are{theta}\")\n",
    "print(\"---------\")\n",
    "#limits on the priors\n",
    "cmin = c1 - 0.2\n",
    "cmax = c1 + 0.2\n",
    "print(f\"The lines we are using, and their minimum/maximum wavelengths are: {c1}\")\n",
    "print(f\"{cmin}\")\n",
    "print(f\"{cmax}\")\n",
    "smin = 0 #sigma of the fit\n",
    "smax = 1\n",
    "gmin = 0 #gamma of the fit\n",
    "gmax = 1\n",
    "dmin = 0 #depth of the fit\n",
    "dmax = 2\n",
    "offset_min = -0.2 #offset of the fit\n",
    "offset_max = 0.2\n",
    "Delta_min = 0 #RV shift\n",
    "Delta_max = 60\n",
    "\n",
    "\n",
    "\n",
    "def f(l, theta):\n",
    "    #function to generate the model\n",
    "    Delta = float(theta[0])\n",
    "    s = theta[1::5]\n",
    "    g = theta[2::5]\n",
    "    d = theta[3::5]\n",
    "    c = theta[4::5]\n",
    "    offset_values = theta[5::5] \n",
    "    model = 1 + np.sum([offset - (voigt_profile((l-(ci*(1+Delta/299792.0))),si, gi )) for ci, si,gi, di,offset in zip(c, s,g, d,offset_values)], axis=0)\n",
    "    return model\n",
    "\n",
    "#test plot to see how the model looks like compared to the spectrum \n",
    "for i in lines:\n",
    "    plt.plot(l, f(l,theta))\n",
    "    #plt.plot(l,D)\n",
    "    plt.fill_between(l,f(l,theta) - sigma, f(l,theta) + sigma, color='gray', alpha=0.3)\n",
    "    plt.xlim(i -10,i +10 )\n",
    "    #plt.ylim(0.4,1.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#minimum and maximum arrays for initial values\n",
    "prior_mini = np.array([Delta_min] + [smin,gmin, dmin, offset_min]*n ) \n",
    "prior_min = np.concatenate((prior_mini,cmin))\n",
    "\n",
    "prior_maxi = np.array([Delta_max] + [smax,gmax, dmax, offset_max ]*n )\n",
    "prior_max = np.concatenate((prior_maxi,cmax))\n",
    "\n",
    "\n",
    "def log_likelihood(theta, l, D,sigma):\n",
    "    residual = D - f(l, theta[:-1])\n",
    "    return -0.5 * np.sum((residual / sigma)**2 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "# Define the log prior function\n",
    "def log_prior(theta):\n",
    "    total_log_prior = 0\n",
    "    #defining priors again\n",
    "    Delta = theta[0]\n",
    "    s_values = theta[1::n]\n",
    "    g_values = theta[2::n]\n",
    "    d_values = theta[3::n]\n",
    "    c_values = theta[4::n]\n",
    "    offset_values = theta[5::n] \n",
    "\n",
    "    #setting the logprior to False if outside the priors\n",
    "    if not (Delta_min < Delta < Delta_max):\n",
    "        return -np.inf\n",
    "    if not (Delta_min < Delta < Delta_max):\n",
    "        return -np.inf\n",
    "\n",
    "    for (s,g, d,c,offs,c_init) in (zip(s_values, g_values, d_values,c_values,offset_values, c1)):\n",
    "        #adding priors\n",
    "        c_prior_logpdf = np.sum(norm.logpdf(c, loc = c_init,scale = 0.04))\n",
    "        total_log_prior += c_prior_logpdf\n",
    "\n",
    "        offset_prior_logpdf = np.sum(norm.logpdf(offset, loc = 0,scale = 0.2))\n",
    "        total_log_prior += offset_prior_logpdf\n",
    "\n",
    "        #restricting for bounds on parameters\n",
    "        if not (smin < s < smax) or not (gmin < g < gmax) or not (dmin < d < dmax) or not (c_init -0.5 < c < c_init+0.5) or not (offset_min < offs < offset_max):\n",
    "            return -np.inf\n",
    "    #adding a logprior function\n",
    "    \n",
    "    #prior_mu = norm.logpdf(theta[0], loc=0, scale=1)\n",
    "    \n",
    "    return total_log_prior\n",
    "\n",
    "# Define the log posterior function\n",
    "def log_probability(theta, l, D, sigma):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, l, D, sigma)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize walker positions using Latin hypercube sampling - also works - different way to initialize the sampling\n",
    "\"\"\" \n",
    "from pyDOE import lhs\n",
    "nwalkers = 64\n",
    "ndim = len(theta)  # Assuming n is defined elsewhere\n",
    "bounds = np.array([prior_min, prior_max]).T  # Define bounds for each parameter\n",
    "lhs_samples = lhs(ndim, samples=nwalkers)\n",
    "data = pd.DataFrame(lhs_samples)\n",
    "pos = bounds[:, 0] + lhs_samples * (bounds[:, 1] - bounds[:, 0])\n",
    " \"\"\"\n",
    "\n",
    " \n",
    "#initialise walkers and pos using normal gaussian distributions\n",
    "pos = (theta) + 1e-4 * np.random.randn(3*len(theta), len(theta))\n",
    "nwalkers, ndim = pos.shape \n",
    "\n",
    "\n",
    "\n",
    "# Run the sampler\n",
    "nsteps = 2000\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(l,D,sigma))\n",
    "sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting corner plot for trial model\n",
    "\n",
    "#with plt.rc_context(default_settings):\n",
    "#plt.rcParams.update(default_settings)\n",
    "paramnames = [[('RV', r'\\Delta')]] + [[(f's{i}', f's_{i}'), (f'g{i}', f'g_{i}'),(f'd{i}',f'd_{i}'),(f'c{i}',f'c_{i}'),(f'offset{i}',f'offset_{i}')] for i in range(n)] \n",
    "paramnames = [item for sublist in paramnames for item in sublist]\n",
    "\n",
    "# Get the samples\n",
    "samples = sampler.get_chain(discard=100, thin=1, flat=False)\n",
    "labels = [name_tuple[0] for name_tuple in paramnames]\n",
    "# Plot the traces\n",
    "\n",
    "#plotting all the variables\n",
    "\n",
    "fig, axes = plt.subplots(len(labels), figsize=(10, 10), sharex=True)\n",
    "#labels = [\"rv\",\"w1\",\"d1\",\"w2\",\"d2\",\"sigma\"]\n",
    "for i in range(len(labels)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "plt.show() \n",
    "\"\"\" \n",
    "fig, axes = plt.subplots(1, figsize=(10, 6), sharex=True)\n",
    "axes.plot(samples[:, :, 0], \"k\", alpha=0.3)\n",
    "axes.set_xlim(0, len(samples))\n",
    "axes.set_ylabel(\"RV (km/s)\")\n",
    "axes.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes.set_xlabel(\"step number\")\n",
    "plt.show() \"\"\"\n",
    "\n",
    "# Flatten the samples\n",
    "flat_samples = sampler.get_chain(discard=0, thin=15, flat=True)\n",
    "percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)  # This gives the 16th, 50th (median), and 84th percentiles\n",
    "\n",
    "\n",
    "print(percentiles)\n",
    "# Compute the medians of the parameter samples\n",
    "#variables = percentiles[1]\n",
    "variables = np.median(flat_samples, axis=0)\n",
    "\n",
    "print(variables)\n",
    "\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels,truths=[variables])\n",
    "\n",
    "# Annotate variables on the plot\n",
    "for i, var_value in enumerate(variables.items()):\n",
    "    plt.annotate(f\" {var_value:.5f}\", xy=(-1, 4.5 - i * 0.5), xycoords='axes fraction', fontsize=20, ha='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another attempt at making a simpler initialization of the model, where I label each parameter individually\n",
    "\n",
    "#importing, cropping, and normalising data\n",
    "wav,flux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170714/product/mbgphH201707140017_u2wm.fits\")\n",
    "#rwav,rflux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphR201707110020_u2wm.fits\")\n",
    "#wav = np.concatenate((bwav,rwav))\n",
    "#flux = np.concatenate((bflux,rflux))\n",
    "#lower bound of window\n",
    "u_loc = np.searchsorted(wav, 5200) #5200\n",
    "closest_value = wav[max(0, u_loc-1)]\n",
    "u_bound = np.where(wav == closest_value)\n",
    "u_bound = int(u_bound[0])\n",
    "#find the lower bound of the window\n",
    "l_loc = np.searchsorted(wav, 4100) #4100\n",
    "closest_value = wav[max(0, l_loc-1)]\n",
    "l_bound = np.where(wav == closest_value)\n",
    "l_bound = int(l_bound[0])\n",
    "y = flux[l_bound:u_bound]\n",
    "l = wav[l_bound:u_bound]\n",
    "p_result = poly_fit(l,y)\n",
    "D = y/p_result\n",
    "sigma = calculate_error(D)/2\n",
    "\n",
    "\n",
    "lines = ([i[1] for i in b_lines])\n",
    "\n",
    "#initializing values\n",
    "\n",
    "rv = 36\n",
    "\n",
    "#First profile - Mg 4481 - initial values\n",
    "ci_4481 = 4481.185\n",
    "si_4481 = 0.3\n",
    "gi_4481 = 0.3\n",
    "di_4481 = 0.5\n",
    "oi_4481 = 0\n",
    "\n",
    "#Second profile - H 4860\n",
    "ci_4860 = 4860.986\n",
    "si_4860 = 1.4\n",
    "gi_4860 = 1.4\n",
    "di_4860 = 2\n",
    "oi_4860 = -0.05\n",
    "\n",
    "#Third profile - Si 5055\n",
    "ci_5055 = 5055.984\n",
    "si_5055 = 0.3\n",
    "gi_5055 = 0.3\n",
    "di_5055 = 0.5\n",
    "oi_5055 = 0\n",
    "\n",
    "#Vague general limits on the fits - \n",
    "smax = 20\n",
    "smin = 0.00001\n",
    "gmax = 20\n",
    "gmin = 0.00001\n",
    "dmin = 0.0001\n",
    "dmax = 10\n",
    "omax = 0.1\n",
    "omin = -0.1\n",
    "rvmax = 60\n",
    "rvmin = 0\n",
    "\n",
    "\n",
    "n = 3 #number of lines \n",
    "theta = [rv, ci_4481, si_4481, gi_4481,di_4481 ,oi_4481, ci_4860, si_4860, gi_4860, di_4860, oi_4860, ci_5055, si_5055, gi_5055, di_5055, oi_5055]  \n",
    "\n",
    "#initilizing values\n",
    "#priors for the values\n",
    "#gaussian profile for line centre with the x as a function\n",
    "\n",
    "#first attempt to make gaussian priors, now doing inside the logprior function\n",
    "\"\"\" \n",
    "for j in c1:\n",
    "    gaussian = (norm.pdf(D, j, 0.04)) #make a prior function for the line \n",
    "    plt.plot(D,gaussian)\n",
    "    plt.show()\n",
    "    sum = np.sum(gaussian)\n",
    "    c_priors_list.append(sum)\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(l, theta):\n",
    "    #function to generate the model\n",
    "    #model = 1\n",
    "    model = np.ones_like(D)\n",
    "    model += theta[5] -  theta[4] * voigt_profile( (l-(theta[1]*(1+(theta[0])/299792.0)))  ,theta[2], theta[3])\n",
    "    model += theta[10] - theta[9] * voigt_profile( (l-(theta[6]*(1+(theta[0])/299792.0)))  ,theta[7], theta[8])\n",
    "    model += theta[15] - theta[14]* voigt_profile( (l-(theta[11]*(1+(theta[0])/299792.0)))  ,theta[12], theta[13])\n",
    "\n",
    "    return model\n",
    "\n",
    "#test plot to see how the model looks like compared to the spectrum \n",
    "for i in lines:\n",
    "    plt.plot(l, f(l,theta))\n",
    "    plt.plot(l,D)\n",
    "    plt.fill_between(l,f(l,theta) - sigma, f(l,theta) + sigma, color='gray', alpha=0.3)\n",
    "    plt.xlim(i -20,i +20)\n",
    "    plt.ylim(0.4,1.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def log_likelihood(theta, l, D,sigma):\n",
    "    residual = D - f(l, theta)\n",
    "    return -0.5 * np.sum((residual / sigma)**2 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "# Define the log prior function\n",
    "def log_prior(theta):\n",
    "    total_log_prior = 0\n",
    "    \n",
    "    #setting the logprior to False if outside the priors\n",
    "    if not (rvmin < theta[0] < rvmax):\n",
    "        return -np.inf\n",
    "    \n",
    "    if not (smin < theta[2] < smax) or not (smin < theta[7] < smax) or not (smin < theta[12] < smax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (gmin < theta[3] < gmax) or  not (gmin < theta[8] < gmax) or  not (gmin < theta[13] < gmax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (dmin < theta[4] < dmax) or not (dmin < theta[9] < dmax) or  not (dmin < theta[14] < dmax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (omin < theta[5] < omax) or  not (omin < theta[10] < omax) or   not (omin < theta[15] < omax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not ((ci_4481 - 0.1) < theta[1] < (ci_4481 + 0.1)) or not ((ci_4860 - 0.1) < theta[6] < (ci_4860 + 0.1)) or not ((ci_4481 - 0.1) < theta[11] < (ci_4481 +0.1)):\n",
    "         return -np.inf\n",
    "     \n",
    "    c4481_prior_logpdf = np.sum(norm.logpdf(theta[1], loc = ci_4481,scale = 0.001))\n",
    "    c4860_prior_logpdf = np.sum(norm.logpdf(theta[6], loc = ci_4860,scale = 0.001))\n",
    "    c5055_prior_logpdf = np.sum(norm.logpdf(theta[11], loc = ci_5055,scale = 0.001))\n",
    "    total_log_prior += c4481_prior_logpdf + c4860_prior_logpdf + c5055_prior_logpdf\n",
    "    \n",
    "        #restricting for bounds on parameters\n",
    "        \n",
    "    #adding a logprior function\n",
    "    \n",
    "    #prior_mu = norm.logpdf(theta[0], loc=0, scale=1)\n",
    "    \n",
    "    return total_log_prior\n",
    "\n",
    "# Define the log posterior function\n",
    "def log_probability(theta, l, D,sigma):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, l, D,sigma)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize walker positions using Latin hypercube sampling - also works - different way to initialize the sampling\n",
    "\"\"\" \n",
    "from pyDOE import lhs\n",
    "nwalkers = 64\n",
    "ndim = len(theta)  # Assuming n is defined elsewhere\n",
    "bounds = np.array([prior_min, prior_max]).T  # Define bounds for each parameter\n",
    "lhs_samples = lhs(ndim, samples=nwalkers)\n",
    "data = pd.DataFrame(lhs_samples)\n",
    "pos = bounds[:, 0] + lhs_samples * (bounds[:, 1] - bounds[:, 0])\n",
    " \"\"\"\n",
    "\n",
    " \n",
    "#initialise walkers and pos using normal gaussian distributions\n",
    "pos = (theta) + 1e-2 * np.random.randn(3*len(theta), len(theta))\n",
    "nwalkers, ndim = pos.shape \n",
    "\n",
    "\n",
    "\n",
    "# Run the sampler\n",
    "with Pool() as pool:\n",
    "\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(l,D,sigma),pool = pool)\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramnames = [[('RV', r'\\Delta')]] + [[(f'c{i}',f'c_{i}'),(f's{i}', f's_{i}'), (f'g{i}', f'g_{i}'),(f'd{i}',f'd_{i}'),(f'offset{i}',f'offset_{i}')] for i in range(n)] \n",
    "paramnames = [item for sublist in paramnames for item in sublist]\n",
    "\n",
    "# Get the samples\n",
    "samples = sampler.get_chain(discard=100, thin=1, flat=False)\n",
    "labels = [name_tuple[0] for name_tuple in paramnames]\n",
    "# Plot the traces\n",
    "\n",
    "#plotting all the variables\n",
    "\n",
    "fig, axes = plt.subplots(len(labels), figsize=(10, 10), sharex=True)\n",
    "#labels = [\"rv\",\"w1\",\"d1\",\"w2\",\"d2\",\"sigma\"]\n",
    "for i in range(len(labels)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "plt.show() \n",
    "\"\"\" \n",
    "fig, axes = plt.subplots(1, figsize=(10, 6), sharex=True)\n",
    "axes.plot(samples[:, :, 0], \"k\", alpha=0.3)\n",
    "axes.set_xlim(0, len(samples))\n",
    "axes.set_ylabel(\"RV (km/s)\")\n",
    "axes.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes.set_xlabel(\"step number\")\n",
    "plt.show() \"\"\"\n",
    "\n",
    "# Flatten the samples\n",
    "flat_samples = sampler.get_chain(discard=0, thin=15, flat=True)\n",
    "percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)  # This gives the 16th, 50th (median), and 84th percentiles\n",
    "\n",
    "\n",
    "print(percentiles)\n",
    "# Compute the medians of the parameter samples\n",
    "#variables = percentiles[1]\n",
    "variables = np.median(flat_samples, axis=0)\n",
    "\n",
    "print(variables)\n",
    "\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels,truths=[variables])\n",
    "\n",
    "# Annotate variables on the plot\n",
    "for i, var_value in enumerate(variables.items()):\n",
    "    plt.annotate(f\" {var_value:.5f}\", xy=(-1, 4.5 - i * 0.5), xycoords='axes fraction', fontsize=20, ha='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another attempt, two lines - no central variance of the lines - this is working!!\n",
    "\n",
    "#importing, cropping, and normalising data\n",
    "wav,flux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170714/product/mbgphH201707140017_u2wm.fits\")\n",
    "#rwav,rflux,rt,rstar = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphR201707110020_u2wm.fits\")\n",
    "#wav = np.concatenate((bwav,rwav))\n",
    "#flux = np.concatenate((bflux,rflux))\n",
    "#lower bound of window\n",
    "u_loc = np.searchsorted(wav, 5200) #5200\n",
    "closest_value = wav[max(0, u_loc-1)]\n",
    "u_bound = np.where(wav == closest_value)\n",
    "u_bound = int(u_bound[0])\n",
    "#find the lower bound of the window\n",
    "l_loc = np.searchsorted(wav, 4100) #4100\n",
    "closest_value = wav[max(0, l_loc-1)]\n",
    "l_bound = np.where(wav == closest_value)\n",
    "l_bound = int(l_bound[0])\n",
    "y = flux[l_bound:u_bound]\n",
    "l = wav[l_bound:u_bound]\n",
    "p_result = poly_fit(l,y)\n",
    "D = y/p_result\n",
    "sigma = calculate_error(D)/2\n",
    "\n",
    "\n",
    "lines = ([i[1] for i in b_lines])\n",
    "\n",
    "#initializing values\n",
    "\n",
    "rv = 36\n",
    "\n",
    "#First profile - Mg 4481 - initial values\n",
    "ci_4481 = 4481.185\n",
    "si_4481 = 0.3\n",
    "gi_4481 = 0.3\n",
    "di_4481 = 0.5\n",
    "oi_4481 = 0\n",
    "\n",
    "\n",
    "#Third profile - Si 5055\n",
    "ci_5055 = 5055.984\n",
    "si_5055 = 0.2\n",
    "gi_5055 = 0.2\n",
    "di_5055 = 0.3\n",
    "oi_5055 = 0\n",
    "\n",
    "#Vague general limits on the fits - \n",
    "#for absorption lines\n",
    "smax = 1\n",
    "smin = 0.00001\n",
    "gmax = 1\n",
    "gmin = 0.00001\n",
    "\n",
    "#for H lines\n",
    "hsmax = 20\n",
    "hsmin = 0.0001\n",
    "hgmax = 20\n",
    "hgmin = 0.0001\n",
    "\n",
    "dmin = 0.0001\n",
    "dmax = 1\n",
    "omax = 0.1\n",
    "omin = -0.1\n",
    "rvmax = 60\n",
    "rvmin = 0\n",
    "\n",
    "\n",
    "n = 2 #number of lines \n",
    "theta = [rv, si_4481, gi_4481,di_4481 ,oi_4481, si_5055, gi_5055, di_5055, oi_5055]  \n",
    "\n",
    "#initilizing values\n",
    "#priors for the values\n",
    "#gaussian profile for line centre with the x as a function\n",
    "\n",
    "#first attempt to make gaussian priors, now doing inside the logprior function\n",
    "\"\"\" \n",
    "for j in c1:\n",
    "    gaussian = (norm.pdf(D, j, 0.04)) #make a prior function for the line \n",
    "    plt.plot(D,gaussian)\n",
    "    plt.show()\n",
    "    sum = np.sum(gaussian)\n",
    "    c_priors_list.append(sum)\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(l, theta):\n",
    "    #function to generate the model\n",
    "    #model = 1\n",
    "    model = np.ones_like(D)\n",
    "    # offset - depth * Voigt profile(centre, sigma, gamma) \n",
    "    #With centre RV shifted \n",
    "    model += theta[4] -  theta[3] * voigt_profile( (l-(ci_4481*(1+(theta[0])/299792.0)))  ,theta[1], theta[2])\n",
    "    model += theta[8] - theta[7] * voigt_profile( (l-(ci_5055*(1+(theta[0])/299792.0)))  ,theta[5], theta[6])\n",
    "    #model += theta[12] - theta[11]* voigt_profile( (l-(ci_5055*(1+(theta[0])/299792.0)))  ,theta[9], theta[10]) if you add another line it goes here\n",
    "\n",
    "    return model\n",
    "\n",
    "#test plot to see how the model looks like compared to the spectrum \n",
    "for i in lines:\n",
    "    plt.plot(l, f(l,theta))\n",
    "    plt.plot(l,D)\n",
    "    plt.fill_between(l,f(l,theta) - sigma, f(l,theta) + sigma, color='gray', alpha=0.3)\n",
    "    plt.xlim(i -20,i +20)\n",
    "    plt.ylim(0.4,1.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def log_likelihood(theta, l, D):\n",
    "    residual = D - f(l, theta)\n",
    "    return -0.5 * np.sum((residual / sigma)**2 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "# Define the log prior function\n",
    "def log_prior(theta):\n",
    "    total_log_prior = 0\n",
    "    \n",
    "    #setting the logprior to False if outside the priors\n",
    "    if not (rvmin < theta[0] < rvmax):\n",
    "        return -np.inf\n",
    "    \n",
    "    if not (smin < theta[1] < smax) or not (smin < theta[5] < smax): #or not (smin < theta[9] < smax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (gmin < theta[2] < gmax) or  not (gmin < theta[6] < gmax): # or  not (gmin < theta[10] < gmax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (dmin < theta[3] < dmax) or not (dmin < theta[7] < dmax): # or  not (dmin < theta[11] < dmax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (omin < theta[4] < omax) or  not (omin < theta[8] < omax): # or  not (omin < theta[12] < omax):\n",
    "            return -np.inf\n",
    "    \"\"\"\n",
    "    if not ((ci_4481 - 0.1) < theta[1] < (ci_4481 + 0.1)) or not ((ci_4860 - 0.1) < theta[6] < (ci_4860 + 0.1)) or not ((ci_4481 - 0.1) < theta[11] < (ci_4481 +0.1)):\n",
    "         return -np.inf\n",
    "    \n",
    "    c4481_prior_logpdf = np.sum(norm.logpdf(theta[1], loc = ci_4481,scale = 0.04))\n",
    "    c4860_prior_logpdf = np.sum(norm.logpdf(theta[6], loc = ci_4860,scale = 0.04))\n",
    "    c5055_prior_logpdf = np.sum(norm.logpdf(theta[11], loc = ci_5055,scale = 0.04))\n",
    "    total_log_prior += c4481_prior_logpdf + c4860_prior_logpdf + c5055_prior_logpdf\n",
    "    \"\"\"\n",
    "    #restricting for bounds on parameters\n",
    "        \n",
    "    #adding a logprior function\n",
    "    \n",
    "    #prior_mu = norm.logpdf(theta[0], loc=0, scale=1)\n",
    "    \n",
    "    return total_log_prior\n",
    "\n",
    "# Define the log posterior function\n",
    "def log_probability(theta, l, D):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, l, D)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize walker positions using Latin hypercube sampling - also works - different way to initialize the sampling\n",
    "prior_min = np.array([rvmin] + [smin,gmin, dmin, omin]*n ) \n",
    "prior_max = np.array([rvmax] + [smax,gmax, dmax, omax]*n ) \n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "nwalkers = 3* len(theta)\n",
    "ndim = len(theta)  # Assuming n is defined elsewhere\n",
    "bounds = np.array([prior_min, prior_max]).T  # Define bounds for each parameter\n",
    "lhs_samples = lhs(ndim, samples=nwalkers)\n",
    "data = pd.DataFrame(lhs_samples)\n",
    "pos = bounds[:, 0] + lhs_samples * (bounds[:, 1] - bounds[:, 0])\n",
    "print(pos.shape)\n",
    "\n",
    "\n",
    " \n",
    "#initialise walkers and pos using normal gaussian distributions\n",
    "\"\"\" \n",
    "pos = (theta) + 1e-2 * np.random.randn(2*len(theta), len(theta))\n",
    "nwalkers, ndim = pos.shape \n",
    "print(pos.shape)\n",
    " \"\"\"\n",
    "\n",
    "# Run the sampler\n",
    "nsteps = 5000\n",
    "with Pool() as pool:\n",
    "\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(l,D),pool = pool)\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "    \n",
    "flat_samples = sampler.get_chain(discard=2000, thin=1, flat=True)\n",
    "percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)  # This gives the 16th, 50th (median), and 84th percentiles\n",
    "print(percentiles)\n",
    "rv_lerr = percentiles[0,0]\n",
    "rv = percentiles[1,0]\n",
    "rv_herr = percentiles[2,0]\n",
    "print(rv_lerr,rv,rv_herr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_samples = flat_samples[:, 0]\n",
    "rvp = np.percentile(rv_samples, [16, 50, 84])\n",
    "# Plot the probability density function (PDF) of the RV parameter\n",
    "plt.figure(figsize=(4, 6),dpi = 200)\n",
    "plt.hist(rv_samples, bins=30, density=True, color='skyblue', alpha=0.7)\n",
    "for percentile in rvp:\n",
    "    plt.axvline(percentile, color='red', linestyle='--', linewidth=1)\n",
    "    plt.text(percentile - 0.16, 0.3, f'{percentile:.2f}', rotation=90, verticalalignment='bottom', horizontalalignment='center', color='red')\n",
    "\n",
    "plt.xlabel('RV (km/s)')\n",
    "plt.ylabel('Probability Density')\n",
    "#plt.title('Probability Density Function (PDF) of RV Parameter')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "paramnames = [[('RV', r'\\Delta')]] + [[(f's{i}', f's_{i}'), (f'g{i}', f'g_{i}'),(f'd{i}',f'd_{i}'),(f'offset{i}',f'offset_{i}')] for i in range(n)] \n",
    "paramnames = [item for sublist in paramnames for item in sublist]\n",
    "\n",
    "# Get the samples\n",
    "samples = sampler.get_chain(discard=2000, thin=1, flat=False)\n",
    "labels = [name_tuple[0] for name_tuple in paramnames]\n",
    "# Plot the traces\n",
    "\n",
    "#plotting all the variables\n",
    "\n",
    "fig, axes = plt.subplots(len(labels), figsize=(10, 10), sharex=True,dpi = 200)\n",
    "#labels = [\"rv\",\"w1\",\"d1\",\"w2\",\"d2\",\"sigma\"]\n",
    "for i in range(len(labels)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "plt.show() \n",
    "\"\"\" \n",
    "fig, axes = plt.subplots(1, figsize=(10, 6), sharex=True)\n",
    "axes.plot(samples[:, :, 0], \"k\", alpha=0.3)\n",
    "axes.set_xlim(0, len(samples))\n",
    "axes.set_ylabel(\"RV (km/s)\")\n",
    "axes.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes.set_xlabel(\"step number\")\n",
    "plt.show() \"\"\"\n",
    "\n",
    "# Flatten the samples\n",
    "\n",
    "# Compute the medians of the parameter samples\n",
    "#variables = percentiles[1]\n",
    "variables = np.median(flat_samples, axis=0)\n",
    "\n",
    "for i in lines:\n",
    "    plt.plot(l, f(l,variables))\n",
    "    plt.plot(l,D)\n",
    "    plt.fill_between(l,f(l,variables) - sigma, f(l,variables) + sigma, color='gray', alpha=0.3)\n",
    "    plt.xlim(i -20,i +20)\n",
    "    plt.ylim(0.4,1.6)\n",
    "    plt.show()\n",
    "\n",
    "#fig = corner.corner(flat_samples, labels=labels,truths=[variables],dpi = 300)\n",
    "\n",
    "# Annotate variables on the plot\n",
    "#for i, var_value in enumerate(variables.items()):\n",
    "#    plt.annotate(f\" {var_value:.5f}\", xy=(-1, 4.5 - i * 0.5), xycoords='axes fraction', fontsize=20, ha='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another attempt, two lines - no central variance of the lines - attempting to get a prior on central value of line now\n",
    "#Working - iterating for everything\n",
    "\n",
    "\n",
    "rvs = []\n",
    "rvs_ul = []\n",
    "rvs_ll = []\n",
    "time_strings = []\n",
    "star = \"WD1929\"\n",
    "n = 0\n",
    "length = len(b_files)\n",
    "\n",
    "for num,i in enumerate(b_files):\n",
    "#importing, cropping, and normalising data\n",
    "    wav,flux,t,obj = Get_Wavelength_Flux_File(i)\n",
    "    if obj.startswith(star):\n",
    "        #\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170714/product/mbgphH201707140017_u2wm.fits\"\n",
    "        #rwav,rflux,rt,rstar = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphR201707110020_u2wm.fits\")\n",
    "        #wav = np.concatenate((bwav,rwav))\n",
    "        #flux = np.concatenate((bflux,rflux))\n",
    "        #lower bound of window\n",
    "        u_loc = np.searchsorted(wav, 5200) #5200\n",
    "        closest_value = wav[max(0, u_loc-1)]\n",
    "        u_bound = np.where(wav == closest_value)\n",
    "        u_bound = int(u_bound[0])\n",
    "        #find the lower bound of the window\n",
    "        l_loc = np.searchsorted(wav, 4100) #4100\n",
    "        closest_value = wav[max(0, l_loc-1)]\n",
    "        l_bound = np.where(wav == closest_value)\n",
    "        l_bound = int(l_bound[0])\n",
    "        y = flux[l_bound:u_bound]\n",
    "        l = wav[l_bound:u_bound]\n",
    "        p_result = poly_fit(l,y)\n",
    "        D = y/p_result\n",
    "        sigma = calculate_error(D)/2\n",
    "\n",
    "\n",
    "        lines = ([i[1] for i in b_lines])\n",
    "\n",
    "        #initializing values\n",
    "\n",
    "        rv = 36\n",
    "\n",
    "        #First profile - Mg 4481 - initial values\n",
    "        ci_4481 = 4481.185\n",
    "        si_4481 = 0.3\n",
    "        gi_4481 = 0.3\n",
    "        di_4481 = 0.5\n",
    "        oi_4481 = 0\n",
    "\n",
    "\n",
    "        #Second profile - Si 5055\n",
    "        ci_5055 = 5055.984\n",
    "        si_5055 = 0.2\n",
    "        gi_5055 = 0.2\n",
    "        di_5055 = 0.3\n",
    "        oi_5055 = 0\n",
    "\n",
    "        #Vague general limits on the fits - \n",
    "        #for absorption lines\n",
    "        smax = 1\n",
    "        smin = 0.00001\n",
    "        gmax = 1\n",
    "        gmin = 0.00001\n",
    "\n",
    "        #for H lines\n",
    "        hsmax = 20\n",
    "        hsmin = 0.0001\n",
    "        hgmax = 20\n",
    "        hgmin = 0.0001\n",
    "\n",
    "        dmin = 0.0001\n",
    "        dmax = 1\n",
    "        omax = 0.01\n",
    "        omin = -0.01\n",
    "        rvmax = 60\n",
    "        rvmin = 0\n",
    "\n",
    "\n",
    "        n = 2 #number of lines \n",
    "        theta = [rv, ci_4481,si_4481, gi_4481,di_4481 ,oi_4481,ci_5055, si_5055, gi_5055, di_5055, oi_5055]  \n",
    "\n",
    "        #initilizing values\n",
    "        #priors for the values\n",
    "        #gaussian profile for line centre with the x as a function\n",
    "\n",
    "        #first attempt to make gaussian priors, now doing inside the logprior function\n",
    "        \"\"\" \n",
    "        for j in c1:\n",
    "            gaussian = (norm.pdf(D, j, 0.04)) #make a prior function for the line \n",
    "            plt.plot(D,gaussian)\n",
    "            plt.show()\n",
    "            sum = np.sum(gaussian)\n",
    "            c_priors_list.append(sum)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def f(l, theta):\n",
    "            #function to generate the model\n",
    "            #model = 1\n",
    "            model = np.ones_like(D)\n",
    "            # offset - depth * Voigt profile(centre, sigma, gamma) \n",
    "            #With centre RV shifted \n",
    "            model += theta[5] -  theta[4] * voigt_profile( (l-(theta[1]*(1+(theta[0])/299792.0)))  ,theta[2], theta[3])\n",
    "            model += theta[10] - theta[9] * voigt_profile( (l-(theta[6]*(1+(theta[0])/299792.0)))  ,theta[7], theta[8])\n",
    "            #model += theta[12] - theta[11]* voigt_profile( (l-(ci_5055*(1+(theta[0])/299792.0)))  ,theta[9], theta[10]) if you add another line it goes here\n",
    "\n",
    "            return model\n",
    "        \n",
    "        #test plot to see how the model looks like compared to the spectrum \n",
    "        for i in lines:\n",
    "            plt.plot(l, f(l,theta))\n",
    "            plt.plot(l,D)\n",
    "            plt.fill_between(l,f(l,theta) - sigma, f(l,theta) + sigma, color='gray', alpha=0.3)\n",
    "            plt.xlim(i -20,i +20)\n",
    "            plt.ylim(0.4,1.6)\n",
    "            plt.show()\n",
    "         \n",
    "\n",
    "        def log_likelihood(theta, l, D):\n",
    "            residual = D - f(l, theta)\n",
    "            return -0.5 * np.sum((residual / sigma)**2 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "        # Define the log prior function\n",
    "        def log_prior(l,theta):\n",
    "            total_log_prior = 0\n",
    "            \n",
    "            #setting the logprior to False if outside the priors\n",
    "            if not (rvmin < theta[0] < rvmax):\n",
    "                return -np.inf\n",
    "            \n",
    "            if not (smin < theta[2] < smax) or not (smin < theta[7] < smax): #or not (smin < theta[9] < smax):\n",
    "                    return -np.inf\n",
    "            \n",
    "            if not (gmin < theta[3] < gmax) or  not (gmin < theta[8] < gmax): # or  not (gmin < theta[10] < gmax):\n",
    "                    return -np.inf\n",
    "            \n",
    "            if not (dmin < theta[4] < dmax) or not (dmin < theta[9] < dmax): # or  not (dmin < theta[11] < dmax):\n",
    "                    return -np.inf\n",
    "            \n",
    "            if not (omin < theta[5] < omax) or  not (omin < theta[10] < omax): # or  not (omin < theta[12] < omax):\n",
    "                    return -np.inf\n",
    "            \n",
    "            if not ((4481.15) < theta[1] < (4481.22)) or not ((5055.93) < theta[6] < (5056.08)): #or not ((ci_4481 - 0.1) < theta[11] < (ci_4481 +0.1)):\n",
    "                return -np.inf\n",
    "            \"\"\"\n",
    "            c4481_prior_logpdf = np.sum(norm.logpdf(theta[1], loc = ci_4481,scale = 0.04))\n",
    "            c4860_prior_logpdf = np.sum(norm.logpdf(theta[6], loc = ci_4860,scale = 0.04))\n",
    "            c5055_prior_logpdf = np.sum(norm.logpdf(theta[11], loc = ci_5055,scale = 0.04))\n",
    "            total_log_prior += c4481_prior_logpdf + c4860_prior_logpdf + c5055_prior_logpdf\n",
    "            \"\"\"\n",
    "            #restricting for bounds on parameters\n",
    "                \n",
    "            #adding a logprior function\n",
    "            \n",
    "            #prior_mu = norm.logpdf(theta[0], loc=0, scale=1)\n",
    "            \n",
    "            return total_log_prior\n",
    "\n",
    "        # Define the log posterior function\n",
    "        def log_probability(theta, l, D):\n",
    "            lp = log_prior(l,theta)\n",
    "            if not np.isfinite(lp):\n",
    "                return -np.inf\n",
    "            return lp + log_likelihood(theta, l, D)\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize walker positions using Latin hypercube sampling - also works - different way to initialize the sampling\n",
    "        prior_min = np.array([rvmin] + [4481.1,smin,gmin, dmin, omin] + [5055.98,smin,gmin, dmin, omin]) \n",
    "        prior_max = np.array([rvmax] + [4481.2, smax,gmax, dmax, omax] + [5056.03, smax,gmax, dmax, omax]) \n",
    "\n",
    "\n",
    "        from pyDOE import lhs\n",
    "        nwalkers = 3* len(theta)\n",
    "        ndim = len(theta)  # Assuming n is defined elsewhere\n",
    "        bounds = np.array([prior_min, prior_max]).T  # Define bounds for each parameter\n",
    "        lhs_samples = lhs(ndim, samples=nwalkers)\n",
    "        data = pd.DataFrame(lhs_samples)\n",
    "        pos = bounds[:, 0] + lhs_samples * (bounds[:, 1] - bounds[:, 0])\n",
    "        print(pos.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        #initialise walkers and pos using normal gaussian distributions\n",
    "        \"\"\" \n",
    "        pos = (theta) + 1e-2 * np.random.randn(2*len(theta), len(theta))\n",
    "        nwalkers, ndim = pos.shape \n",
    "        print(pos.shape)\n",
    "        \"\"\"\n",
    "        print(f\"{num} / {length} spectra\")\n",
    "        # Run the sampler\n",
    "        nsteps = 5000\n",
    "        with Pool() as pool:\n",
    "\n",
    "            sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(l,D),pool = pool)\n",
    "            sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "        flat_samples = sampler.get_chain(discard=2000, thin=1, flat=True)\n",
    "        percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)  # This gives the 16th, 50th (median), and 84th percentiles\n",
    "        print(percentiles)\n",
    "        rv_lerr = percentiles[0,0]\n",
    "        rv = percentiles[1,0]\n",
    "        rv_herr = percentiles[2,0]\n",
    "        #print(rv_lerr,rv,rv_herr)\n",
    "\n",
    "        rv_samples = flat_samples[:, 0]\n",
    "        rvp = np.percentile(rv_samples, [16, 50, 84])\n",
    "        # Plot the probability density function (PDF) of the RV parameter\n",
    "        plt.figure(figsize=(4, 6),dpi = 200)\n",
    "        plt.hist(rv_samples, bins=30, density=True, color='skyblue', alpha=0.7)\n",
    "        for percentile in rvp:\n",
    "            plt.axvline(percentile, color='red', linestyle='--', linewidth=1)\n",
    "            plt.text(percentile - 0.16, 0.3, f'{percentile:.2f}', rotation=90, verticalalignment='bottom', horizontalalignment='center', color='red')\n",
    "\n",
    "        plt.xlabel('RV (km/s)')\n",
    "        plt.ylabel('Probability Density')\n",
    "        #plt.title('Probability Density Function (PDF) of RV Parameter')\n",
    "        plt.show()\n",
    "\n",
    "        rvs.append((rv)) #-rv_corr\n",
    "        rvs_ul.append((rv_herr)) \n",
    "        rvs_ll.append((rv_lerr))\n",
    "        time_strings.append((t))\n",
    "    else:\n",
    "        print(\"This is a file for \" + str(obj) + \" instead of \" + str(star))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining data points that are the same times\n",
    "t_list = Time(time_strings, scale='utc')\n",
    "times = t_list.datetime \n",
    "t0 = times[0]\n",
    "tseconds = np.array([(dt - t0).total_seconds() for dt in times])\n",
    "tdays = tseconds * 1/(24*3600)\n",
    "mean = np.nanmean(rvs)\n",
    "print(mean)\n",
    "\n",
    "\n",
    "tarray = np.array(tdays)\n",
    "rvarray = np.array(rvs)\n",
    "rvuerrsarray = np.array(rvs_ul)\n",
    "rvlerrsarray = np.array(rvs_ll)\n",
    "#rverrsarray = np.array(rverrs)\n",
    "#rvcorrsarray = np.array(rv_corrs)\n",
    "#rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "filtered_indices = np.where(( rvarray > 34 ) & ( rvarray < 38 ))[0] #& (rvarray > 10)\n",
    "filtered_rvarray = rvarray[filtered_indices]\n",
    "filtered_tarray = tarray[filtered_indices]\n",
    "filtered_rvuerrsarray = rvuerrsarray[filtered_indices]\n",
    "filtered_rvlerrsarray = rvlerrsarray[filtered_indices]\n",
    "#filtered_rvcorrsarray = rvcorrsarray[filtered_indices]\n",
    "\n",
    "filtered_mean = np.nanmean(filtered_rvarray)\n",
    "print(filtered_mean)\n",
    "unique_times = np.unique(filtered_tarray)\n",
    "averages_unfiltered = np.array([tarray, rvarray, rvlerrsarray,rvuerrsarray])\n",
    "\n",
    "averages = np.array([[t, np.nanmean(filtered_rvarray[filtered_tarray == t]), np.nanmean(filtered_rvlerrsarray[filtered_tarray == t]),np.nanmean(filtered_rvuerrsarray[filtered_tarray == t])] for t in unique_times])\n",
    "#averages = averages[np.isnan(averages[:,1])]\n",
    "#averages = np.array([times,rvarray,rvlerrsarray,rvuerrsarray])\n",
    "print(averages)#\n",
    "\n",
    "np.savetxt(\"/data/wdplanetary/omri/Output/resultfiles/bayesian/first_try_two_lines.txt\",averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramnames = [[('RV', r'\\Delta')]] + [[(f'c{i}', f'c_{i}'), (f's{i}', f's_{i}'), (f'g{i}', f'g_{i}'),(f'd{i}',f'd_{i}'),(f'offset{i}',f'offset_{i}')] for i in range(n)] \n",
    "paramnames = [item for sublist in paramnames for item in sublist]\n",
    "\n",
    "# Get the samples\n",
    "samples = sampler.get_chain(discard=2000, thin=1, flat=False)\n",
    "labels = [name_tuple[0] for name_tuple in paramnames]\n",
    "# Plot the traces\n",
    "\n",
    "#plotting all the variables\n",
    "\n",
    "fig, axes = plt.subplots(len(labels), figsize=(10, 10), sharex=True)\n",
    "#labels = [\"rv\",\"w1\",\"d1\",\"w2\",\"d2\",\"sigma\"]\n",
    "for i in range(len(labels)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "plt.show() \n",
    "\"\"\" \n",
    "fig, axes = plt.subplots(1, figsize=(10, 6), sharex=True)\n",
    "axes.plot(samples[:, :, 0], \"k\", alpha=0.3)\n",
    "axes.set_xlim(0, len(samples))\n",
    "axes.set_ylabel(\"RV (km/s)\")\n",
    "axes.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes.set_xlabel(\"step number\")\n",
    "plt.show() \"\"\"\n",
    "\n",
    "# Flatten the samples\n",
    "\n",
    "# Compute the medians of the parameter samples\n",
    "#variables = percentiles[1]\n",
    "variables = np.median(flat_samples, axis=0)\n",
    "\n",
    "for i in lines:\n",
    "    plt.plot(l, f(l,variables))\n",
    "    plt.plot(l,D)\n",
    "    plt.fill_between(l,f(l,variables) - sigma, f(l,variables) + sigma, color='gray', alpha=0.3)\n",
    "    plt.xlim(i -20,i +20)\n",
    "    plt.ylim(0.4,1.6)\n",
    "    plt.show()\n",
    "\n",
    "fig = corner.corner(flat_samples, labels=labels,truths=[variables])\n",
    "\n",
    "# Annotate variables on the plot\n",
    "for i, var_value in enumerate(variables.items()):\n",
    "    plt.annotate(f\" {var_value:.5f}\", xy=(-1, 4.5 - i * 0.5), xycoords='axes fraction', fontsize=20, ha='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AAttempt with all the abs lines - crazy amount of parameters\n",
    "from multiprocessing import Pool\n",
    "#importing, cropping, and normalising data\n",
    "bwav,bflux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170714/product/mbgphH201707140017_u2wm.fits\")\n",
    "rwav,rflux,rt,rstar = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphR201707110020_u2wm.fits\")\n",
    "wav = np.concatenate((bwav,rwav))\n",
    "flux = np.concatenate((bflux,rflux))\n",
    "#lower bound of window\n",
    "u_loc = np.searchsorted(wav, 7000) #5200\n",
    "closest_value = wav[max(0, u_loc-1)]\n",
    "u_bound = np.where(wav == closest_value)\n",
    "u_bound = int(u_bound[0])\n",
    "#find the lower bound of the window\n",
    "l_loc = np.searchsorted(wav, 4100) #4100\n",
    "closest_value = wav[max(0, l_loc-1)]\n",
    "l_bound = np.where(wav == closest_value)\n",
    "l_bound = int(l_bound[0])\n",
    "y = flux[l_bound:u_bound]\n",
    "l = wav[l_bound:u_bound]\n",
    "p_result = poly_fit(l,y)\n",
    "D = y/p_result\n",
    "sigma = calculate_error(D)/2\n",
    "\n",
    "blines = ([i[1] for i in b_lines])\n",
    "rlines = ([i[1] for i in r_lines])\n",
    "lines = blines + rlines\n",
    "print(lines)\n",
    "#initializing values\n",
    "\n",
    "rv = 36\n",
    "\n",
    "#First profile - Mg 4481 - initial values\n",
    "ci_4481 = 4481.185\n",
    "si_4481 = 0.3\n",
    "gi_4481 = 0.3\n",
    "di_4481 = 0.5\n",
    "oi_4481 = 0\n",
    "\n",
    "#Second profile - Si 5055\n",
    "ci_5055 = 5055.984\n",
    "si_5055 = 0.3\n",
    "gi_5055 = 0.3\n",
    "di_5055 = 0.5\n",
    "oi_5055 = 0\n",
    "\n",
    "ci_5957 = 5957.56\n",
    "si_5957 = 0.3\n",
    "gi_5957 = 0.3\n",
    "di_5957 = 0.5\n",
    "oi_5957 = 0\n",
    "\n",
    "ci_5978 = 5978.93\n",
    "si_5978 = 0.3\n",
    "gi_5978 = 0.3\n",
    "di_5978 = 0.5\n",
    "oi_5978 = 0\n",
    "\n",
    "ci_6347 = 6347.100\n",
    "si_6347 = 0.3\n",
    "gi_6347 = 0.3\n",
    "di_6347 = 0.5\n",
    "oi_6347 = 0\n",
    "\n",
    "ci_6371 = 6371.360\n",
    "si_6371 = 0.3\n",
    "gi_6371 = 0.3\n",
    "di_6371 = 0.5\n",
    "oi_6371 = 0\n",
    "\n",
    "\n",
    "\n",
    "#Vague general limits on the fits - \n",
    "#for absorption lines\n",
    "smax = 1\n",
    "smin = 0.00001\n",
    "gmax = 1\n",
    "gmin = 0.00001\n",
    "\n",
    "#for H lines\n",
    "hsmax = 20\n",
    "hsmin = 0.0001\n",
    "hgmax = 20\n",
    "hgmin = 0.0001\n",
    "\n",
    "dmin = 0.001\n",
    "dmax = 1\n",
    "omax = 0.1\n",
    "omin = -0.1\n",
    "rvmax = 60\n",
    "rvmin = 0\n",
    "\n",
    "\n",
    "n = 6 #number of lines \n",
    "theta = [rv, si_4481, gi_4481,di_4481 ,oi_4481, si_5055, gi_5055, di_5055, oi_5055,si_5957, gi_5957, di_5957, oi_5957,si_5978, gi_5978, di_5978, oi_5978, si_6347, gi_6347, di_6347, oi_6347,si_6371, gi_6371, di_6371, oi_6371 ]  \n",
    "\n",
    "#initilizing values\n",
    "#priors for the values\n",
    "#gaussian profile for line centre with the x as a function\n",
    "\n",
    "#first attempt to make gaussian priors, now doing inside the logprior function\n",
    "\"\"\" \n",
    "for j in c1:\n",
    "    gaussian = (norm.pdf(D, j, 0.04)) #make a prior function for the line \n",
    "    plt.plot(D,gaussian)\n",
    "    plt.show()\n",
    "    sum = np.sum(gaussian)\n",
    "    c_priors_list.append(sum)\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(l, theta):\n",
    "    #function to generate the model\n",
    "    #model = 1\n",
    "    model = np.ones_like(D)\n",
    "    # offset - depth * Voigt profile(centre, sigma, gamma) \n",
    "    #With centre RV shifted \n",
    "    \n",
    "    model += theta[4] -  theta[3] * voigt_profile( (l-(ci_4481*(1+(theta[0])/299792.0)))  ,theta[1], theta[2])\n",
    "    model += theta[8] - theta[7] * voigt_profile( (l-(ci_5055*(1+(theta[0])/299792.0)))  ,theta[5], theta[6])\n",
    "    model += theta[12] - theta[11]* voigt_profile( (l-(ci_5957*(1+(theta[0])/299792.0)))  ,theta[9], theta[10]) #if you add another line it goes here\n",
    "    model += theta[16] - theta[15]* voigt_profile( (l-(ci_5978*(1+(theta[0])/299792.0)))  ,theta[13], theta[14])\n",
    "    model += theta[20] - theta[19]* voigt_profile( (l-(ci_5978*(1+(theta[0])/299792.0)))  ,theta[17], theta[18])\n",
    "    model += theta[24] - theta[23]* voigt_profile( (l-(ci_5978*(1+(theta[0])/299792.0)))  ,theta[21], theta[22])\n",
    "    return model\n",
    "\n",
    "#test plot to see how the model looks like compared to the spectrum \n",
    "for i in lines:\n",
    "    plt.plot(l, f(l,theta))\n",
    "    plt.plot(l,D)\n",
    "    plt.fill_between(l,f(l,theta) - sigma, f(l,theta) + sigma, color='gray', alpha=0.3)\n",
    "    plt.xlim(i -20,i +20)\n",
    "    plt.ylim(0.4,1.6)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def log_likelihood(theta, l, D):\n",
    "    residual = D - f(l, theta)\n",
    "    return -0.5 * np.sum((residual / sigma)**2 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "# Define the log prior function\n",
    "def log_prior(theta):\n",
    "    total_log_prior = 0\n",
    "    \n",
    "    #setting the logprior to False if outside the priors\n",
    "    if not (rvmin < theta[0] < rvmax):\n",
    "        return -np.inf\n",
    "    \n",
    "    if not (smin < theta[1] < smax) or not (smin < theta[5] < smax) or not (smin < theta[9] < smax) or not (smin < theta[13] < smax) or not (smin < theta[17] < smax) or not (smin < theta[21] < smax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (gmin < theta[2] < gmax) or not (gmin < theta[6] < gmax) or not (gmin < theta[10] < gmax) or not (gmin < theta[14] < gmax) or not (gmin < theta[18] < gmax) or not (gmin < theta[22] < gmax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (dmin < theta[3] < dmax) or not (dmin < theta[7] < dmax) or not (dmin < theta[11] < dmax) or not (dmin < theta[15] < dmax) or not (dmin < theta[19] < dmax) or not (dmin < theta[23] < dmax):\n",
    "            return -np.inf\n",
    "    \n",
    "    if not (omin < theta[4] < omax) or not (omin < theta[8] < omax) or  not (omin < theta[12] < omax) or not (omin < theta[16] < omax) or not (omin < theta[20] < omax) or not (omin < theta[24] < omax):\n",
    "            return -np.inf\n",
    "    \"\"\"\n",
    "    if not ((ci_4481 - 0.1) < theta[1] < (ci_4481 + 0.1)) or not ((ci_4860 - 0.1) < theta[6] < (ci_4860 + 0.1)) or not ((ci_4481 - 0.1) < theta[11] < (ci_4481 +0.1)):\n",
    "         return -np.inf\n",
    "    \n",
    "    c4481_prior_logpdf = np.sum(norm.logpdf(theta[1], loc = ci_4481,scale = 0.04))\n",
    "    c4860_prior_logpdf = np.sum(norm.logpdf(theta[6], loc = ci_4860,scale = 0.04))\n",
    "    c5055_prior_logpdf = np.sum(norm.logpdf(theta[11], loc = ci_5055,scale = 0.04))\n",
    "    total_log_prior += c4481_prior_logpdf + c4860_prior_logpdf + c5055_prior_logpdf\n",
    "    \"\"\"\n",
    "    #restricting for bounds on parameters\n",
    "        \n",
    "    #adding a logprior function\n",
    "    \n",
    "    #prior_mu = norm.logpdf(theta[0], loc=0, scale=1)\n",
    "    \n",
    "    return total_log_prior\n",
    "\n",
    "# Define the log posterior function\n",
    "def log_probability(theta, l, D):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, l, D)\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "# Initialize walker positions using Latin hypercube sampling - also works - different way to initialize the sampling\n",
    "prior_min = np.array([rvmin] + [smin,gmin, dmin, omin]*n ) \n",
    "prior_max = np.array([rvmax] + [smax,gmax, dmax, omax]*n ) \n",
    "\n",
    "\n",
    "from pyDOE import lhs\n",
    "nwalkers = 3* len(theta)\n",
    "ndim = len(theta)  # Assuming n is defined elsewhere\n",
    "bounds = np.array([prior_min, prior_max]).T  # Define bounds for each parameter\n",
    "lhs_samples = lhs(ndim, samples=nwalkers)\n",
    "data = pd.DataFrame(lhs_samples)\n",
    "pos = bounds[:, 0] + lhs_samples * (bounds[:, 1] - bounds[:, 0])\n",
    "print(pos.shape)\n",
    " \"\"\"\n",
    "\n",
    " \n",
    "#initialise walkers and pos using normal gaussian distributions\n",
    "\n",
    "pos = (theta) + 1e-2 * np.random.randn(3*len(theta), len(theta))\n",
    "nwalkers, ndim = pos.shape \n",
    "print(pos.shape)\n",
    "\n",
    "\n",
    "# Run the sampler\n",
    "\n",
    "nsteps = 5000\n",
    "with Pool() as pool:\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(l,D),pool=pool)\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "flat_samples = sampler.get_chain(discard=2000, thin=1, flat=True)\n",
    "percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)  # This gives the 16th, 50th (median), and 84th percentiles\n",
    "print(percentiles)\n",
    "rv_lerr = percentiles[0,0]\n",
    "rv = percentiles[1,0]\n",
    "rv_herr = percentiles[2,0]\n",
    "print(rv_lerr,rv,rv_herr)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
