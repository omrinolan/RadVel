{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radial velocity shifts from SALT spectral data imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Open the PNG file\n",
    "img = Image.open(\"/data/wdplanetary/omri/Output/Gaussianline/blue/MgII_4481/MgII_4481_2020-07-30 00:00:00.png\")\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import numpy as np\n",
    "import astropy.io.fits as fits\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.modeling import models,fitting\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.stats import norm\n",
    "from scipy.special import voigt_profile\n",
    "from scipy import signal\n",
    "from scipy.signal import correlate\n",
    "from PyAstronomy import pyasl\n",
    "from scipy.optimize import curve_fit\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, EarthLocation\n",
    "from astropy.constants import c\n",
    "import astropy.io.fits as fits\n",
    "from astropy.units import dimensionless_unscaled\n",
    "from lmfit.models import LinearModel, GaussianModel, VoigtModel, PolynomialModel\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "import fnmatch\n",
    "import os\n",
    "from astropy.timeseries import LombScargle\n",
    "from plotnine import *\n",
    "import traceback\n",
    "import sys\n",
    "import linecache\n",
    "import textwrap\n",
    "import re\n",
    "import emcee\n",
    "import corner\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Mike data function\n",
    "\n",
    "\n",
    "def read_mike_spec(filename):\n",
    "    \n",
    "    \n",
    "      \n",
    "    hdulist = fits.open(filename)\n",
    "    obj_fits = hdulist[0]\n",
    "    header = obj_fits.header\n",
    "    date_obs = header.get('UT-DATE', None)\n",
    "    time_obs = header.get('UT-START',None)\n",
    "    \n",
    "    OBJECT = header.get(\"OBJECT\", None)\n",
    "   \n",
    "    sky_spec = obj_fits.data[0,:,:]\n",
    "    obj_spec = obj_fits.data[1,:,:]\n",
    "    noi_spec = obj_fits.data[2,:,:]\n",
    "    snr_spec = obj_fits.data[3,:,:]\n",
    "    lamp_spec = obj_fits.data[4,:,:]\n",
    "    flat_spec = obj_fits.data[5,:,:]\n",
    "    nobj_spec = obj_fits.data[6,:,:]\n",
    "    hdulist.close()\n",
    "\n",
    "    # Ca H and K are found in orders 18-22 which are indexed 17-21. Over these\n",
    "    # 4 order, the extracted Ca H and K can be combined to further reduce noise\n",
    "   \n",
    "    i = 0\n",
    "    found = -1\n",
    "   \n",
    "    while (found == -1) and (i < len(header)):\n",
    "        found = str(header[i]).find('spec1')\n",
    "        i += 1\n",
    "   \n",
    "    all_order_wav = str(header[(i-1)::])\n",
    "    all_order_wav = '\\''+all_order_wav\n",
    "   \n",
    "    #The following loop can be used to remove all the header keys from the string\n",
    "    i = 1\n",
    "    match = True\n",
    "    while match:\n",
    "        string = '\\'WAT2_%03d= \\'' % i\n",
    "        wat = re.compile(string)\n",
    "        match = wat.search(all_order_wav)\n",
    "        all_order_wav = wat.sub('',all_order_wav)\n",
    "        i +=1\n",
    "   \n",
    "    wavelength_soln_data = []\n",
    "   \n",
    "    #This loop parses the wavelength solns by the spec keyword and stores the entire\n",
    "    #solution as a list of lists, each entry containing one place for each solved\n",
    "    #parameter.\n",
    "    i = 1\n",
    "    match = True\n",
    "    while match:\n",
    "        string = 'spec%d' %i\n",
    "        spec_key = re.compile(string)\n",
    "        match = spec_key.search(all_order_wav)\n",
    "        if match:\n",
    "            wavelength_soln_data.append((all_order_wav[(match.start()+10):match.start()+91]).split())\n",
    "        i+=1\n",
    "   \n",
    "    #This loop generates an array of arrays with each array containing the wavelength\n",
    "    #soln.\n",
    "    wav_solns = []\n",
    "    for i in range(len(wavelength_soln_data)):\n",
    "        wav_start = float(wavelength_soln_data[i][3])\n",
    "        wav_delta = float(wavelength_soln_data[i][4])\n",
    "        wav_end = wav_start + wav_delta*float(wavelength_soln_data[i][5])\n",
    "        wav_solns.append(np.arange(wav_start,wav_end,wav_delta))\n",
    "        i+=1\n",
    "    #    \n",
    "    #    plt.figure(1)\n",
    "    #    plt.clf()\n",
    "    #    for i in range(len(wav_solns)):\n",
    "    #        plt.plot(wav_solns[i],nobj_spec[i,:])\n",
    "       \n",
    "    all_data = np.zeros([len(wav_solns),9],dtype=object)\n",
    "    for i in range(len(wav_solns)):\n",
    "        all_data[i,0] = i+1\n",
    "        all_data[i,1] = wav_solns[i]\n",
    "        all_data[i,2] = sky_spec[i,:]\n",
    "        all_data[i,3] = obj_spec[i,:]\n",
    "        all_data[i,4] = noi_spec[i,:]\n",
    "        all_data[i,5] = snr_spec[i,:]\n",
    "        all_data[i,6] = lamp_spec[i,:]\n",
    "        all_data[i,7] = flat_spec[i,:]\n",
    "        all_data[i,8] = nobj_spec[i,:]\n",
    "    keywords = ['Order','Wavelength Soln', 'Sky Spectrum', 'Object Spectrum',\n",
    "                'Noise Spectrum', 'SNR Spectrum', 'Lamp Spectrum', 'Flat Spectrum',\n",
    "                'Normalized Object Spectrum']\n",
    "\n",
    "\n",
    "    #First, calculating the heliocentric correction\n",
    "\n",
    "    site_lat = header.get(\"SITELAT\",None)\n",
    "    site_long = header.get(\"SITELONG\",None)\n",
    "    site_alt = header.get(\"SITEALT\",None)\n",
    "    ra = header.get(\"RA-D\",None)  # in degrees\n",
    "    dec = header.get(\"DEC-D\", None)  # in degrees\n",
    "    epoch = str(header.get(\"EPOCH\", None))  # e.g., 'J2000'\n",
    "    time = Time(str(date_obs) + 'T' + str(time_obs), format='fits',scale = \"utc\")\n",
    "    \n",
    "    #print(time)\n",
    "    # Create EarthLocation object\n",
    "    location = EarthLocation.from_geodetic(lat=site_lat*u.deg, lon=site_long*u.deg, height=site_alt*u.m)\n",
    "    sc = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)\n",
    "    heliocorr = sc.radial_velocity_correction('heliocentric', obstime=time, location=location) \n",
    "    #print(heliocorr.to(u.km/u.s)) \n",
    "    heliocorr = (heliocorr.to(u.m/u.s) /c)\n",
    "\n",
    "\n",
    "    \n",
    "    #Now making easy arrays to use:\n",
    "    wav = []\n",
    "    order= []\n",
    "    flux = []\n",
    "    snr = []\n",
    "    for i in range(len(all_data)):\n",
    "        wav.append(all_data[i,1])\n",
    "        order.append(all_data[i,0])\n",
    "        flux.append(all_data[i,8])\n",
    "        snr.append(all_data[i,5])\n",
    "\n",
    "\n",
    "    #Now apply the heliocentric correction to the wavelength data:\n",
    "    for i in range(len(wav)):\n",
    "        for j in range(len(wav[i])):\n",
    "            wav[i][j] *= (1 + heliocorr)\n",
    "\n",
    "        #heliocentric correction\n",
    "     #   wavelength[i] = (CRVAL1 + CDELT1*i) + (HEL_COR/299792 )*(CRVAL1 + CDELT1*i)\n",
    "        #wavelength[i] = (CRVAL1 + CDELT1*i)\n",
    "    \n",
    "    return order,wav,flux,snr, OBJECT,time\n",
    "\n",
    "\n",
    "\n",
    "#filename = \"/data/wdplanetary/laura/MIKE/Data/WD1929+011/blue/galex1931_blue_2011-06-09.fits\"\n",
    "\"\"\" order,wav,flux,snr,OBJECT,time = read_mike_spec(filename)\n",
    "long_string = str(header)\n",
    "wrapped_text = textwrap.wrap(long_string, width=80)\n",
    "for line in wrapped_text:\n",
    "    print(line)\n",
    "plt.figure()\n",
    "plt.plot(wav[8],flux[8])\n",
    "plt.xlim(4475,4485)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wav[8],snr[8])\n",
    "plt.xlim(4475,4485)\n",
    "plt.show() \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salt read in data\n",
    "def Get_Wavelength_Flux_File(filename) :\n",
    "    # Get the wavelength and flux from a MIDAS pipeline file\n",
    "    hdulist = fits.open(filename)\n",
    "    header = hdulist[0].header\n",
    "    date_obs = header.get('DATE-OBS', None)\n",
    "    time_obs = header.get('TIME-OBS',None)\n",
    "    time = Time(str(date_obs) + 'T' + str(time_obs), format='fits')\n",
    "    #time = Time(date_obs,format = \"fits\")\n",
    "    \n",
    "    flux = hdulist[0].data # flux in counts \n",
    "    \n",
    "    CRVAL1 = hdulist[0].header['CRVAL1'] # Coordinate at reference pixel\n",
    "    CRPIX1 = hdulist[0].header['CRPIX1'] # Reference pixel\n",
    "    CDELT1 = hdulist[0].header['CDELT1'] # Coordinate increase per pixel\n",
    "    HEL_COR = hdulist[0].header['HEL_COR'] # Heliocentric correction\n",
    "    OBJECT = str(header.get(\"OBJECT\", None))\n",
    "    \n",
    "    # Need to use the info above to make the wavelength axis\n",
    "    wavelength = np.zeros(len(flux))\n",
    "    \n",
    "    for i in range(len(wavelength)) :\n",
    "        #heliocentric correction\n",
    "        wavelength[i] = (CRVAL1 + CDELT1*i) + (HEL_COR/299792 )*(CRVAL1 + CDELT1*i)\n",
    "        #wavelength[i] = (CRVAL1 + CDELT1*i)\n",
    "    \n",
    "    hdulist.close()\t\n",
    "\n",
    "    return wavelength, flux, time, OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_noise = []\n",
    "\n",
    "#dictionary for all the inter-order noise areas for the SALT blue spectrum\n",
    "#manually removing them\n",
    "io_noise.append((3890,3910))\n",
    "io_noise.append((3890,3910))\n",
    "io_noise.append((3934,3945))\n",
    "io_noise.append((3960,3972))\n",
    "io_noise.append((3995,4010))\n",
    "io_noise.append((4030,4040))\n",
    "io_noise.append((4063,4075))\n",
    "io_noise.append((4103,4115))\n",
    "io_noise.append((4140,4150))\n",
    "io_noise.append((4175,4185))\n",
    "io_noise.append((4175,4185))\n",
    "io_noise.append((4215,4225))\n",
    "io_noise.append((4175,4185))\n",
    "io_noise.append((4255,4265))\n",
    "io_noise.append((4290,4305))\n",
    "io_noise.append((4330,4345))\n",
    "io_noise.append((4370,4382))\n",
    "io_noise.append((4415,4428))\n",
    "io_noise.append((4455,4470))\n",
    "io_noise.append((4495,4510))\n",
    "io_noise.append((4545,4555))\n",
    "io_noise.append((4455,4470))\n",
    "io_noise.append((4585,4600))\n",
    "io_noise.append((4633,4650))\n",
    "io_noise.append((4680,4690))\n",
    "io_noise.append((4728,4740))\n",
    "io_noise.append((4777,4790))\n",
    "io_noise.append((4826,4835))\n",
    "io_noise.append((4878,4888))\n",
    "io_noise.append((4928,4938))\n",
    "io_noise.append((4980,4988))\n",
    "io_noise.append((5032,5050))\n",
    "io_noise.append((5080,5100))\n",
    "io_noise.append((5148,5158))\n",
    "io_noise.append((5208,5215))\n",
    "io_noise.append((5265,5275))\n",
    "io_noise.append((5325,5335))\n",
    "io_noise.append((5325,5335))\n",
    "io_noise.append((5385,5395))\n",
    "io_noise.append((5400,5600)) #big cut off for second channel (red)\n",
    "io_noise.append((5625,5635))\n",
    "io_noise.append((5695,5705))\n",
    "io_noise.append((5765,5779))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdulist = fits.open(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "header = hdulist[0].header\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line dictionaries: Name, position(A), present?, log gf\n",
    "p_lines = []\n",
    "spec_lines = []\n",
    "\n",
    "#pollutant lines\n",
    "p_lines.append((\"CaII_3933\", 3933.663, False)) #only use for MIKE data\n",
    "p_lines.append((\"CaII_4226\",4226.727, False)) #good\n",
    "p_lines.append((\"FeII_4923\",4923.927, False)) \n",
    "p_lines.append((\"FeII_5018\",5018.440, False))\n",
    "p_lines.append((\"FeII_5169\",5169.033, False)) #this one gives an error when trying to fit it\n",
    "p_lines.append((\"SiII_5041\",5041.024, False)) #don't use this\n",
    "p_lines.append((\"SiII_5055\",5055.984, False)) #This one is good use it\n",
    "p_lines.append((\"SiII_5957\",5957.560, False))\n",
    "p_lines.append((\"SiII_5978\",5978.930, False))\n",
    "p_lines.append((\"SiII_6347\",6347.100, False)) #these two are quite strong in WD1929+012\n",
    "p_lines.append((\"SiII_6371\",6371.360, False)) #\n",
    "p_lines.append((\"MgI_5172\",5172.683, False))\n",
    "p_lines.append((\"MgI_5183\",5183.602, False))\n",
    "p_lines.append((\"MgII_4481\",4481.130,False)) #strong magnesium line\n",
    "p_lines.append((\"MgII_4481_2\",4481.327, False))\n",
    "p_lines.append((\"MgII_4481\",4481.185,True)) #weighted combination of mg_4481 lines\n",
    "\n",
    "p_lines.append((\"MgII_7877\",7877.054, False)) \n",
    "p_lines.append((\"MgII_7896\",7896.366, False)) #definitely not present\n",
    "p_lines.append((\"OI_7771\",7771.944, False)) #definitely not present\n",
    "#hydrogen lines\n",
    "p_lines.append((\"H_4860\",4860.680, False))\n",
    "p_lines.append((\"H_4860_2\",4860.968, False))#This one gives better values\n",
    "p_lines.append((\"H_4340\",4340.472,False)) #present\n",
    "p_lines.append((\"H_6563\",6562.79 ,False)) #not present in the 2020 spectra?\n",
    "#pick the spectral lines present in this white dwarf\n",
    "\n",
    "for i in p_lines:\n",
    "    if i[2] == True:\n",
    "        spec_lines.append(i)\n",
    "\n",
    "b_lines = []\n",
    "r_lines = []\n",
    "for i in spec_lines:\n",
    "    if i[1] <= 5550:\n",
    "        #370 - 555 nm\n",
    "        b_lines.append(i)\n",
    "    else:\n",
    "        #555 - 890 nm\n",
    "        r_lines.append(i)\n",
    "\n",
    "#Now define the sky lines that we will use to find the stability corrections from the instrument variability\n",
    "\n",
    "sky_lines = []\n",
    "sky_lines.append((\"OI_5577\",5577.340))\n",
    "sky_lines.append((\"OI_6300\",6300.304))\n",
    "sky_lines.append((\"OI_6364\",6363.776))\n",
    "#sky_lines.append((\"H2O_7392\"))\n",
    "#sky_lines.append((\"H2O_8365\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in b_files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing module for salt files\n",
    "for i in b_files:\n",
    "   wav,flux,time,OBJECT = Get_Wavelength_Flux_File(f\"{i}\")\n",
    "   plt.figure(facecolor=\"white\")\n",
    "   plt.plot(wav, flux, color='black', linewidth=0.5)\n",
    "   plt.title(f\"{i}\")\n",
    "   plt.xlim(5050,5060)\n",
    "   plt.ylim(0,0.02)\n",
    "   plt.show\n",
    "\n",
    "\n",
    "hdulist = fits.open(\"/data/wdplanetary/omri/Data/WD1929+012/2019-2-SCI-049.20210420-2/product/mbgphR202104200024_uwm.fits\")\n",
    "header = hdulist[0].header\n",
    "\n",
    "\n",
    "long_string = str(header)\n",
    "wrapped_text = textwrap.wrap(long_string, width=80)\n",
    "for line in wrapped_text:\n",
    "   print(line)\n",
    "\n",
    "date_obs = header.get('DATE-OBS', None)\n",
    "time = Time(date_obs, format='fits')\n",
    "star = header.get(\"OBJECT\", None)\n",
    "print(str(star))\n",
    "#print(time)\n",
    "hdulist.close()\n",
    "\n",
    "# Print the UTC representation of the time\n",
    "#print(f\"Observation Date and Time (UTC): {time.utc.iso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process the inputted wavelength and flux data, \n",
    "#making binary masks, moving averagses, normalising the continuum\n",
    "\n",
    "def process_data_gaussian(wav,flux):\n",
    "    \n",
    "    #make a new wavelength filter that is accurate to 3dp\n",
    "    min = wav[np.argmin(wav)]\n",
    "    max = wav[np.argmax(wav)]\n",
    "    xwav = np.arange(min,max,0.001)\n",
    "\n",
    "    # Adjust the size of the flux data to fit the mask\n",
    "    interp_func = interp1d(wav, flux, bounds_error=False, fill_value=np.nan)\n",
    "    padded_flux = interp_func(xwav)\n",
    "\n",
    "    #movingavg\n",
    "    window_size = 3000\n",
    "    n_moving_avg = uniform_filter1d(padded_flux, size=window_size)\n",
    "    \n",
    "    return xwav,padded_flux,n_moving_avg\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to process the inputted wavelength and flux data, \n",
    "#making binary masks, moving averagses, normalising the continuum\n",
    "\n",
    "def process_data_cc(wav,flux):\n",
    "\n",
    "    #smoothing data\n",
    "    s_flux = savgol_filter(flux, window_length=501, polyorder=3)\n",
    "    flux = flux/s_flux\n",
    "    \n",
    "    #interpolating for the interorder noise\n",
    "    \n",
    "    #for i in io_noise:\n",
    "    #    a = np.searchsorted(wav, i[0])\n",
    "    #    b = np.searchsorted(wav, i[1])\n",
    "    #    flux[a:b] = np.nan\n",
    " \n",
    "    #not_nan = np.where(~np.isnan(flux))[0]\n",
    "    #x = np.arange(flux.shape[0])\n",
    "    #flux[np.isnan(flux)] = np.interp(x[np.isnan(flux)], not_nan, flux[not_nan])\n",
    "\n",
    "    #make a new binary mask that is accurate to 3dp\n",
    "    #xwav = np.arange(3857.2,8871.2,0.001)\n",
    "    #mask = np.zeros(len(xwav))\n",
    "    #mask[:]=1\n",
    "\n",
    "    #adjust the size of the flux data to fit the mask\n",
    "    #padded_flux = np.interp(xwav, wav, flux)\n",
    "    \n",
    "    #normalise the continuum\n",
    "    #smoothed_continuum = savgol_filter(flux, window_length=501, polyorder=3)\n",
    "    #n_flux = flux / smoothed_continuum\n",
    "    #n_p_flux = padded_flux / smoothed_continuum\n",
    "\n",
    "    # Adjust the size of the flux data to fit the mask\n",
    "    #interp_func = interp1d(wav, flux, bounds_error=False, fill_value=np.nan)\n",
    "    #padded_flux = interp_func(xwav)\n",
    "\n",
    "    # Normalise the continuum using Savitzky-Golay filter\n",
    "    #smoothed_continuum = savgol_filter(padded_flux, window_length=2001, polyorder=3)\n",
    "    #n_p_flux = np.nan_to_num(padded_flux/smoothed_continuum, nan=1, posinf=1, neginf=1)\n",
    "\n",
    "    # Clip values to the bound\n",
    "    #bound = 2\n",
    "    #n_p_flux = np.clip(n_p_flux, -bound, bound)\n",
    "\n",
    "    #also maybe we could make a mask that is basically a moving average of the spectrum \n",
    "    #over a very wide moving average?\n",
    "    #window_size = 30000\n",
    "    #n_moving_avg = uniform_filter1d(padded_flux, size=window_size)\n",
    "\n",
    "    #dw_masked = np.ma.masked_invalid(wav)\n",
    "    #df_masked = np.ma.masked_invalid(s_flux)\n",
    "\n",
    "    # Get masked arrays with NaN excluded\n",
    "    #dw_comp = dw_masked.compressed() \n",
    "\n",
    "    # Get valid (non NaN) indices  \n",
    "    #valid_ind = ~np.isnan(flux)\n",
    "    #wav = wav[valid_ind]\n",
    "    #flux = flux[valid_ind]\n",
    "\n",
    "    \n",
    "    \n",
    "    return wav,flux #,xwav,padded_flux,n_moving_avg n_mask\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_model(wav,flux,c_lines):\n",
    "    u_loc = np.searchsorted(wav, (5200))\n",
    "    closest_value = wav[max(0, u_loc-1)]\n",
    "    u_bound = np.where(wav == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(wav, (4100))\n",
    "    closest_value = wav[max(0, l_loc-1)]\n",
    "    l_bound = np.where(wav == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    #make small datasets around the window\n",
    "    flux = flux[l_bound:u_bound]\n",
    "    wav = wav[l_bound:u_bound]\n",
    "\n",
    "\n",
    "    #interpolate the spectrum to make it more detailed\n",
    "    minimum = wav[np.argmin(wav)]\n",
    "    maximum = wav[np.argmax(wav)]\n",
    "    xwav = np.arange(minimum,maximum,0.01)\n",
    "\n",
    "    # Adjust the size of the flux data to fit the mask\n",
    "    interp_func = interp1d(wav, flux, bounds_error=False, fill_value=np.nan)\n",
    "    padded_flux = interp_func(xwav)\n",
    "    \n",
    "\n",
    "    #normalise the continuum\n",
    "    s_flux = savgol_filter(padded_flux, window_length=501, polyorder=4)\n",
    "\n",
    "    n_flux = padded_flux / s_flux\n",
    "\n",
    "    #n_flux = flux\n",
    "    \n",
    "\n",
    "\n",
    "    #now add voight profiles to the continuum where the lines should be:\n",
    "    # Add a Voigt profile for each line -\n",
    "    #we need to check the outputted voigt parameters for the direct Voigt fits and use these\n",
    "    for i in c_lines:\n",
    "    #Mg4481 line\n",
    "        center = float(i[1])\n",
    "        amplitude = -0.05\n",
    "        sigma =  0.1\n",
    "        gamma = 0.1\n",
    "        model = VoigtModel()\n",
    "        #print((model.func(wav, amplitude, center, sigma, gamma)))\n",
    "        n_flux += model.func(xwav, amplitude, center, sigma,gamma)\n",
    "\n",
    "    return xwav,n_flux\n",
    "\n",
    "#wav,n_flux = process_data_model(model_wav,model_flux,b_lines)\n",
    "#plt.plot(wav,n_flux)\n",
    "#plt.ylim(0,4)\n",
    "#plt.xlim(4100,5200)\n",
    "#plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_mike_gaussian(wav,flux,c_lines):\n",
    "    #make a new wavelength filter that is accurate to 3dp\n",
    "    min = wav[np.argmin(wav)]\n",
    "    max = wav[np.argmax(wav)]\n",
    "    xwav = np.arange(min,max,0.001)\n",
    "\n",
    "    minlen = np.minimum(len(wav),len(flux))\n",
    "\n",
    "    # Adjust the size of the flux data to fit the mask\n",
    "    interp_func = interp1d(wav[:minlen], flux[:minlen], bounds_error=False, fill_value=np.nan)\n",
    "    padded_flux = interp_func(xwav)\n",
    "\n",
    "    #movingavg\n",
    "    window_size = 3000\n",
    "    n_moving_avg = uniform_filter1d(padded_flux, size=window_size)\n",
    "    return xwav, padded_flux, n_moving_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate errors - faster\n",
    "\n",
    "window_size = int(500) #In raw data points\n",
    "\n",
    "def calculate_error(data):\n",
    "    # Use a rolling window approach for efficient calculation of standard deviation\n",
    "    #least mean squared method\n",
    "    rolling_std = np.sqrt(np.convolve(data**2, np.ones(window_size) / window_size, mode='valid') - np.convolve(data, np.ones(window_size) / window_size, mode='valid')**2)\n",
    "    \n",
    "    # Extend the result to match the original length of the data\n",
    "    pad_width = window_size // 2\n",
    "    errors = np.pad(rolling_std, (pad_width, pad_width), mode='edge')\n",
    "    #print(len(errors),len(data))\n",
    "    errorsv = np.nan_to_num(errors)\n",
    "    #print(np.mean(errorsv))\n",
    "    return errorsv[:len(data)]\n",
    "\n",
    "\n",
    "\n",
    "#Currently using a window size of 100 - better representing the individual variation\n",
    "#Also currently multiplying the raw error figure in the data by 5 to try and get a more representative result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(gwav,gdata):\n",
    "    poly_model = PolynomialModel(degree=7)\n",
    "    params = poly_model.guess(gdata, x=gwav)\n",
    "    poly_result = poly_model.fit(gdata, params, x=gwav)\n",
    "    print(poly_result.params['c0'],poly_result.params['c1'],poly_result.params['c2'],poly_result.params['c3'],poly_result.params['c4'])\n",
    "    \n",
    "    #find the individual polynomial best fit\n",
    "    c0 = poly_result.params['c0'].value\n",
    "    c1 = poly_result.params['c1'].value\n",
    "    c2 = poly_result.params['c2'].value\n",
    "    c3 = poly_result.params[\"c3\"].value\n",
    "    c4 = poly_result.params[\"c4\"].value\n",
    "    c5 = poly_result.params[\"c5\"].value\n",
    "    c6 = poly_result.params[\"c6\"].value\n",
    "    c7 = poly_result.params[\"c7\"].value\n",
    "    \n",
    "    p_result = c0 + c1 * gwav + c2 * gwav**2 + c3 * gwav**3 + c4 * gwav**4 + c5 * gwav**5 + c6 * gwav**6 + c7 * gwav**7\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(gwav, p_result, color='green', linewidth=1)\n",
    "    plt.plot(gwav, gdata, color='blue', linewidth=0.5)\n",
    "    \n",
    "    plt.show()\n",
    "    return p_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pick the files that we want\n",
    "\n",
    "def pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "    \"\"\"\n",
    "    Pick files in a folder based on given start and end patterns in their filename.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files.\n",
    "    - start_patterns (list): List of patterns to match at the start of filenames.\n",
    "    - end_patterns (list): List of patterns to match at the end of filenames.\n",
    "\n",
    "    Returns:\n",
    "    - List of filenames matching the specified start and end patterns.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        fp = os.path.join(folder_path, folder_name,\"product/\")\n",
    "\n",
    "        # List all files in the folder\n",
    "        try:\n",
    "            all_files = os.listdir(fp)\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "\n",
    "        # Filter files based on start patterns\n",
    "        #for start_pattern in start_patterns:\n",
    "        #   matching_files.extend(fnmatch.filter(all_files, start_pattern + '*'))\n",
    "\n",
    "        # Filter files based on end patterns\n",
    "        #for end_pattern in end_patterns:\n",
    "         #   matching_files.extend(fnmatch.filter(all_files, '*' + end_pattern))\n",
    "        for file in all_files:\n",
    "            if file.startswith(start_patterns) and file.endswith(end_patterns):\n",
    "                matching_files.append(os.path.join(fp, file))\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "\n",
    "def mike_pick_files_by_patterns(folder_path, start_patterns, end_patterns):\n",
    "    matching_files = []\n",
    "    # Ensure the folder path is valid\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Error: {folder_path} is not a valid directory.\")\n",
    "        return matching_files\n",
    "    try:\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            matching_files.append(os.path.join(folder_path, file_name))\n",
    "    except FileNotFoundError:\n",
    "        # Handle the case where the folder doesn't exist\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Return the list of matching files\n",
    "    return matching_files\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#This is where we pick the folder path and the name of the file that we want\n",
    "#Need to adjust this so that it is more clear\n",
    "star = \"WD1929+012\"\n",
    "SALT_folder_path = \"/data/wdplanetary/omri/Data/WD1929+012/\"\n",
    "MIKE_blue_folder_path = \"/data/wdplanetary/laura/MIKE/Data/WD1929+011/blue/\"\n",
    "MIKE_red_folder_path = \"/data/wdplanetary/laura/MIKE/Data/WD1929+011/red/\"\n",
    "\n",
    "\n",
    "blue_start = (\"mbgphH\")\n",
    "red_start =  (\"mbgphR\")\n",
    "end_patterns = (\"u2wm.fits\")  #End pattern for the object fiber\n",
    "sky_end_patterns = (\"u1wm.fits\") #End pattern for the sky fiber\n",
    "merged_end_patterns = (\"uwm.fits\") #end pattern for the reduced fiber\n",
    "mike_start = (\"gal\")\n",
    "mike_end = (\".fits\")\n",
    "\n",
    "#Creating file directory for the blue and red channels separately\n",
    "b_files = pick_files_by_patterns(SALT_folder_path, blue_start, end_patterns)\n",
    "merged_b_files = pick_files_by_patterns(SALT_folder_path, blue_start, merged_end_patterns)\n",
    "r_files = pick_files_by_patterns(SALT_folder_path, red_start, end_patterns)\n",
    "sky_r_files = pick_files_by_patterns(SALT_folder_path, red_start, sky_end_patterns)\n",
    "mike_b_files = mike_pick_files_by_patterns(MIKE_blue_folder_path, mike_start,mike_end)\n",
    "mike_r_files = mike_pick_files_by_patterns(MIKE_red_folder_path, mike_start,mike_end)\n",
    "\n",
    "b_files.sort()\n",
    "r_files.sort()\n",
    "sky_r_files.sort()\n",
    "mike_b_files.sort()\n",
    "mike_r_files.sort()\n",
    "bad_date = ['/data/wdplanetary/omri/Data/WD1929+012/2019-1-SCI-008.20190513',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2020-1-SCI-043.20200531',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2018-1-SCI-043.20180605',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2019-2-SCI-049.20200824',\n",
    "            '/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170813/product/mbgphH20170813003'\n",
    "            ]\n",
    "\n",
    "b_files = [f for f in b_files if not any(f.startswith(start) for start in bad_date)]\n",
    "r_files = [f for f in r_files if not any(f.startswith(start) for start in bad_date)] \n",
    "sky_r_files = [f for f in sky_r_files if not any(f.startswith(start) for start in bad_date)] \n",
    "merged_b_files = [f for f in merged_b_files if not any(f.startswith(start) for start in bad_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mike_r_files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in b_files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting files up in to boxes\n",
    "def splitfiles(wavelength,flux):\n",
    "    #l= 2\n",
    "    l = abs(np.ptp(wavelength)/600)\n",
    "    wavboxes = np.array_split(wavelength, l)\n",
    "    fluxboxes = np.array_split(flux, l)\n",
    "    return wavboxes,fluxboxes\n",
    "\n",
    "#Split files into mini boxes around each line\n",
    "def split_files_by_line(xwav,flux,line,w_size):\n",
    "    #find the upper bound of the window\n",
    "    line = float(line)\n",
    "    u_loc = np.searchsorted(xwav, (line+(w_size/2)))\n",
    "    closest_value = xwav[max(0, u_loc-1)]       \n",
    "    u_bound = np.where(xwav == closest_value)\n",
    "    u_bound = int(u_bound[0])       \n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(xwav, (line-(w_size/2)))\n",
    "    closest_value = xwav[max(0, l_loc-1)]\n",
    "    l_bound = np.where(xwav == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    #make small datasets around the line\n",
    "    gdata = flux[l_bound:u_bound]\n",
    "    gwav = xwav[l_bound:u_bound]\n",
    "    return gwav,gdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#making windows and bounds\n",
    "def make_window(xwav,flux,errors,moving_avg,i,w_size):\n",
    "#find the upper bound of the window\n",
    "    u_loc = np.searchsorted(xwav, (i[1]+(w_size/2)))\n",
    "    closest_value = xwav[max(0, u_loc-1)]\n",
    "    u_bound = np.where(xwav == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "    closest_value = xwav[max(0, l_loc-1)]\n",
    "    l_bound = np.where(xwav == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    #make small datasets around the line\n",
    "    gdata = flux[l_bound:u_bound]\n",
    "    gwav = xwav[l_bound:u_bound]\n",
    "    gavg = moving_avg[l_bound:u_bound]\n",
    "    gavg = np.where(gavg != 0, gavg, 1)\n",
    "    #make a new window for errors which is just outside the window - basically the next window down instead\n",
    "    #Instead we want to make a small window around the absorption line \n",
    "    #and then take the errors from the points around it \n",
    "    #and then take the average value of that \n",
    "    #and set the errors of the line to it\n",
    "    el_loc = np.searchsorted(xwav, (i[1]-(1/10)*(w_size)))\n",
    "    closest_value = xwav[max(0, el_loc-1)]\n",
    "    el_bound = np.where(xwav == closest_value)\n",
    "    el_bound = int(el_bound[0])\n",
    "    eh_loc = np.searchsorted(xwav, (i[1]+(1/10)*(w_size)))\n",
    "    closest_value = xwav[max(0, eh_loc-1)]\n",
    "    eh_bound = np.where(xwav == closest_value)\n",
    "    eh_bound = int(eh_bound[0])\n",
    "\n",
    "    firsterrorbox = (errors[l_bound:el_bound])\n",
    "    seconderrorbox = (errors[eh_bound:u_bound])\n",
    "    err_avg = (np.mean(firsterrorbox) + np.mean(seconderrorbox) )/2\n",
    "    lineerroravg= []*(eh_bound-el_bound)\n",
    "    lineerroravg[el_bound:eh_bound] = [err_avg] * (eh_bound - el_bound)\n",
    "\n",
    "    gerrors = np.concatenate((firsterrorbox, lineerroravg, seconderrorbox))\n",
    "    #gerrors[np.isnan(gerrors)] = 0.001\n",
    "    #gerrors[:len(gdata)]\n",
    "    \n",
    "    gweights = np.where(gerrors != 0, 1 / gerrors, 0.001)\n",
    "    \n",
    "    \n",
    "    # Calculate the mean signal\n",
    "    if l_bound < el_bound and eh_bound < u_bound:\n",
    "        signal_mean = (np.mean(flux[l_bound:el_bound]) + np.mean(flux[eh_bound:u_bound])) / 2 - np.min(flux[el_bound:eh_bound])\n",
    "    else:\n",
    "        signal_mean = 10\n",
    "    # Calculate the line depth\n",
    "    depth = signal_mean / err_avg\n",
    "    \n",
    "\n",
    "    #now calculate snr\n",
    "    snr = np.median(flux[l_bound:el_bound])/(np.std(flux[l_bound:el_bound]))\n",
    "\n",
    "\n",
    "    return (gwav,gdata,gerrors,gweights,depth,snr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Laura's Gaussian fitting\n",
    "def Gaussian(wavelength,flux,moving_avg,errors,c_lines,time,n,plot_dir,ax):\n",
    "    w_size = 30\n",
    "    snr_cutoff = 8\n",
    "    line_depth_cutoff= 1\n",
    "    n=n-1\n",
    "    #initialise arrays\n",
    "    RV = np.array([])\n",
    "    RV_weight = np.array([])\n",
    "    RV_err = np.array([])\n",
    "    D = np.array([])\n",
    "    SNR = np.array([])\n",
    "    print(\"Lines plotted\" + str(n))\n",
    "    \n",
    "    \n",
    "    for i in c_lines:\n",
    "        v_precision = 0.5 #from the observed stability if not recent calibrations have been done - Kniazev\n",
    "         #0.0424/float(i[1]) * 299272 from the resolving power of the instrument\n",
    "        #change plot directory\n",
    "        wd = os.path.join(plot_dir, str(i[0])+\"/\")\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(wd):\n",
    "            os.makedirs(wd)\n",
    "        n=n+1\n",
    "\n",
    "        \n",
    "        #gwav,gdata,gerrors,gweights,D,snr = make_window(wav,flux,moving_avg,i,w_size):\n",
    "\n",
    "        #find the upper bound of the window\n",
    "        u_loc = int(np.searchsorted(wavelength, (i[1]+(w_size/2))))\n",
    "        closest_value = wavelength[max(0, u_loc-1)]\n",
    "        u_bound = np.where(wavelength == closest_value)\n",
    "        u_bound = int(u_bound[0])\n",
    "        #find the lower bound of the window\n",
    "        l_loc = int(np.searchsorted(wavelength, (i[1]-(w_size/2))))\n",
    "        closest_value = wavelength[max(0, l_loc-1)]\n",
    "        l_bound = np.where(wavelength == closest_value)\n",
    "        l_bound = int(l_bound[0])\n",
    "        #make small datasets around the line\n",
    "        gdata = flux[l_bound:u_bound]\n",
    "        gwav = wavelength[l_bound:u_bound]\n",
    "        gavg = moving_avg[l_bound:u_bound]\n",
    "        gavg = np.where(gavg != 0, gavg, 1)\n",
    "        #make a new window for errors which is just outside the window - basically the next window down instead\n",
    "        #Instead we want to make a small window around the absorption line \n",
    "        #and then take the errors from the points around it \n",
    "        #and then take the average value of that \n",
    "        #and set the errors of the line to it\n",
    "        el_loc = int(np.searchsorted(wavelength, (i[1]-(1/10)*(w_size))))\n",
    "        closest_value = wavelength[max(0, el_loc-1)]\n",
    "        el_bound = np.where(wavelength == closest_value)\n",
    "        el_bound = int(el_bound[0])\n",
    "        eh_loc = int(np.searchsorted(wavelength, (i[1]+(1/10)*(w_size))))\n",
    "        closest_value = wavelength[max(0, eh_loc-1)]\n",
    "        eh_bound = np.where(wavelength == closest_value)\n",
    "        eh_bound = int(eh_bound[0])\n",
    "\n",
    "        firsterrorbox = (errors[l_bound:el_bound])\n",
    "        seconderrorbox = (errors[eh_bound:u_bound])\n",
    "        err_avg = (np.mean(firsterrorbox) + np.mean(seconderrorbox) )/2\n",
    "        lineerroravg= []*(eh_bound-el_bound)\n",
    "        lineerroravg[el_bound:eh_bound] = [err_avg] * (eh_bound - el_bound)\n",
    "\n",
    "        gerrors = np.concatenate((firsterrorbox, lineerroravg, seconderrorbox))\n",
    "        #gerrors[np.isnan(gerrors)] = 0.001\n",
    "        gerrors[:len(gdata)]\n",
    "        \n",
    "        gweights = np.where(gerrors != 0, 1 / gerrors, 0.001)\n",
    "        \n",
    "        \n",
    "        # Calculate the mean signal\n",
    "        signal_mean = (np.mean(flux[l_bound:el_bound]) + np.mean(flux[eh_bound:u_bound])) / 2 - np.min(flux[el_bound:eh_bound])\n",
    "        # Calculate the line depth\n",
    "        depth = signal_mean / err_avg\n",
    "        print(depth)\n",
    "        D = np.append(D,depth)\n",
    "\n",
    "        #now calculate snr\n",
    "        snr = np.median(flux[l_bound:el_bound])/(np.std(flux[l_bound:el_bound]))\n",
    "        #Try to add an error to the x values\n",
    "        #wav_errors = np.full(len(gwav),0.0424)\n",
    "\n",
    "        #set the initial guess of the mean value of the gaussian to the wavelength in air\n",
    "        line = i[1]\n",
    "        print(\"made error boxes and calculated snr\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            voigt_model = VoigtModel(prefix='voigt_')\n",
    "            linear_model = LinearModel(prefix='linear_')\n",
    "            poly_model = PolynomialModel(degree=2)\n",
    "            #combine models - can swap in or out the polynomial or linear model as we wish \n",
    "            composite_model = voigt_model  + poly_model\n",
    "            #if line_type is a sky line then it is an emission line with a positive peak, if not, then it is an absorption line with negative peak\n",
    "            print(c_lines)\n",
    "            if c_lines is sky_lines:\n",
    "                if depth <= line_depth_cutoff:\n",
    "                    raise Exception(\"Line not significant: Line depth < \"+str(line_depth_cutoff))\n",
    "                params = voigt_model.make_params(voigt_amplitude=0.03, voigt_center=line, voigt_sigma=0.1, voight_gamma = 0.1)\n",
    "                params['voigt_amplitude'].min = 0\n",
    "                params['voigt_amplitude'].max = 1\n",
    "                params['voigt_center'].min = (line - 2)\n",
    "                params['voigt_center'].max = (line +2)\n",
    "                print(\"sky params made\")\n",
    "            else:\n",
    "                print(\"recognised it is not a sky line\")\n",
    "                #CHANGE THE SNR VALUE TO FILTER DATASETS THAT ARE NOT FITTING WELL\n",
    "                #We want this code to try the next line in the dataset \n",
    "                if snr <= snr_cutoff:\n",
    "                    raise Exception(\"SNR too low: < \" +str(snr_cutoff))\n",
    "                if depth <= line_depth_cutoff:\n",
    "                    raise Exception(\"Line not significant: Line depth < \"+str(line_depth_cutoff))\n",
    "                ####\n",
    "                \n",
    "                print(f\"SNR is {snr:.3g}\")\n",
    "                params = voigt_model.make_params(voigt_amplitude=-0.01, voigt_center=line, voigt_sigma=0.1, voight_gamma = 0.1)\n",
    "                params['voigt_amplitude'].min = -2\n",
    "                params['voigt_amplitude'].max = 0.0\n",
    "                params['voigt_center'].min = (line - 2)\n",
    "                params['voigt_center'].max = (line +2)\n",
    "                print(\"absorption line  params made\")\n",
    "            #params += linear_model.make_params(slope=0, intercept=np.median(gdata))\n",
    "            params += poly_model.guess(gdata, x=gwav)\n",
    "            \n",
    "            result = composite_model.fit(gdata, params, x=gwav, weights = gweights )#,sigma = wav_errors)\n",
    "            #print(result.values['voigt_center'],result.params['voigt_center'])\n",
    "            \n",
    "            #find the individual polynomial best fit\n",
    "            c0 = result.params['c0'].value\n",
    "            c1 = result.params['c1'].value\n",
    "            c2 = result.params['c2'].value\n",
    "            p_result = c0 + c1 * gwav + c2 * gwav**2\n",
    "            \n",
    "            t = time.datetime\n",
    "            #plt.show()\n",
    "            #print(result.values['voigt_center'])\n",
    "            #print(result.fit_report())\n",
    "            Line_Offset = result.values['voigt_center'] - line\n",
    "            rv = (Line_Offset/line) * 299792\n",
    "            err = (((result.params['voigt_center'].stderr * 2.35 )/result.values['voigt_center']) * 299792)\n",
    "\n",
    "            #Append values to RV\n",
    "            RV = np.append(RV,rv)\n",
    "            RV_err = np.append(RV_err,(err)) #+v_precision\n",
    "            RV_weight = np.append(RV_weight,(1/(err)))\n",
    "\n",
    "            rv_significant_figures = max(3, -int(np.floor(np.log10(err))) + 2)\n",
    "\n",
    "            # Format the radial velocity and its error using the determined significant figures\n",
    "            rv_formatted = f\"{rv:.{4}g}\"\n",
    "            err_formatted = f\"{err:.{2}g}\"\n",
    "            \n",
    "            #Plotting results\n",
    "            if c_lines is not sky_lines:\n",
    "\n",
    "                #Add a simple plot to the stacked plot\n",
    "                \n",
    "                ax.plot(gwav, gdata/p_result  + n/4 , color='black', linewidth=0.5)\n",
    "                ax.plot(gwav, result.best_fit/p_result  + n/4, color='red')\n",
    "                ax.set_xlim(line-5,line+5)\n",
    "                x_text = gwav[-1] + 0.2  # Add an offset for spacing\n",
    "                ax.text(x_text, (1 + n/4), f\"SNR:{snr:.3g}\" , verticalalignment='center')\n",
    "                \n",
    "\n",
    "                print(\"stacked plot made\")\n",
    "                \n",
    "                \n",
    "                #We can add a label next to the line by using \n",
    "                \"\"\" \n",
    "                #Now save the line plot with the best fit and the shaded errors \n",
    "                plt.figure(facecolor='white')\n",
    "                #add spectrum\n",
    "                plt.plot(gwav, gdata/p_result, color='black', linewidth=0.5)\n",
    "                #add best fit line\n",
    "                plt.plot(gwav, result.best_fit/p_result, color='red')\n",
    "                #shade standard deviation of data\n",
    "                plt.fill_between(gwav, (result.best_fit - gerrors)/p_result, (result.best_fit + gerrors)/p_result, color='gray', alpha=0.3)\n",
    "                \n",
    "                plt.title(f\"{t.year}/{t.month}/{t.day}  Line Centre:{result.values['voigt_center']}\")\n",
    "                plt.text(0.95, 0.10, f\"SNR = {snr:.3g}\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                plt.text(0.95, 0.05, f\" RV = {rv:.{4}g} ± {err:.{2}g} km/s\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                plt.ylim(0.5,1.3)\n",
    "                title = str(i[0]) + \"_\" + str(t) + '.png'\n",
    "                title_without_spaces = title.replace(\" \", \"\")\n",
    "                plt.savefig(os.path.join(wd, title_without_spaces))\n",
    "                plt.show()\n",
    "                plt.close()   \"\"\"\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            n = n-1\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            RV = np.append(RV,np.nan)\n",
    "            RV_err = np.append(RV_err,np.nan)\n",
    "            RV_weight = np.append(RV_weight,np.nan)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #weighted mean and standard deviation error calculations\n",
    "    if np.any(~np.isnan(RV)):\n",
    "        # Weighted mean and standard deviation error calculations\n",
    "        mean = (np.sum((RV[~np.isnan(RV)] * RV_weight[~np.isnan(RV_weight)])) / (np.sum(RV_weight[~np.isnan(RV_weight)])))\n",
    "        mean_error = np.sqrt(np.sum(RV_weight[~np.isnan(RV_weight)] * RV_err[~np.isnan(RV_err)]**2) / np.sum(RV_weight[~np.isnan(RV_weight)]) + 0.5**2 ) #Adding errors in quadrature\n",
    "\n",
    "    else:\n",
    "        mean = np.nan\n",
    "        mean_error = np.nan\n",
    "    if c_lines is sky_lines:\n",
    "        print(\"The sky RV correction mean and error is\", mean,mean_error)\n",
    "\n",
    "    else:\n",
    "        print(\"The rv mean and error is\", mean,mean_error)\n",
    "\n",
    "        \n",
    "    return mean, mean_error,ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laura's Gaussian fitting\n",
    "\n",
    "\n",
    "def Gaussian(wavelength,flux,moving_avg,errors,c_lines,time,n,plot_dir):\n",
    "    print(\"------------NEW DATE FILE -------------\")\n",
    "    v_precision = 0.2 #kniazev stability analysis for SALT HRS LRS\n",
    "    w_size = 30\n",
    "    snr_cutoff = 13 #10\n",
    "    line_depth_cutoff= 6 #3\n",
    "    sky_line_depth_cutoff= 3\n",
    "    #initialise result arrays\n",
    "    RV = np.array([])\n",
    "    RV_weight = np.array([])\n",
    "    RV_err = np.array([])\n",
    "\n",
    "    #initialise the arrays for plotting\n",
    "    D = np.array([])\n",
    "    SNR = np.array([])\n",
    "    wavelengths = ([])\n",
    "    best_fits = np.array([])\n",
    "    flux_results = np.array([])\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in c_lines:\n",
    "        \n",
    "        #change plot directory\n",
    "        wd = os.path.join(plot_dir, str(i[0])+\"/\")\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(wd):\n",
    "            os.makedirs(wd)\n",
    "\n",
    "        \n",
    "        #gwav,gdata,gerrors,gweights,D,snr = make_window(wav,flux,moving_avg,i,w_size):\n",
    "\n",
    "        #find the upper bound of the window\n",
    "        u_loc = np.searchsorted(xwav, (i[1]+(w_size/2)))\n",
    "        closest_value = xwav[max(0, u_loc-1)]\n",
    "        u_bound = np.where(xwav == closest_value)\n",
    "        u_bound = int(u_bound[0])\n",
    "        #find the lower bound of the window\n",
    "        l_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "        closest_value = xwav[max(0, l_loc-1)]\n",
    "        l_bound = np.where(xwav == closest_value)\n",
    "        l_bound = int(l_bound[0])\n",
    "        #make small datasets around the line\n",
    "        gdata = flux[l_bound:u_bound]\n",
    "        gwav = wavelength[l_bound:u_bound]\n",
    "        gavg = moving_avg[l_bound:u_bound]\n",
    "        gavg = np.where(gavg != 0, gavg, 1)\n",
    "        #make a new window for errors which is just outside the window - basically the next window down instead\n",
    "        #Instead we want to make a small window around the absorption line \n",
    "        #and then take the errors from the points around it \n",
    "        #and then take the average value of that \n",
    "        #and set the errors of the line to it\n",
    "        el_loc = np.searchsorted(xwav, (i[1]-(1/20)*(w_size)))\n",
    "        closest_value = xwav[max(0, el_loc-1)]\n",
    "        el_bound = np.where(xwav == closest_value)\n",
    "        el_bound = int(el_bound[0])\n",
    "        eh_loc = np.searchsorted(xwav, (i[1]+(1/20)*(w_size)))\n",
    "        closest_value = xwav[max(0, eh_loc-1)]\n",
    "        eh_bound = np.where(xwav == closest_value)\n",
    "        eh_bound = int(eh_bound[0])\n",
    "\n",
    "        firsterrorbox = (errors[l_bound:el_bound])\n",
    "        seconderrorbox = (errors[eh_bound:u_bound])\n",
    "        err_avg = (np.mean(firsterrorbox) + np.mean(seconderrorbox) )/2\n",
    "        lineerroravg= []*(eh_bound-el_bound)\n",
    "        lineerroravg[el_bound:eh_bound] = [err_avg] * (eh_bound - el_bound)\n",
    "\n",
    "        gerrors = np.concatenate((firsterrorbox, lineerroravg, seconderrorbox))\n",
    "        #gerrors[np.isnan(gerrors)] = 0.001\n",
    "        gerrors[:len(gdata)]\n",
    "        \n",
    "        gweights = np.where(gerrors != 0, 1 / gerrors, 0.001)\n",
    "        \n",
    "        \n",
    "        # Calculate the mean signal\n",
    "        signal_mean = ((np.mean(flux[l_bound:el_bound]) + np.mean(flux[eh_bound:u_bound])) / 2) - np.min(flux[el_bound:eh_bound])\n",
    "        # Calculate the line depth\n",
    "        depth = np.abs(signal_mean / err_avg)\n",
    "        D = np.append(D,depth)\n",
    "\n",
    "        #now calculate snr\n",
    "        snr = np.median(flux[l_bound:el_bound])/(np.std(flux[l_bound:el_bound]))\n",
    "        SNR = np.append(SNR,snr)\n",
    "\n",
    "        #set the initial guess of the mean value of the gaussian to the wavelength in air\n",
    "        line = i[1]\n",
    "        try:\n",
    "            \n",
    "            voigt_model = VoigtModel(prefix='voigt_')\n",
    "            linear_model = LinearModel(prefix='linear_')\n",
    "            poly_model = PolynomialModel(degree=3)\n",
    "            #combine models - can swap in or out the polynomial or linear model as we wish \n",
    "            composite_model = voigt_model  + poly_model\n",
    "            #if line_type is a sky line then it is an emission line with a positive peak, if not, then it is an absorption line with negative peak\n",
    "            if c_lines is sky_lines:\n",
    "                if depth <= sky_line_depth_cutoff:\n",
    "                    raise Exception(\"Line not significant: Line depth < \"+str(line_depth_cutoff))\n",
    "                params = voigt_model.make_params(voigt_amplitude=0.03, voigt_center=line, voigt_sigma=0.1, voight_gamma = 0.1)\n",
    "                params['voigt_amplitude'].min = 0\n",
    "                params['voigt_amplitude'].max = 1\n",
    "                params['voigt_center'].min = (line-2)\n",
    "                params['voigt_center'].max = (line+2)\n",
    "                print(\"sky params made\")\n",
    "            else:\n",
    "                #CHANGE THE SNR VALUE TO FILTER DATASETS THAT ARE NOT FITTING WELL\n",
    "                #We want this code to try the next line in the dataset \n",
    "                if snr <= snr_cutoff:\n",
    "                    raise Exception(f\"SNR too low: <{snr_cutoff}\")\n",
    "                if depth <= line_depth_cutoff:\n",
    "                    raise Exception(f\"Line not significant: Line depth < {line_depth_cutoff}\")\n",
    "                ####\n",
    "                \n",
    "                print(f\"SNR is {snr:.3g}\")\n",
    "                params = voigt_model.make_params(voigt_amplitude=-0.01, voigt_center=line, voigt_sigma=0.3, voight_gamma = 0.3)\n",
    "                params['voigt_amplitude'].min = -2\n",
    "                params['voigt_amplitude'].max = 0.0\n",
    "                params['voigt_center'].min = (line - 2)\n",
    "                params['voigt_center'].max = (line + 2)\n",
    "                print(\"absorption params made\")\n",
    "            #params += linear_model.make_params(slope=0, intercept=np.median(gdata))\n",
    "            params += poly_model.guess(gdata, x=gwav)\n",
    "            \n",
    "            result = composite_model.fit(gdata, params, x=gwav, weights = gweights )#,sigma = wav_errors)\n",
    "            #print(result.values['voigt_center'],result.params['voigt_center'])\n",
    "            \n",
    "            #find the individual polynomial best fit\n",
    "            c0 = result.params['c0'].value\n",
    "            c1 = result.params['c1'].value\n",
    "            c2 = result.params['c2'].value\n",
    "            c3 = result.params['c3'].value\n",
    "            p_result = c0 + c1 * gwav + c2 * gwav**2 + c3 * gwav**3\n",
    "            \n",
    "            t = time.datetime\n",
    "            #plt.show()\n",
    "            #print(result.values['voigt_center'])\n",
    "            #print(result.fit_report())\n",
    "            Line_Offset = result.values['voigt_center'] - line\n",
    "            rv = (Line_Offset/line) * 299792\n",
    "            #print(f\"The voigt sigma is {(result.params['voigt_sigma'])} and the center error is{ result.params['voigt_center'].stderr}\")\n",
    "            if result.params['voigt_center'].stderr is not None:\n",
    "                err = (((result.params['voigt_center'].stderr * 2.35 )/result.values['voigt_center']) * 299792)\n",
    "            \n",
    "            else:\n",
    "                err = 0.2\n",
    "            #err = (((result.params['voigt_sigma'].value)/result.values['voigt_center']) * 299792)\n",
    "\n",
    "            #Append values to RV\n",
    "            RV = np.append(RV,rv)\n",
    "            RV_err = np.append(RV_err,(err)) #+v_precision\n",
    "            RV_weight = np.append(RV_weight,(1/(err)))\n",
    "\n",
    "            rv_significant_figures = max(3, -int(np.floor(np.log10(err))) + 2)\n",
    "\n",
    "            # Format the radial velocity and its error using the determined significant figures\n",
    "            rv_formatted = f\"{rv:.{4}g}\"\n",
    "            err_formatted = f\"{err:.{2}g}\"\n",
    "            \n",
    "            #Plotting results\n",
    "            if c_lines is not sky_lines:\n",
    "\n",
    "                #Add a simple plot to the stacked plot\n",
    "                \"\"\" \n",
    "                ax.plot(gwav, gdata/p_result  + n/3 , color='black', linewidth=0.5)\n",
    "                ax.plot(gwav, result.best_fit/p_result  + n/3, color='red')\n",
    "                ax.set_xlim(line-10,line+10)\n",
    "                x_text = gwav[-1] + 0.2  # Add an offset for spacing\n",
    "                ax.text(line+11, (1 + n/3), f\"SNR:{snr:.3g}\" , verticalalignment='center')\n",
    "                n = n+1\n",
    "                \"\"\"\n",
    "                #print(\"stacked plot made\")\n",
    "\n",
    "                #save the datafiles instead of stacking the plot here\n",
    "                data = pd.DataFrame({\"Wavelength\":(gwav),\"Normalized Data\": (gdata/p_result), \"Voigt fit\": (result.best_fit/p_result), \"Time\": t, \"SNR\": snr, \"Depth\": depth, \"RV\": rv, \"Error\": err}) #[t],[snr], [depth], [rv], [err]])\n",
    "                dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/all_abs_lines/snr_cutoff_{snr:.2g}/line_depth_cutoff_{depth:.2g}/{t}/\"\n",
    "                dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "                os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "                file_end = f\"{str(line)}.txt\"\n",
    "                file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "                file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "                #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "                data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "                #how to read the data back in \n",
    "                #data = pd.read_csv(\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/only_mg_line/snr_cutoff_16.05612842459828/2021-04-21T03:09:52.703/4481.185.txt\", sep='\\t')\n",
    "                 \n",
    "                #Now save the line plot with the best fit and the shaded errors \n",
    "                plt.figure(facecolor='white',figsize = (8,6))\n",
    "                #add spectrum\n",
    "                plt.plot(gwav, gdata/p_result, color='black', linewidth=0.5)\n",
    "                #add best fit line\n",
    "                plt.plot(gwav, result.best_fit/p_result, color='red')\n",
    "                #shade standard deviation of data\n",
    "                plt.fill_between(gwav, (result.best_fit - gerrors)/p_result, (result.best_fit + gerrors)/p_result, color='gray', alpha=0.3)\n",
    "                \n",
    "                plt.title(f\"{t.year}/{t.month}/{t.day}  Line Centre:{result.values['voigt_center']:.7g} Å\")\n",
    "                #plt.text(0.95, 0.10, f\"SNR = {snr:.3g}\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                #plt.text(0.95, 0.05, f\" RV = {rv:.{4}g} ± {err:.{2}g} km/s\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                plt.ylim(0.5,1.3)\n",
    "                plt.xlabel(\"Wavelength (Å)\")\n",
    "                plt.ylabel(\"Normalized Flux\")\n",
    "                title = str(i[0]) + \"_\" + str(t) + '.png'\n",
    "                title_without_spaces = title.replace(\" \", \"\")\n",
    "                plt.savefig(os.path.join(wd, title_without_spaces))\n",
    "                plt.show()\n",
    "                plt.close() \n",
    "\n",
    "                print(\"line depth: \",  depth, \"SNR: \", snr) \n",
    "            \n",
    "                \n",
    "        except Exception as e:\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "            line_num = exc_tb.tb_lineno\n",
    "            line_str = linecache.getline(file_name, line_num).strip()\n",
    "            \n",
    "            # Print out the error message along with the line of code\n",
    "            print(f\"Error occurred at line {line_num}: {line_str}\")\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            RV = np.append(RV,np.nan)\n",
    "            RV_err = np.append(RV_err,np.nan)\n",
    "            RV_weight = np.append(RV_weight,np.nan)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #weighted mean and standard deviation error calculations\n",
    "    #if np.any(~np.isnan(RV)):\n",
    "        # Weighted mean and standard deviation error calculations\n",
    "    mean = (np.sum((RV[~np.isnan(RV)] * RV_weight[~np.isnan(RV_weight)])) / (np.sum(RV_weight[~np.isnan(RV_weight)])))\n",
    "    mean_error = np.sqrt(np.sum(RV_weight[~np.isnan(RV_weight)] * RV_err[~np.isnan(RV_err)]**2) / np.sum(RV_weight[~np.isnan(RV_weight)])+ v_precision**2) \n",
    "\n",
    "    \"\"\" else:\n",
    "        mean = np.nan\n",
    "        mean_error = np.nan \"\"\"\n",
    "    if c_lines is sky_lines:\n",
    "        print(\"The sky RV correction mean and error is\", mean,mean_error)\n",
    "\n",
    "    else:\n",
    "        print(\"The rv mean and error is\", mean,mean_error)\n",
    "\n",
    "        \n",
    "    return mean, mean_error,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voigt fitting for the MIKE Spectrum\n",
    " \n",
    "def Mike_Gaussian(wav_list,flux_list,moving_avg_list,errors_list,snr_list,c_lines,time,n,plot_dir):\n",
    "    print(\"------------NEW--FILE----------\", str(time))\n",
    "    v_precision = 0. #kneeed to find the stability for MIKE\n",
    "    w_size = 30\n",
    "    snr_cutoff = 13\n",
    "    line_depth_cutoff= 5\n",
    "    #initialise result arrays\n",
    "    RV = np.array([])\n",
    "    RV_weight = np.array([])\n",
    "    RV_err = np.array([])\n",
    "\n",
    "    #initialise the arrays for plotting\n",
    "    D = np.array([])\n",
    "    SNR = np.array([])\n",
    "\n",
    "    \n",
    "    for i in c_lines:\n",
    "        print(i)\n",
    "        try: \n",
    "            for j, wav_value in enumerate(wav_list):\n",
    "                if float(np.min(wav_value)) <= float(i[1]) <= float(np.max(wav_value)):\n",
    "                    print(f\" For order {j} the line is in the order\")\n",
    "\n",
    "        \n",
    "                    xwav = wav_value\n",
    "                    xflux = flux_list[j]\n",
    "                    xmoving_avg = moving_avg_list[j]\n",
    "                    xerrors = errors_list[j]\n",
    "                    xsnr = snr_list[j]\n",
    "\n",
    "                    #change plot directory\n",
    "                    wd = os.path.join(plot_dir, str(i[0])+\"/\")\n",
    "                    # Create the directory if it doesn't exist\n",
    "                    if not os.path.exists(wd):\n",
    "                        os.makedirs(wd)\n",
    "\n",
    "                    \n",
    "                    #gwav,gdata,gerrors,gweights,D,snr = make_window(wav,flux,moving_avg,i,w_size):\n",
    "\n",
    "                    #find the upper bound of the window\n",
    "                    u_loc = np.searchsorted(xwav, (i[1]+(w_size/2)))\n",
    "                    closest_value = xwav[max(0, u_loc-1)]\n",
    "                    u_bound = np.where(xwav == closest_value)\n",
    "                    u_bound = int(u_bound[0])\n",
    "                    #find the lower bound of the window\n",
    "                    l_loc = np.searchsorted(xwav, (i[1]-(w_size/2)))\n",
    "                    closest_value = xwav[max(0, l_loc-1)]\n",
    "                    l_bound = np.where(xwav == closest_value)\n",
    "                    l_bound = int(l_bound[0])\n",
    "                    #make small datasets around the line\n",
    "                    #if l_bound >= 0 and u_bound <= len(flux):\n",
    "                        # Slice the array using integer indices\n",
    "    \n",
    "                    gdata = xflux[l_bound:u_bound]\n",
    "                    gwav = xwav[l_bound:u_bound]\n",
    "                    gavg = xmoving_avg[l_bound:u_bound]\n",
    "                    gavg = np.where(gavg != 0, gavg, 1)\n",
    "                    gsnr = xsnr[l_bound:u_bound]\n",
    "                    \"\"\" else:\n",
    "                        raise Exception(\"The window falls outside the limit of the order\") \"\"\"\n",
    "                    #make a new window for errors which is just outside the window - basically the next window down instead\n",
    "                    #Instead we want to make a small window around the absorption line \n",
    "                    #and then take the errors from the points around it \n",
    "                    #and then take the average value of that \n",
    "                    #and set the errors of the line to it\n",
    "                    el_loc = np.searchsorted(xwav, (i[1]-(1/10)*(w_size)))\n",
    "                    closest_value = xwav[max(0, el_loc-1)]\n",
    "                    el_bound = np.where(xwav == closest_value)\n",
    "                    el_bound = int(el_bound[0])\n",
    "                    eh_loc = np.searchsorted(xwav, (i[1]+(1/10)*(w_size)))\n",
    "                    closest_value = xwav[max(0, eh_loc-1)]\n",
    "                    eh_bound = np.where(xwav == closest_value)\n",
    "                    eh_bound = int(eh_bound[0])\n",
    "\n",
    "                    firsterrorbox = (xerrors[l_bound:el_bound])\n",
    "                    seconderrorbox = (xerrors[eh_bound:u_bound])\n",
    "                    err_avg = (np.mean(firsterrorbox) + np.mean(seconderrorbox) )/2\n",
    "                    lineerroravg= []*(eh_bound-el_bound)\n",
    "                    lineerroravg[el_bound:eh_bound] = [err_avg] * (eh_bound - el_bound)\n",
    "\n",
    "                    gerrors = np.concatenate((firsterrorbox, lineerroravg, seconderrorbox))\n",
    "                    #gerrors[np.isnan(gerrors)] = 0.001\n",
    "                    gerrors[:len(gdata)]\n",
    "                    \n",
    "                    gweights = np.where(gerrors != 0, 1 / gerrors, 0.001)\n",
    "                    \n",
    "                    \n",
    "                    # Calculate the mean signal\n",
    "                    signal_mean = ((np.mean(xflux[l_bound:el_bound]) + np.mean(xflux[eh_bound:u_bound])) / 2) - np.min(xflux[el_bound:eh_bound])\n",
    "                    # Calculate the signal-to-noise ratio (SNR)\n",
    "                    depth = np.abs(signal_mean / err_avg)\n",
    "                    D = np.append(D,depth)\n",
    "\n",
    "                    #now calculate snr\n",
    "                    #snr = np.median(xflux[l_bound:el_bound])/(np.std(xflux[l_bound:el_bound]))\n",
    "                    \n",
    "\n",
    "                    #instead do this with the snr dataset - take mean of snr in bounds\n",
    "                    snr = np.mean(gsnr)\n",
    "\n",
    "                    SNR = np.append(SNR,snr)\n",
    "\n",
    "                    #set the initial guess of the mean value of the gaussian to the wavelength in air\n",
    "                    line = i[1]\n",
    "                    \n",
    "                        \n",
    "                    #p_result = poly_fit(xwav,gdata)\n",
    "                    poly_model = PolynomialModel(degree=5)\n",
    "                    params = poly_model.guess(gdata, x=gwav)\n",
    "                    poly_result = poly_model.fit(gdata, params, x=gwav, weights = gweights )\n",
    "                    print(poly_result.params['c0'],poly_result.params['c1'],poly_result.params['c2'],poly_result.params['c3'],poly_result.params['c4'])\n",
    "                    \n",
    "                    #find the individual polynomial best fit\n",
    "                    c0 = poly_result.params['c0'].value\n",
    "                    c1 = poly_result.params['c1'].value\n",
    "                    c2 = poly_result.params['c2'].value\n",
    "                    c3 = poly_result.params[\"c3\"].value\n",
    "                    c4 = poly_result.params[\"c4\"].value\n",
    "                    c5 = poly_result.params[\"c5\"].value\n",
    "                    p_result = c0 + c1 * gwav + c2 * gwav**2 + c3 * gwav**3 + c4 * gwav**4 + c5 * gwav**5\n",
    " \n",
    "                    plt.figure()\n",
    "                    plt.plot(gwav, p_result, color='green', linewidth=1)\n",
    "                    plt.plot(gwav, gdata, color='blue', linewidth=0.5)\n",
    "                   \n",
    "                    #plt.show()\n",
    "                    plt.close()\n",
    "\n",
    "\n",
    "                    n_flux = gdata/p_result\n",
    "                    n_errors = gerrors/p_result\n",
    "                    n_weights = np.where(n_errors != 0, 1 / n_errors, 0.001)\n",
    "                    #CHANGE THE SNR VALUE TO FILTER DATASETS THAT ARE NOT FITTING WELL\n",
    "                    #We want this code to try the next line in the dataset \n",
    "                    if snr <= snr_cutoff:\n",
    "                        raise Exception(f\"SNR too low: SNR {snr} <{snr_cutoff}\")\n",
    "                    if depth <= line_depth_cutoff:\n",
    "                        raise Exception(f\"Line not significant: Line depth {depth} < {line_depth_cutoff}\")\n",
    "                    ####\n",
    "                    \n",
    "                    print(f\"SNR is {snr:.3g}\")\n",
    "\n",
    "\n",
    "                    voigt_model = VoigtModel(prefix='voigt_')\n",
    "                    linear_model = LinearModel(prefix='linear_')\n",
    "                    composite_model = voigt_model + linear_model\n",
    "                    #When these parameters fit right for the Ca 3xxx fit the amp is -200, the sigma and gamma are 0.03\n",
    "                    params = voigt_model.make_params(voigt_amplitude=-0.6, voigt_center=(line + 0.6), voigt_sigma=0.03, voight_gamma = 0.03)\n",
    "                    params += linear_model.make_params(slope=0, intercept=np.median(n_flux))\n",
    "                    params['voigt_amplitude'].min = -1\n",
    "                    params['voigt_amplitude'].max = -0.02\n",
    "                    params['voigt_center'].min = (line - 2)\n",
    "                    params['voigt_center'].max = (line + 2) \n",
    "                    #params['voigt_sigma'].max = 0.05\n",
    "                    print(\"absorption params made\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    result = composite_model.fit(n_flux, params, x=gwav, weights = n_weights )#,sigma = wav_errors)\n",
    "                    print(result.values['voigt_center'],result.params['voigt_center'],result.params[\"voigt_amplitude\"])\n",
    "                    #print(result.fit_report())\n",
    "                    \n",
    "                   \n",
    "                    t = time.datetime\n",
    "                    #plt.show()\n",
    "                    #print(result.values['voigt_center'])\n",
    "                    #print(result.fit_report())\n",
    "                    Line_Offset = result.values['voigt_center'] - line\n",
    "                    rv = (Line_Offset/line) * 299792\n",
    "                    #print(f\"The voigt sigma is {(result.params['voigt_sigma'])} and the center error is{ result.params['voigt_center'].stderr}\")\n",
    "                    if result.params['voigt_center'].stderr is not None:\n",
    "                        err = (((result.params['voigt_center'].stderr * 3 )/result.values['voigt_center']) * 299792)\n",
    "                    \n",
    "                    else:\n",
    "                        err = 0.2\n",
    "                    #err = (((result.params['voigt_sigma'].value)/result.values['voigt_center']) * 299792)\n",
    "\n",
    "                    #Append values to RV\n",
    "                    if 35 < rv < 40:\n",
    "                        RV = np.append(RV,rv)\n",
    "                        RV_err = np.append(RV_err,(err)) #+v_precision\n",
    "                        RV_weight = np.append(RV_weight,(1/(err)))\n",
    "                    else:\n",
    "                        raise Exception(\"The rv shift is not in the bounds\")\n",
    "\n",
    "                    rv_significant_figures = max(3, -int(np.floor(np.log10(err))) + 2)\n",
    "\n",
    "                    # Format the radial velocity and its error using the determined significant figures\n",
    "                    rv_formatted = f\"{rv:.{4}g}\"\n",
    "                    err_formatted = f\"{err:.{2}g}\"\n",
    "                    \n",
    "                    #Plotting results\n",
    "                    if c_lines is not sky_lines:\n",
    "\n",
    "                        #Add a simple plot to the stacked plot\n",
    "                        \"\"\" \n",
    "                        ax.plot(gwav, gdata/p_result  + n/3 , color='black', linewidth=0.5)\n",
    "                        ax.plot(gwav, result.best_fit/p_result  + n/3, color='red')\n",
    "                        ax.set_xlim(line-10,line+10)\n",
    "                        x_text = gwav[-1] + 0.2  # Add an offset for spacing\n",
    "                        ax.text(line+11, (1 + n/3), f\"SNR:{snr:.3g}\" , verticalalignment='center')\n",
    "                        n = n+1\n",
    "                        \"\"\"\n",
    "                        print(\"stacked plot made\")\n",
    "                        \n",
    "                        \n",
    "                        #We can add a label next to the line by using \n",
    "                             \n",
    "                        #Now save the line plot with the best fit and the shaded errors \n",
    "                        plt.figure(facecolor='white',figsize = (10,8))\n",
    "                        #add spectrum\n",
    "                        plt.plot(gwav, n_flux, color='black', linewidth=0.5)\n",
    "                        #add best fit line\n",
    "                        plt.plot(gwav, result.best_fit, color='red')\n",
    "                        #shade standard deviation of data\n",
    "                        plt.fill_between(gwav, (result.best_fit - n_errors), (result.best_fit + n_errors), color='gray', alpha=0.3)\n",
    "                        \n",
    "                        plt.title(f\"{t.year}/{t.month}/{t.day}, Order: {j},  Line Centre:{result.values['voigt_center']}\")\n",
    "                        plt.text(0.95, 0.10, f\"SNR = {snr:.3g}\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                        plt.text(0.95, 0.05, f\" RV = {rv:.{4}g} ± {err:.{2}g} km/s\", horizontalalignment='right', verticalalignment='top', transform=plt.gca().transAxes)\n",
    "                        plt.ylim(0.5,1.3)\n",
    "                        plt.xlim(line-6,line+6)\n",
    "                        plt.xlabel(\"Wavelength (Å)\")\n",
    "                        plt.ylabel(\"Normalized Flux\")\n",
    "                        title = str(i[0]) + \"_\" + str(t) + '.pdf'\n",
    "                        title_without_spaces = title.replace(\" \", \"\")\n",
    "                        plt.savefig(os.path.join(wd, title_without_spaces))\n",
    "                        #plt.show()\n",
    "                        plt.close() \n",
    "\n",
    "                        #save the datafiles instead of stacking the plot here\n",
    "                        data = pd.DataFrame({\"Wavelength\":(gwav),\"Normalized Data\": (gdata/p_result), \"Voigt fit\": (result.best_fit/p_result), \"Time\": t, \"SNR\": snr, \"Depth\": depth, \"RV\": rv, \"Error\": err}) #[t],[snr], [depth], [rv], [err]])\n",
    "                        dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/MIKE_Voigt_fitting/all_abs_lines/snr_cutoff_{snr:.2g}/line_depth_cutoff_{depth:.2g}/{t}/\"\n",
    "                        dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "                        os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "                        file_end = f\"{str(line)}_order{j}.txt\"\n",
    "                        file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "                        file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "                        #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "                        data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "                        \n",
    "                    else:  \n",
    "                        raise Exception(\"Line not in this order\") \n",
    "                        \n",
    "        except Exception as e:\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            file_name = exc_tb.tb_frame.f_code.co_filename\n",
    "            line_num = exc_tb.tb_lineno\n",
    "            line_str = linecache.getline(file_name, line_num).strip()\n",
    "            \n",
    "            # Print out the error message along with the line of code\n",
    "            print(f\"Error occurred at line {line_num}: {line_str}\")\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            \n",
    "            RV = np.append(RV,np.nan)\n",
    "            RV_err = np.append(RV_err,np.nan)\n",
    "            RV_weight = np.append(RV_weight,np.nan)\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    #weighted mean and standard deviation error calculations\n",
    "    if np.any(~np.isnan(RV)) :\n",
    "         #Weighted mean and standard deviation error calculations\n",
    "        mean = (np.sum((RV[~np.isnan(RV)] * RV_weight[~np.isnan(RV_weight)])) / (np.sum(RV_weight[~np.isnan(RV_weight)])))\n",
    "        mean_error = np.sqrt(np.sum(RV_weight[~np.isnan(RV_weight)] * RV_err[~np.isnan(RV_err)]**2) / np.sum(RV_weight[~np.isnan(RV_weight)])+ v_precision**2) \n",
    "\n",
    "    else:\n",
    "        mean = np.nan\n",
    "        mean_error = np.nan\n",
    "    \n",
    "    print(\"The rv mean and error is\", mean,mean_error)\n",
    "\n",
    "        \n",
    "    return mean, mean_error,n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fit all of the individual voigt profiles together as a compound model\n",
    "\n",
    "def Combined_Voigt(wavelength,flux,moving_avg,errors,c_lines,time):\n",
    "    v_precision = 0.2 #kniazev stability analysis for SALT HRS LRS\n",
    "    poly_model = PolynomialModel(degree=4)\n",
    "    params = poly_model.guess(flux, x=wavelength)\n",
    "    composite_model = poly_model\n",
    "\n",
    "    #make list of parameters for the fit errs\n",
    "    fit_err = []\n",
    "\n",
    "    # Calculate a single radial velocity offset for all Voigt profiles\n",
    "    global_offset_rv = 'global_offset_rv'\n",
    "    voigt_center = f'voigt_{c_lines[0][0]}center'  # Getting the center of the first Voigt line\n",
    "    global_rv_expr = f'(line - {voigt_center}) / line * 299792'  # Correct expression\n",
    "    params.add(global_offset_rv, expr=global_rv_expr,vary=True, min=0, max=60)\n",
    "\n",
    "    for i, (line, _) in enumerate(c_lines):\n",
    "        #first line analysis\n",
    "        line= i[0]\n",
    "        i1 = c_lines[0]\n",
    "        line1 = i1[0]\n",
    "\n",
    "        voigt_model = VoigtModel(prefix=f'voigt_{line}')  # Unique prefix for each Voigt model\n",
    "        composite_model += voigt_model  # Add Voigt model to the composite model\n",
    "        \n",
    "        # Calculate the offset between voigt_center and line center\n",
    "        offset_rv = f'voigt_{line}center_offset' #name of parameter\n",
    "        rv_value = f\"(line - voigt_{line}center /line) * 299792\" #calculated value of parameter\n",
    "        params.add(offset_rv, expr=rv_value)  # Add offset parameter\n",
    "\n",
    "        \n",
    "        # Initialize Voigt model parameters including the offset parameter\n",
    "        params.update(voigt_model.make_params(voigt_amplitude=-0.01, voigt_center=line,\n",
    "                                               voigt_sigma=0.3, voigt_gamma=0.3,\n",
    "                                               **{offset_rv: 0}))\n",
    "        \n",
    "        # Set constraints for the center and offset parameters\n",
    "        params[f'voigt_{line}amplitude'].min = -100\n",
    "        params[f'voigt_{line}amplitude'].max = 0.0\n",
    "        params[f'voigt_{line}center'].min = line - 1  # Adjust constraints for line center\n",
    "        params[f'voigt_{line}center'].max = line + 2\n",
    "        params[offset_rv].set(expr = \"global_offset_rv\")\n",
    "        fit_errs.append((f\"(result.params[voigt_{line}center].sigma /{line}) * 299792\"))\n",
    "\n",
    "\n",
    "    result = composite_model.fit(flux, params, x=wavelength, weights=1/errors)\n",
    "    RV = result.values[offset_rv]\n",
    "    print(RV)\n",
    "\n",
    "    #for i in c_lines \n",
    "\n",
    "  \n",
    "    \n",
    "    t = time.datetime\n",
    "\n",
    "    #save the datafiles instead of stacking the plot here\n",
    "    data = pd.DataFrame({\"Wavelength\":(wavelength),\"Flux\": flux, \"Voigt template fit\": (result.best_fit), \"Time\": t, \"RV\": RV, \"Error\": err}) #[t],[snr], [depth], [rv], [err]])\n",
    "    dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Template_Voigt_fitting/all_abs_lines/{t}\"\n",
    "    dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "    os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "    file_end = f\".txt\"\n",
    "    file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "    file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "    #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "    data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "\n",
    "    return RV,v_precision,t\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gaussian fitting for the sky spectra to determine the correction of the \n",
    "\n",
    "def Sky_Gaussian(wavelength,flux,sky_lines,time):\n",
    "    window_size = 10\n",
    "    rv_corr = []\n",
    "    corr_err = []\n",
    "    corr_weight = []\n",
    "    t = time.datetime\n",
    "    D = np.array([])\n",
    "    line_depth_cutoff = 3\n",
    "    \n",
    "    for i in sky_lines:\n",
    "        line = i[1]\n",
    "            \n",
    "        u_loc = np.searchsorted(wavelength, (i[1]+(window_size/2)))\n",
    "        closest_value = wavelength[max(0, u_loc-1)]\n",
    "        u_bound = np.where(wavelength == closest_value)\n",
    "        u_bound = int(u_bound[0])\n",
    "        #find the lower bound of the window\n",
    "        l_loc = np.searchsorted(wavelength, (i[1]-(window_size/2)))\n",
    "        closest_value = wavelength[max(0, l_loc-1)]\n",
    "        l_bound = np.where(wavelength == closest_value)\n",
    "        l_bound = int(l_bound[0])\n",
    "        #make small datasets around the line\n",
    "        gdata = flux[l_bound:u_bound]\n",
    "        gwav = wavelength[l_bound:u_bound]  \n",
    "\n",
    "        try:\n",
    "            #CHANGE THE SNR VALUE TO FILTER DATASETS THAT ARE NOT FITTING WELL\n",
    "            #We want this code to try the next line in the dataset \n",
    "            if D <= line_depth_cutoff:\n",
    "                raise Exception(\"Sky Line not significant: Line depth < \"+str(line_depth_cutoff))\n",
    "            ####\n",
    "            #Fitting a gaussian model to the line to find the central point\n",
    "            gaussian_model = GaussianModel(prefix = 'gaussian_')\n",
    "            poly_model = PolynomialModel(degree=2)\n",
    "            params = gaussian_model.make_params(gaussian_amplitude=max(gdata) - min(gdata),\n",
    "                                                gaussian_center=line, \n",
    "                                                gaussian_sigma=0.3)\n",
    "            params += poly_model.guess(gdata, x=gwav)\n",
    "            composite_model = poly_model + gaussian_model\n",
    "            result = composite_model.fit(gdata, params, x=gwav)\n",
    "\n",
    "            #Line_Offset = result.params['gaussian_center'] - line\n",
    "\n",
    "            #Plotting the spectral line with the gaussian fit\n",
    "            plt.figure(facecolor=\"white\")\n",
    "            plt.title(f\"{t.year}/{t.month}/{t.day}\")\n",
    "            plt.plot(gwav, gdata, color='black', linewidth=0.5)\n",
    "            plt.plot(gwav,result.best_fit,color = 'red')\n",
    "            #plt.xlim(l_bound, u_bound )\n",
    "            plt.ylim(0,1.2* max(gdata))\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            wavmax = gwav[np.argmax(gdata)]\n",
    "            Line_Offset = wavmax - line\n",
    "\n",
    "            #Calculating the radial velocity correction from this fit \n",
    "            #swap out gaussian centre value for maxwavelength as needed\n",
    "            rv = ((Line_Offset/line) * 299792)\n",
    "            if result.params['gaussian_center'].stderr is not None:\n",
    "                err = (result.params['gaussian_center'].stderr / result.params['gaussian_center'].value) * 299792\n",
    "                weight = 1 / err\n",
    "            else:\n",
    "                # Handle the case where stderr is None\n",
    "                print(\"Warning: Standard error is None, skipping calculation of error\")\n",
    "                err = np.nan\n",
    "                weight = np.nan\n",
    "            #err =(((result.params['gaussian_center'].stderr)/result.values['gaussian_center']) * 299792)\n",
    "        #weight = 1/err\n",
    "        print(\"The sky offset is\",str(Line_Offset),str(line))\n",
    "        rv_corr = np.append(rv_corr, rv)\n",
    "        corr_err = np.append(corr_err,err)\n",
    "        corr_weight = np.append(corr_weight, weight)\n",
    "\n",
    "    rv_corr_mean = np.average(rv_corr, weights=corr_weight)\n",
    "    corr_err_mean = np.sqrt(np.sum(corr_weight * corr_err**2) / np.sum(corr_weight))\n",
    "\n",
    "    return rv_corr_mean,corr_err_mean\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the cross coreelation of the entire spectrum as created by Lalitha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlation(reference_flux, observed_flux,wavelength_observed,time):\n",
    "    # Perform cross-correlation using NumPy's correlate function. The reference spectra here is any one spectra. \n",
    "    # Ideally I would take the first epoch spectra as the reference spectra and measure the radial velocity \n",
    "    # shift with respect to the first epoch spectra.\n",
    "    cc = correlate(observed_flux, reference_flux, mode='full')\n",
    "\n",
    "    \n",
    "    # Determine the velocity axis corresponding to the cross-correlation result\n",
    "    rv = (np.arange(len(cc)) - (len(cc) - 1) / 2) * \\\n",
    "                     (wavelength_observed[1] - wavelength_observed[0])\n",
    "\n",
    "    \n",
    "    n_cc = cc/np.abs(max(cc))\n",
    "    gaussian1 = GaussianModel(prefix = \"gaussian1_\")\n",
    "    gaussian2 = GaussianModel(prefix = \"gaussian2_\")\n",
    "    linear_model = LinearModel(prefix = \"linear_\")\n",
    "    composite_model = gaussian1 + gaussian2 + linear_model\n",
    "    params = linear_model.make_params(slope=0, intercept=np.min(n_cc))\n",
    "\n",
    "    params += gaussian1.make_params(gaussian1_amplitude=0.1, gaussian1_center=0, gaussian1_sigma=1.)\n",
    "    params['gaussian1_amplitude'].min = 0\n",
    "    params['gaussian1_amplitude'].max = 1\n",
    "    params['gaussian1_center'].min = (-30)\n",
    "    params['gaussian1_center'].max = (30)\n",
    "    params['gaussian1_sigma'].min = (0)\n",
    "    params['gaussian1_sigma'].max = (15)\n",
    "\n",
    "    params += gaussian2.make_params(gaussian2_amplitude=0.01, gaussian2_center=0, gaussian2_sigma=50)\n",
    "    params['gaussian2_amplitude'].min = 0\n",
    "    params['gaussian2_amplitude'].max = 1\n",
    "    params['gaussian2_center'].min = (-30)\n",
    "    params['gaussian2_center'].max = (30)\n",
    "    params['gaussian2_sigma'].min = (10)\n",
    "    params['gaussian2_sigma'].max = (1000)\n",
    "            \n",
    "    result = composite_model.fit(n_cc, params, x=rv)#,sigma = wav_errors)\n",
    "            #print(result.values['voigt_center'],result.params['voigt_center'])\n",
    "    max_rv = result.values[\"gaussian1_center\"]\n",
    "    sigma = result.params[\"gaussian1_sigma\"]\n",
    "    print(sigma)\n",
    "\n",
    "    t = time.datetime\n",
    "    plt.subplot(2,1,2)  \n",
    "    plt.errorbar(rv, n_cc, fmt='.', label='Data', zorder=0) #ccfs_err\n",
    "    plt.plot(rv, result.best_fit, 'r-', label='Fit', zorder=1)\n",
    "    plt.xlabel('Velocity (km/s)')\n",
    "    plt.ylabel('Cross-correlation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(max_rv)\n",
    "    print(f'Optimized mu: {max_rv:.2f} km/s')\n",
    "\n",
    "\n",
    "    #save the datafiles instead of stacking the plot here\n",
    "    data = pd.DataFrame({\"RV\":(rv),\"CC\": (n_cc), \"Bimodal fit\": (result.best_fit), \"Time\": t, \"RV\": max_rv, \"Error\": sigma.value}) #[t],[snr], [depth], [rv], [err]])\n",
    "    dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Old_method_Reference_CCF/wholespectrum/\"\n",
    "    dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "    os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "    file_end = f\"{t}.txt\"\n",
    "    file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "    file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "    #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "    data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "        #how to read the data back in \n",
    "        #data = pd.read_csv(\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/only_mg_line/snr_cutoff_16.05612842459828/2021-04-21T03:09:52.703/4481.185.txt\", sep='\\t')\n",
    "\n",
    "    return max_rv,sigma.value \n",
    "   \n",
    "    \"\"\"  # Plot the cross-correlation function\n",
    "    plt.figure()\n",
    "    plt.text(0,0,time)\n",
    "    plt.plot(velocity_axis, cross_correlation_result, label = \"CCF\")\n",
    "    #plt.plot(velocity_axis, result.best_fit,label = \"Voigt Fit\", color = \"red\")\n",
    "    plt.xlabel('Velocity (km/s)')\n",
    "    plt.ylabel('Arbitrary CCF value')\n",
    "    #plt.xlim(-300,300)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Find the velocity shift corresponding to the maximum cross-correlation value\n",
    "    #velocity_shift = centre\n",
    "    #error = stdev\n",
    "    max_index = np.argmax(cross_correlation_result)\n",
    "    velocity_shift = velocity_axis[max_index]\n",
    "    error = 1\n",
    "    #print('Velocity Shift:', velocity_shift, 'km/s')\n",
    "    return velocity_shift, error \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working cross- correlation pyastronomy crosscorrRV with bimodal gaussian fits to the central cc peak\n",
    "\n",
    "def RVCC(wavelength_reference, flux_reference,wavelength_observed,flux_observed,window_size):\n",
    "    \n",
    "    tw,tf = process_data_cc(wavelength_reference,flux_reference)\n",
    "    dw,df = process_data_cc(wavelength_observed,flux_observed)\n",
    "\n",
    "    #find the upper bound of the window\n",
    "    u_loc = np.searchsorted(tw, 5200)\n",
    "    closest_value = tw[max(0, u_loc-1)]\n",
    "    u_bound = np.where(tw == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(tw, 4100)\n",
    "    closest_value = tw[max(0, l_loc-1)]\n",
    "    l_bound = np.where(tw == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    #make new datasets to exclude the edge of the files\n",
    "    tf = tf[l_bound:u_bound]\n",
    "    tw = tw[l_bound:u_bound]\n",
    "\n",
    "    #make a new dataset for the observed wav\n",
    "    u_loc = np.searchsorted(dw, 5200) #5200\n",
    "    closest_value = dw[max(0, u_loc-1)]\n",
    "    u_bound = np.where(dw == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(dw, 4100) #4100\n",
    "    closest_value = dw[max(0, l_loc-1)]\n",
    "    l_bound = np.where(dw == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    df = df[l_bound:u_bound]\n",
    "    dw = dw[l_bound:u_bound]\n",
    "    \n",
    "    df = np.clip(df, 0, 2)\n",
    "    tf = np.clip(tf, 0, 2)\n",
    "\n",
    "    # Carry out the cross-correlation.\n",
    "    # The first and last x points of the data are skipped.\n",
    "    rv, cc = pyasl.crosscorrRV(dw, df, tw, tf, -window_size , window_size, 0.1, skipedge=50)\n",
    "    n_cc = cc/np.abs(max(cc))\n",
    "    \n",
    "\n",
    "    gaussian1 = GaussianModel(prefix = \"gaussian1_\")\n",
    "    voigt = VoigtModel(prefix = \"voigt_\")\n",
    "    gaussian2 = GaussianModel(prefix = \"gaussian2_\")\n",
    "    linear_model = LinearModel(prefix = \"linear_\")\n",
    "    composite_model = gaussian1 + gaussian2 + linear_model\n",
    "    params = linear_model.make_params(slope=0, intercept=np.min(n_cc))\n",
    "    \"\"\" \n",
    "    params += voigt.make_params(voigt_amplitude=0.1, voigt_center=0, voigt_sigma=1.,voigt_gamma=1)\n",
    "    params['voigt_amplitude'].min = 0\n",
    "    params['voigt_amplitude'].max = 1\n",
    "    params['voigt_center'].min = (-10)\n",
    "    params['voigt_center'].max = (10)\n",
    "    params['voigt_sigma'].min = (0)\n",
    "    params['voigt_sigma'].max = (15)\n",
    "    \"\"\"\n",
    "    params += gaussian1.make_params(gaussian1_amplitude=0.1, gaussian1_center=0, gaussian1_sigma=1.)\n",
    "    params['gaussian1_amplitude'].min = 0\n",
    "    params['gaussian1_amplitude'].max = 1\n",
    "    params['gaussian1_center'].min = (-30)\n",
    "    params['gaussian1_center'].max = (30)\n",
    "    params['gaussian1_sigma'].min = (0)\n",
    "    params['gaussian1_sigma'].max = (15)\n",
    "    \n",
    "    params += gaussian2.make_params(gaussian2_amplitude=0.01, gaussian2_center=0, gaussian2_sigma=50)\n",
    "    params['gaussian2_amplitude'].min = 0\n",
    "    params['gaussian2_amplitude'].max = 1\n",
    "    params['gaussian2_center'].min = (-30)\n",
    "    params['gaussian2_center'].max = (30)\n",
    "    params['gaussian2_sigma'].min = (10)\n",
    "    params['gaussian2_sigma'].max = (1000)\n",
    "            \n",
    "    result = composite_model.fit(n_cc, params, x=rv)#,sigma = wav_errors)\n",
    "    print(result.values['gaussian1_center'],result.params['gaussian1_center'])\n",
    "    max_rv = result.values[\"gaussian1_center\"]\n",
    "    sigma = result.params[\"gaussian1_sigma\"].value\n",
    "\n",
    "    #shifted wavelength data\n",
    "    rv_dw = dw * (1+ max_rv/299792)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    # Plot template and data\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Spectrum cross-correlation against reference spectrum\")\n",
    "    plt.plot(tw, tf,color='blue', linewidth=0.5 , label=\"Reference Template\")\n",
    "    plt.text(4483,1.3, f\"RV shift = {max_rv:.3f} ± {sigma:.2f}\")\n",
    "    plt.errorbar(rv_dw , df, color='green', linewidth=0.5, label=\"Shifted Observed Spectrum\") #need to shift this back by the rv shift\n",
    "    plt.xlim(4470,4490)\n",
    "    plt.ylim(0.5,1.6)\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.ylabel(\"Normalised Flux\")\n",
    "    plt.legend()\n",
    "     \n",
    "    #plot the cross-correlation function and fit\n",
    "    plt.subplot(2,1,2)  \n",
    "    plt.errorbar(rv, n_cc, fmt='.', label='Data', zorder=0) #ccfs_err\n",
    "    plt.plot(rv, result.best_fit, 'r-', label='Fit', zorder=1)\n",
    "    plt.xlabel('Velocity (km/s)')\n",
    "    plt.ylabel('Cross-correlation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(max_rv)\n",
    "    print(f'Optimized mu: {max_rv:.2f} km/s')\n",
    "\n",
    "    #save the datafiles instead of stacking the plot here\n",
    "    data = pd.DataFrame({\"template wavelength\": tw, \"template flux\": tf, \"Shifted observed wavelength\": rv_dw, \"observed flux\": df, \"RV\":(rv),\"CC\": (n_cc), \"Bimodal fit\": (result.best_fit), \"Time\": t, \"RV\": max_rv, \"Error\": sigma.value}) #[t],[snr], [depth], [rv], [err]])\n",
    "    dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Reference_CCF/bluespectrum/\"\n",
    "    dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "    os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "    file_end = f\"{t}.txt\"\n",
    "    file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "    file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "    #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "    data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "    #how to read the data back in \n",
    "    #data = pd.read_csv(\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/only_mg_line/snr_cutoff_16.05612842459828/2021-04-21T03:09:52.703/4481.185.txt\", sep='\\t')\n",
    "\n",
    "    return max_rv,sigma  \n",
    "\n",
    "    #print(rv[np.argmax(cc)])\n",
    "    #return rv, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working cross- correlation pyastronomy crosscorrRV with bimodal gaussian fits to the central cc peak\n",
    "\n",
    "def RVCC_model(model_wav, model_flux,wavelength_observed,flux_observed,window_size,b_lines):\n",
    "    tw,tf = process_data_model(model_wav,model_flux,b_lines)\n",
    "    dw,df = process_data_cc(wavelength_observed,flux_observed)\n",
    "\n",
    "    \"\"\"   #find the upper bound of the window\n",
    "    u_loc = np.searchsorted(dw, 5200)\n",
    "    closest_value = tw[max(0, u_loc-1)]\n",
    "    u_bound = np.where(tw == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(tw, 4100)\n",
    "    closest_value = tw[max(0, l_loc-1)]\n",
    "    l_bound = np.where(tw == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    #make new datasets to exclude the edge of the files\n",
    "    tf = tf[l_bound:u_bound]\n",
    "    tw = tw[l_bound:u_bound] \"\"\"\n",
    "\n",
    "    #make a new dataset for the observed wav\n",
    "    u_loc = np.searchsorted(dw, 5200) #5200\n",
    "    closest_value = dw[max(0, u_loc-1)]\n",
    "    u_bound = np.where(dw == closest_value)\n",
    "    u_bound = int(u_bound[0])\n",
    "    #find the lower bound of the window\n",
    "    l_loc = np.searchsorted(dw, 4100) #4100\n",
    "    closest_value = dw[max(0, l_loc-1)]\n",
    "    l_bound = np.where(dw == closest_value)\n",
    "    l_bound = int(l_bound[0])\n",
    "    df = df[l_bound:u_bound]\n",
    "    dw = dw[l_bound:u_bound]\n",
    "    \n",
    "    df = np.clip(df, 0, 2)\n",
    "    #tf = np.clip(tf, 0, 2)\n",
    "    \"\"\" \n",
    "    plt.figure()\n",
    "    plt.plot(tw,tf,color = \"green\",label = \"Model spectrum\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "     \"\"\"\n",
    "    # Carry out the cross-correlation.\n",
    "    # The first and last x points of the data are skipped.\n",
    "    rv, cc = pyasl.crosscorrRV(dw, df, tw, tf, -window_size + 35, window_size +35, 0.1, skipedge=500)\n",
    "    n_cc = cc/np.abs(max(cc))\n",
    "    \n",
    "    init = rv[np.argmax(cc)]\n",
    "    print(f\"The max radial velocity from the cc argmax = {init}\")\n",
    "\n",
    "    gaussian1 = GaussianModel(prefix = \"gaussian1_\")\n",
    "    voigt = VoigtModel(prefix = \"voigt_\")\n",
    "    gaussian2 = GaussianModel(prefix = \"gaussian2_\")\n",
    "    linear_model = LinearModel(prefix = \"linear_\")\n",
    "    composite_model =  gaussian1 + gaussian2 + linear_model\n",
    "    params = linear_model.make_params(slope=0, intercept=np.min(n_cc))\n",
    "    \"\"\"\n",
    "    params += voigt.make_params(voigt_amplitude=0.1, voigt_center=0, voigt_sigma=1.,voigt_gamma=1)\n",
    "    params['voigt_amplitude'].min = 0\n",
    "    params['voigt_amplitude'].max = 1\n",
    "    params['voigt_center'].min = (25)\n",
    "    params['voigt_center'].max = (55)\n",
    "    params['voigt_sigma'].min = (0)\n",
    "    params['voigt_sigma'].max = (15)\n",
    "    \"\"\"\n",
    "    params += gaussian1.make_params(gaussian1_amplitude=0.01, gaussian1_center=init, gaussian1_sigma=1.)\n",
    "    params['gaussian1_amplitude'].min = 0\n",
    "    params['gaussian1_amplitude'].max = 1\n",
    "    params['gaussian1_center'].min = (30)\n",
    "    params['gaussian1_center'].max = (45)\n",
    "    params['gaussian1_sigma'].min = (0)\n",
    "    params['gaussian1_sigma'].max = (5)\n",
    "    \n",
    "    params += gaussian2.make_params(gaussian2_amplitude=0.1, gaussian2_center=init, gaussian2_sigma=50)\n",
    "    params['gaussian2_amplitude'].min = 0\n",
    "    params['gaussian2_amplitude'].max = 1\n",
    "    params['gaussian2_center'].min = (0)\n",
    "    params['gaussian2_center'].max = (60)\n",
    "    params['gaussian2_sigma'].min = (0)\n",
    "    params['gaussian2_sigma'].max = (1000)\n",
    "            \n",
    "    result = composite_model.fit(n_cc, params, x=rv)#,sigma = wav_errors)\n",
    "    #print(result.values['voigt_center'],result.params['voigt_center'])\n",
    "    \n",
    "    #results from fit\n",
    "    max_rv = result.values[\"gaussian1_center\"]\n",
    "    sigma = result.params[\"gaussian1_sigma\"].value\n",
    "\n",
    "    #max_rv = init\n",
    "    #sigma = 0.2\n",
    "    #shifted wavelength data\n",
    "    rv_dw = dw * (1- max_rv/299792)\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    # Plot template and data\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Spectrum cross-correlation against reference spectrum\")\n",
    "    plt.plot(tw, tf,color='blue', linewidth=0.5 , label=\"Koester Model Spectrum\")\n",
    "    plt.text(4485,1.25, f\"RV shift = {max_rv:.3f} ± {sigma:.2f}\")\n",
    "    plt.errorbar(rv_dw , df, color='green', linewidth=0.5, label=\"Shifted observed spectrum\") #need to shift this back by the rv shift\n",
    "    plt.xlim(4471,4491)\n",
    "    plt.ylim(0.4,1.5)\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.ylabel(\"Normalised Flux\")\n",
    "    plt.legend()\n",
    "     \n",
    "    #plot the cross-correlation function and fit\n",
    "    plt.subplot(2,1,2)  \n",
    "    plt.errorbar(rv, n_cc, fmt='.', label='Data', zorder=0) #ccfs_err\n",
    "    plt.plot(rv, result.best_fit, 'r-', label='Fit', zorder=1)\n",
    "    plt.xlabel('Velocity (km/s)')\n",
    "    plt.ylabel('Cross-correlation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(max_rv)\n",
    "    print(f'Optimized mu: {max_rv:.2f} km/s')\n",
    "\n",
    "\n",
    "    #save the datafiles instead of stacking the plot here\n",
    "    data = pd.DataFrame({\"template wavelength\": tw, \"template flux\": tf, \"Shifted observed wavelength\": rv_dw, \"observed flux\": df, \"RV\":(rv),\"CC\": (n_cc), \"Bimodal fit\": (result.best_fit), \"Time\": t, \"RV\": max_rv, \"Error\": sigma.value}) #[t],[snr], [depth], [rv], [err]])\n",
    "    dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Model_CCF/bluespectrum/\"\n",
    "    dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "    os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "    file_end = f\"{t}.txt\"\n",
    "    file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "    file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "    #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "    data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "    #how to read the data back in \n",
    "    #data = pd.read_csv(\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/only_mg_line/snr_cutoff_16.05612842459828/2021-04-21T03:09:52.703/4481.185.txt\", sep='\\t')\n",
    "\n",
    "    return max_rv,sigma  \n",
    "\n",
    "    #print(rv[np.argmax(cc)])\n",
    "    #return rv, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RVCC_MIKE(tw,tf,dw,df,line,obs_time):\n",
    "    window_size = 100\n",
    "\n",
    "    # Create the new dataset excluding the edge points\n",
    "    exclude_points = int(0.06 * len(tf))\n",
    "    tf = tf[exclude_points:-exclude_points]\n",
    "    tw = tw[exclude_points:-exclude_points]\n",
    "    df = df[exclude_points:-exclude_points]\n",
    "    dw = dw[exclude_points:-exclude_points]\n",
    "\n",
    "\n",
    "    #normalize the dataset\n",
    "    ref_p_result = poly_fit(tw,tf)\n",
    "    obs_p_result = poly_fit(dw,df)\n",
    "    ntf = tf/ref_p_result\n",
    "    ndf = df/obs_p_result   \n",
    "    \n",
    "    print(tw[0],dw[0])\n",
    "    rv, cc = pyasl.crosscorrRV(dw, ndf, tw, ntf, -window_size , window_size, 0.1, skipedge=50)\n",
    "\n",
    "    n_cc = cc/np.abs(max(cc))\n",
    "    init = rv[np.argmax(cc)]\n",
    "    print(f\"The max radial velocity from the cc argmax = {init}\")\n",
    "\n",
    "    #ccf fitting \n",
    "    gaussian1 = GaussianModel(prefix = \"gaussian1_\")\n",
    "    #voigt = VoigtModel(prefix = \"voigt_\")\n",
    "    gaussian2 = GaussianModel(prefix = \"gaussian2_\")\n",
    "    linear_model = LinearModel(prefix = \"linear_\")\n",
    "    composite_model =  gaussian1 + gaussian2 + linear_model\n",
    "    params = linear_model.make_params(slope=0, intercept=np.min(n_cc))\n",
    "    \"\"\"\n",
    "    params += voigt.make_params(voigt_amplitude=0.1, voigt_center=0, voigt_sigma=1.,voigt_gamma=1)\n",
    "    params['voigt_amplitude'].min = 0\n",
    "    params['voigt_amplitude'].max = 1\n",
    "    params['voigt_center'].min = (25)\n",
    "    params['voigt_center'].max = (55)\n",
    "    params['voigt_sigma'].min = (0)\n",
    "    params['voigt_sigma'].max = (15)\n",
    "    \"\"\"\n",
    "    params += gaussian1.make_params(gaussian1_amplitude=0.01, gaussian1_center=init, gaussian1_sigma=1.)\n",
    "    params['gaussian1_amplitude'].min = 0\n",
    "    params['gaussian1_amplitude'].max = 1\n",
    "    params['gaussian1_center'].min = (-1)\n",
    "    params['gaussian1_center'].max = (1)\n",
    "    params['gaussian1_sigma'].min = (0)\n",
    "    params['gaussian1_sigma'].max = (5)\n",
    "    \n",
    "    params += gaussian2.make_params(gaussian2_amplitude=0.1, gaussian2_center=init, gaussian2_sigma=50)\n",
    "    params['gaussian2_amplitude'].min = 0\n",
    "    params['gaussian2_amplitude'].max = 1\n",
    "    #params['gaussian2_center'].min = (-)\n",
    "    #params['gaussian2_center'].max = (60)\n",
    "    params['gaussian2_sigma'].min = (0)\n",
    "    params['gaussian2_sigma'].max = (1000)\n",
    "            \n",
    "    result = composite_model.fit(n_cc, params, x=rv)#,sigma = wav_errors)\n",
    "    #print(result.values['voigt_center'],result.params['voigt_center'])\n",
    "    \n",
    "    #results from fit\n",
    "    max_rv = result.values[\"gaussian1_center\"]\n",
    "    sigma = result.params[\"gaussian1_sigma\"].value\n",
    "\n",
    "    #max_rv = init\n",
    "    #sigma = 0.2\n",
    "    #shifted wavelength data\n",
    "    rv_dw = dw * (1- max_rv/299792)\n",
    "\n",
    "    #plotting for the spectrum and ccf\n",
    "    t = obs_time.datetime\n",
    "    plt.figure(figsize = (10,10))\n",
    "    # Plot template and data\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Spectrum cross-correlation against reference spectrum\")\n",
    "    plt.plot(tw, ntf,color='blue', linewidth=0.5 , label=\"Reference Spectrum\")\n",
    "    plt.text(0.7*np.max(tw) ,1.25, f\"RV shift = {max_rv:.3f} ± {sigma:.2f}\")\n",
    "    plt.errorbar(rv_dw , ndf, color='green', linewidth=0.5, label=f\"Shifted spectrum: {t.year}/{t.month}/{t.day}\") \n",
    "    #plt.xlim(line -10,line +10)\n",
    "    #plt.ylim(0.4,1.5)\n",
    "    plt.xlabel(\"Wavelength (Å)\")\n",
    "    plt.ylabel(\"Normalised Flux\")\n",
    "    plt.legend()\n",
    "     \n",
    "    #plot the cross-correlation function and fit\n",
    "    plt.subplot(2,1,2)  \n",
    "    plt.errorbar(rv, n_cc, fmt='.', label='Data', zorder=0) #ccfs_err\n",
    "    plt.plot(rv, result.best_fit, 'r-', label='Fit', zorder=1)\n",
    "    plt.xlabel('V (km/s)')\n",
    "    plt.ylabel('CCF')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(max_rv)\n",
    "    print(f'Optimized mu: {max_rv:.2f} km/s')\n",
    "\n",
    "    #save the datafiles instead of stacking the plot here\n",
    "    data = pd.DataFrame({\"template wavelength\": tw, \"template flux\": ntf, \"Shifted observed wavelength\": rv_dw, \"observed flux\": ndf, \"RV\":(rv),\"CC\": (n_cc), \"Bimodal fit\": (result.best_fit), \"Time\": t, \"RV\": max_rv, \"Error\": sigma.value}) #[t],[snr], [depth], [rv], [err]])\n",
    "    dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Reference_CCF/wholespectrum/\"\n",
    "    dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "    os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "    file_end = f\"{t}.txt\"\n",
    "    file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "    file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "    #np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "    data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "    #how to read the data back in \n",
    "    #data = pd.read_csv(\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/only_mg_line/snr_cutoff_16.05612842459828/2021-04-21T03:09:52.703/4481.185.txt\", sep='\\t')\n",
    "\n",
    "    return max_rv,sigma\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lalitha's cross-correlation processing \n",
    "\n",
    "# Define the bimodal function with two Gaussian components\n",
    "def bimodal_ccf(x, offset1, amp1, mu, std1, offset2, amp2, std2):\n",
    "    y0 = 1. - offset1 - amp1 * np.exp(-(x - mu) ** 2 / (2. * std1 ** 2))\n",
    "    y1 = 1. - offset2 - amp2 * np.exp(-(x - mu) ** 2 / (2. * std2 ** 2))\n",
    "    y_total = (y0 + y1 - 1.)\n",
    "    return y_total\n",
    "\n",
    "# Define the linear function\n",
    "def linear(x, A, B):\n",
    "    return A + B * x\n",
    "\n",
    "# Define the custom model that combines the bimodal and linear functions\n",
    "def custom_model(x, offset1, amp1, mu, std1, offset2, amp2, std2, A, B):\n",
    "    return bimodal_ccf(x, offset1, amp1, mu, std1, offset2, amp2, std2) + linear(x, A, B)\n",
    "\n",
    "def CCF_processing(rv,cc):\n",
    "    # Read CCF data from the previously saved text file\n",
    "    #ccf_data = np.loadtxt('cross_correlation_results_2.txt', unpack=True)\n",
    "\n",
    "    # Extract velocity and CCF values\n",
    "    #velocity = ccf_data[0]\n",
    "    #ccfs = ccf_data[1]\n",
    "    velocity = rv\n",
    "    ccfs = cc\n",
    "\n",
    "    # Set initial guess values for the fitting\n",
    "    offset1 = 0.1\n",
    "    amp1 = 0.5\n",
    "    mu = 0  # Initial guess for the common mean value\n",
    "    std1 = 50.0\n",
    "    offset2 = 0.1\n",
    "    amp2 = 0.5\n",
    "    std2 = 5.0\n",
    "    A = np.median(ccfs) * 1.1\n",
    "    B = 0.0\n",
    "\n",
    "    # Select the continuum region for noise estimation\n",
    "    cont_region = (velocity > -100) & (velocity < 100)\n",
    "    noise_std = np.std(ccfs[cont_region])\n",
    "    ccfs_err = np.ones_like(ccfs) * noise_std\n",
    "\n",
    "    # Fit the model\n",
    "    popt, pcov = curve_fit(custom_model, velocity, ccfs, sigma=ccfs_err,\n",
    "                       p0=[offset1, amp1, mu, std1, offset2, amp2, std2, A, B],\n",
    "                       bounds=([-2, 0, -30, 0.1, -2, 0, 0.1, np.min(ccfs), -100],\n",
    "                               [2, 1, 30, 100, 2, 1, 100, np.max(ccfs), 100]))\n",
    "\n",
    "    # Calculate the reduced chi-squared value\n",
    "    residuals = ccfs - custom_model(velocity, *popt)\n",
    "    chi2 = np.sum((residuals / ccfs_err) ** 2)\n",
    "    dof = len(velocity) - len(popt)\n",
    "    red_chi2 = chi2 / dof\n",
    "    print(f\"Reduced chi-squared: {red_chi2:.3f}\")\n",
    "\n",
    "    # Plot the results\n",
    "    plt.errorbar(velocity, ccfs, fmt='.', label='Data', zorder=0) #ccfs_err\n",
    "    plt.plot(velocity, custom_model(velocity, *popt), 'r-', label='Fit', zorder=1)\n",
    "    plt.xlabel('Velocity (km/s)')\n",
    "    plt.ylabel('Cross-correlation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f'Optimized mu: {popt[2]:.2f} km/s')\n",
    "    return popt,pcov\n",
    "\n",
    "#my function for the double gaussian using lmfit\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct fitting method with emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Doppler shift the model spectrum by a given radial velocity\n",
    "def doppler_shift(wav, rv):\n",
    "    c = 299792.458  # Speed of light in km/s\n",
    "    wav_shifted = wav * (1-rv/c)\n",
    "    return wav_shifted\n",
    "\n",
    "\n",
    "def log_prior(theta):\n",
    "    rv, c1,d1,s1 = theta\n",
    "    if -100 < rv < 100 and 4480 < c1 < 4483 and -10 < d1 < 0 and 0<s1<1:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "# Modified likelihood function taking theta, wav, flux, flux err, radial velocity, and voigt parameters\n",
    "def log_likelihood(theta, x, y, yerr):\n",
    "    rv, c1,s1,d1 = theta\n",
    "    model = np.ones(len(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Voigt profiles\n",
    "    #for voigt_param in voigt_parameters:\n",
    "    center, depth, sigma = c1,d1,s1\n",
    "    voigt = depth * np.exp(-np.log(2) * ((model - center) / sigma) ** 2) / (\n",
    "        sigma * np.sqrt(np.pi)\n",
    "    )\n",
    "    # Apply constraints on the position of the voigt profile\n",
    "    #    if center < x.min() or center > x.max():\n",
    "    #        return -np.inf\n",
    "    model += voigt\n",
    "\n",
    "    # Doppler shift the wavelengths\n",
    "    model = doppler_shift(model, rv)\n",
    "\n",
    "    sigma2 = yerr ** 2 #+ model ** 2 #* np.exp(2 * log_f)\n",
    "\n",
    "\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))\n",
    "\n",
    "# Modified log probability function including radial velocity\n",
    "def log_probability(theta, x, y, yerr):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y, yerr)\n",
    "\n",
    "\n",
    "\n",
    "#x,y,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphH201707110020_u2wm.fits\")\n",
    "#p_result = poly_fit(x,y)\n",
    "#flux = x/p_result\n",
    "#yerr = calculate_error(flux)\n",
    "\n",
    "x = np.arange(4100,5200,0.001)\n",
    "y = np.ones(len(x))\n",
    "center, depth, sigma = 4481.780, -0.6,0.3\n",
    "voigt = depth * np.exp(-np.log(2) * ((x - center) / sigma) ** 2) / (\n",
    "    sigma * np.sqrt(np.pi))\n",
    "y += voigt\n",
    "yerr = 0.1 * np.ones_like(x)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.errorbar(x,y,yerr)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "rv = 36 #km/s\n",
    "#voigt_parameters = [(4481.185,-0.6,0.3)]#,(4860,-0.6,0.6)]\n",
    "c1,s1,d1 = 4481.185,0.3,-0.6\n",
    "\n",
    "# Define the number of walkers and steps for the sampler\n",
    "nwalkers = 32\n",
    "nsteps = 1000\n",
    "\n",
    "# Initialize the walkers around the maximum likelihood solution\n",
    "initial_guess = rv,c1,s1,d1\n",
    "ndim = len(initial_guess)\n",
    "pos = [np.array(initial_guess) + 1e-4 * np.random.randn(len(initial_guess)) for _ in range(nwalkers)]\n",
    "\n",
    "# Set up the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim,log_probability, args=(x, y, yerr))\n",
    "\n",
    "# Run the MCMC sampling\n",
    "sampler.run_mcmc(pos, nsteps, progress=True);\n",
    "\n",
    "# Extract the samples\n",
    "samples = sampler.get_chain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model data\n",
    "\"\"\" l = np.linspace(1,100,1000)\n",
    "c = [20, 40]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "rv = 30\n",
    "sigma = 0.03\n",
    "w1 = 2\n",
    "d1 = 0.5\n",
    "w2 = 1\n",
    "d2 = 1\n",
    "theta = [rv, w1, d1, w2, d2]\n",
    "dist = norm(f(l, theta), sigma)\n",
    "D = dist.rvs()\n",
    "\n",
    "plt.plot(l, f(l,theta), '-')\n",
    "plt.plot(l, D, '.')\n",
    "\n",
    "n = len(c)\n",
    "nDims = 2*n + 2\n",
    "\n",
    "wmin = 0\n",
    "wmax = 10\n",
    "dmin = 0\n",
    "dmax = 1\n",
    "sigma_min = 0\n",
    "sigma_max = 1\n",
    "Delta_min = 0\n",
    "Delta_max = 60 \"\"\"\n",
    "\n",
    "#real data\n",
    "\n",
    "\n",
    "#importing, cropping, and normalising data\n",
    "wav,flux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170714/product/mbgphH201707140017_u2wm.fits\")\n",
    "#rwav,rflux,t,star = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170711/product/mbgphR201707110020_u2wm.fits\")\n",
    "#wav = np.concatenate((bwav,rwav))\n",
    "#flux = np.concatenate((bflux,rflux))\n",
    "#lower bound of window\n",
    "u_loc = np.searchsorted(wav, 5200) #5200\n",
    "closest_value = wav[max(0, u_loc-1)]\n",
    "u_bound = np.where(wav == closest_value)\n",
    "u_bound = int(u_bound[0])\n",
    "#find the lower bound of the window\n",
    "l_loc = np.searchsorted(wav, 4100) #4100\n",
    "closest_value = wav[max(0, l_loc-1)]\n",
    "l_bound = np.where(wav == closest_value)\n",
    "l_bound = int(l_bound[0])\n",
    "y = flux[l_bound:u_bound]\n",
    "l = wav[l_bound:u_bound]\n",
    "p_result = poly_fit(l,y)\n",
    "D = y/p_result\n",
    "sigma = calculate_error(D)/2\n",
    "\n",
    "#initilizing values\n",
    "#priors for the values\n",
    "#gaussian profile for line centre with the x as a function\n",
    "c_vals = []\n",
    "c_priors_list = []\n",
    "lines = ([i[1] for i in b_lines])\n",
    "c1 = np.array(lines)\n",
    "for i in lines:\n",
    "    c_priors_list.append((norm.pdf(D, i, 0.04))) #make a prior function for the line \n",
    "c_priors = np.array(c_priors_list)\n",
    "\n",
    "\n",
    "\n",
    "n = len(c1)\n",
    "print(f\"number of lines: {n}\")\n",
    "#initial values\n",
    "rv = 36\n",
    "#sigma = 0.03\n",
    "s1 = np.full((n,), 0.3)\n",
    "g1 = np.full((n,), 0.3)\n",
    "d1 = np.full((n,), 0.6)\n",
    "offset = np.full((n,), 0)\n",
    "\n",
    "\n",
    "arrays_theta = [rv, s1, g1, d1, c1,offset]  # Assuming theta is a list containing both scalar values and arrays/lists\n",
    "theta = []  # Initialize an empty list to store the expanded values\n",
    "# Iterate through each element in theta\n",
    "for item in arrays_theta:\n",
    "    # Check if the current item is an array/list\n",
    "    if isinstance(item, (list, np.ndarray)):\n",
    "        # If it is an array/list, extend the expanded_theta list with its elements\n",
    "        theta.extend(item)\n",
    "    else:\n",
    "        # If it's a scalar value, append it directly to the expanded_theta list\n",
    "        theta.append(item)\n",
    "print(\"---------\")\n",
    "print(f\"the starting values are{theta}\")\n",
    "print(\"---------\")\n",
    "#limits on the priors\n",
    "cmin = c1 - 0.2\n",
    "cmax = c1 + 0.2\n",
    "print(f\"The lines we are using, and their minimum/maximum wavelengths are: {c1}\")\n",
    "print(f\"{cmin}\")\n",
    "print(f\"{cmax}\")\n",
    "smin = 0 #sigma of the fit\n",
    "smax = 5\n",
    "gmin = 0 #gamma of the fit\n",
    "gmax = 5\n",
    "dmin = 0 #depth of the fit\n",
    "dmax = 2\n",
    "offset_min = -1 #offset of the fit\n",
    "offset_max = 1\n",
    "Delta_min = -50 #RV shift\n",
    "Delta_max = 100\n",
    "\n",
    "\n",
    "\n",
    "def f(l, theta):\n",
    "    #function to generate the model\n",
    "    Delta = float(theta[0])\n",
    "    s = theta[1::n]\n",
    "    g = theta[2::n]\n",
    "    d = theta[3::n]\n",
    "    c = theta[4::n]\n",
    "    offset_values = theta[5::n] \n",
    "    return 1 + offset + np.sum([(-voigt_profile((l-(ci*(1+Delta/299792.0))),si, gi )*di) for ci, si,gi, di,offset in zip(c, s,g, d,offset_values)], axis=0)\n",
    "    \n",
    "\"\"\" for i in lines:\n",
    "    plt.errorbar(l, f(l,theta),sigma)\n",
    "    plt.plot(l,D)\n",
    "    plt.xlim(line -10,line +10 )\n",
    "    plt.ylim(0.4,1.6)\n",
    "    plt.show() \"\"\"\n",
    "    \n",
    "\n",
    "prior_mini = np.array([Delta_min] + [smin,gmin, dmin, offset_min]*n ) \n",
    "prior_min = np.concatenate((prior_mini,cmin))\n",
    "\n",
    "prior_maxi = np.array([Delta_max] + [smax,gmax, dmax, offset_max ]*n )\n",
    "prior_max = np.concatenate((prior_maxi,cmax))\n",
    "\n",
    "\n",
    "def log_likelihood(theta, l, D,sigma):\n",
    "    residual = D - f(l, theta[:-1])\n",
    "    return -0.5 * np.sum((residual / sigma)**2 + np.log(2 * np.pi * sigma**2))\n",
    "\n",
    "# Define the log prior function\n",
    "def log_prior(theta):\n",
    "    total_log_prior = 0\n",
    "    #defining priors again\n",
    "    Delta = theta[0]\n",
    "    s_values = theta[1::n]\n",
    "    g_values = theta[2::n]\n",
    "    d_values = theta[3::n]\n",
    "    c_values = theta[4::n]\n",
    "    offset_values = theta[5::n] \n",
    "\n",
    "    #setting the logprior to False if outside the priors\n",
    "    if not (Delta_min < Delta < Delta_max):\n",
    "        return -np.inf\n",
    "    if not (Delta_min < Delta < Delta_max):\n",
    "        return -np.inf\n",
    "\n",
    "    for (s,g, d,c,offs,c_init) in (zip(s_values, g_values, d_values,c_values,offset_values, c1)):\n",
    "        if not (smin < s < smax) or not (gmin < g < gmax) or not (dmin < d < dmax) or not (c_init-0.2 < c < c_init+0.2) or not (offset_min < offs < offset_max):\n",
    "            return -np.inf\n",
    "    #adding a logprior function\n",
    "    #c_prior_logpdf = np.sum(np.log(c_priors.pdf(c_values)))\n",
    "    #total_log_prior += c_prior_logpdf\n",
    "    return total_log_prior\n",
    "\n",
    "# Define the log posterior function\n",
    "def log_probability(theta, l, D, sigma):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, l, D, sigma)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize walker positions using Latin hypercube sampling\n",
    "\"\"\" \n",
    "from pyDOE import lhs\n",
    "nwalkers = 64\n",
    "ndim = len(theta)  # Assuming n is defined elsewhere\n",
    "bounds = np.array([prior_min, prior_max]).T  # Define bounds for each parameter\n",
    "lhs_samples = lhs(ndim, samples=nwalkers)\n",
    "data = pd.DataFrame(lhs_samples)\n",
    "pos = bounds[:, 0] + lhs_samples * (bounds[:, 1] - bounds[:, 0])\n",
    " \"\"\"\n",
    "\n",
    " \n",
    "#initialise walkers and pos using normal gaussian distributions\n",
    "pos = (theta) + 1e-4 * np.random.randn(64, len(theta))\n",
    "nwalkers, ndim = pos.shape \n",
    "\n",
    "\n",
    "\n",
    "# Run the sampler\n",
    "nsteps = 2000\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, args=(l,D,sigma))\n",
    "sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paramnames = [[('RV', r'\\Delta')]] + [[(f's{i}', f's_{i}'), (f'g{i}', f'g_{i}'),(f'd{i}',f'd_{i}'),(f'c{i}',f'c_{i}'),(f'offset{i}',f'offset_{i}')] for i in range(n)] \n",
    "paramnames = [item for sublist in paramnames for item in sublist]\n",
    "\n",
    "# Get the samples\n",
    "samples = sampler.get_chain(discard=1000, thin=1, flat=False)\n",
    "labels = [name_tuple[0] for name_tuple in paramnames]\n",
    "# Plot the traces\n",
    "\n",
    "#plotting all the variables\n",
    " \n",
    "fig, axes = plt.subplots(len(labels), figsize=(10, 10), sharex=True)\n",
    "#labels = [\"rv\",\"w1\",\"d1\",\"w2\",\"d2\",\"sigma\"]\n",
    "for i in range(len(labels)):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes[-1].set_xlabel(\"step number\")\n",
    "plt.show() \n",
    "\"\"\" \n",
    "fig, axes = plt.subplots(1, figsize=(10, 6), sharex=True)\n",
    "axes.plot(samples[:, :, 0], \"k\", alpha=0.3)\n",
    "axes.set_xlim(0, len(samples))\n",
    "axes.set_ylabel(\"RV (km/s)\")\n",
    "axes.yaxis.set_label_coords(-0.1, 0.5)\n",
    "axes.set_xlabel(\"step number\")\n",
    "plt.show() \"\"\"\n",
    "\n",
    "# Flatten the samples\n",
    "flat_samples = sampler.get_chain(discard=100, thin=15, flat=True)\n",
    "percentiles = np.percentile(flat_samples, [16, 50, 84], axis=0)  # This gives the 16th, 50th (median), and 84th percentiles\n",
    "\n",
    "\n",
    "print(percentiles)\n",
    "# Compute the medians of the parameter samples\n",
    "#variables = percentiles[1]\n",
    "variables = np.median(flat_samples, axis=0)\n",
    "#print(rv,w1,c1,w2,c2,sigma)\n",
    "#print(variables[1:3])\n",
    "print(variables)\n",
    "\n",
    "\"\"\" \n",
    "print(f\"wav = {l}, model ={f(l,variables)}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(l, f(l,variables),color = \"red\")\n",
    "plt.plot(l,D,color = \"black\")\n",
    "plt.fill_between(l, (D - sigma), (D + sigma), color='gray', alpha=0.3)                   \n",
    "plt.xlim(4481 -10,4481 +10 )\n",
    "plt.ylim(0.4,1.6)\n",
    "plt.show()\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels,truths=[variables])\n",
    "\"\"\" \n",
    "variables = {\n",
    "    \"rv\": rv,\n",
    "    \"w1\": w1,\n",
    "    \"c1\": c1,\n",
    "    \"w2\": w2,\n",
    "    \"c2\": c2,\n",
    "    \"sigma\": sigma} \n",
    "# Annotate variables on the plot\n",
    "for i, (var_name, var_value) in enumerate(variables.items()):\n",
    "    plt.annotate(f\"{var_name}: {var_value:.5f}\", xy=(-1, 5.5 - i * 0.5), xycoords='axes fraction', fontsize=20, ha='left')\n",
    "\"\"\"\n",
    "# Annotate variables on the plot\n",
    "for i, var_value in enumerate(variables.items()):\n",
    "    plt.annotate(f\" {var_value:.5f}\", xy=(-1, 4.5 - i * 0.5), xycoords='axes fraction', fontsize=20, ha='left')\n",
    "\n",
    "#save data\n",
    "time = t.datetime\n",
    "#save the datafiles instead of stacking the plot here\n",
    "data = pd.DataFrame({\"labels\": labels, \"flat samples\": flat_samples, \"truths\": variables,\"percentiles\": percentiles, \"wavelength\": l, \"model flux\":(f(l,variables)),\"flux\": (D),\"time\": time}) #[t],[snr], [depth], [rv], [err]])\n",
    "dir_name = f\"/data/wdplanetary/omri/Output/resultfiles/WD1929/MCMC_Bayesian/bluespectrum/\"\n",
    "dir_name_without_spaces = dir_name.replace(\" \", \"\")\n",
    "os.makedirs(dir_name_without_spaces, exist_ok=True)\n",
    "file_end = f\"{time}_test.txt\"\n",
    "file_name = os.path.join(dir_name_without_spaces, file_end)\n",
    "file_name_without_spaces = file_name.replace(\" \", \"\")\n",
    "#np.savetxt(file_name_without_spaces, data, delimiter=',', fmt = '%f') #\"fmt=['%f', '%f', '%f', '%s', '%f', '%f', '%f', '%f'])\n",
    "data.to_csv(file_name_without_spaces, sep='\\t', index=False)\n",
    "#how to read the data back in \n",
    "#data = pd.read_csv(\"/data/wdplanetary/omri/Output/resultfiles/WD1929/Voigt_fitting/only_mg_line/snr_cutoff_16.05612842459828/2021-04-21T03:09:52.703/4481.185.txt\", sep='\\t')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = corner.corner(\n",
    "    flat_samples, labels=labels, truths=[m_true, b_true, np.log(f_true)]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CrossCorrRV run file for the self-correlation\n",
    "window_size = 100.\n",
    "wav_reference,flux_reference,ref_time,ref_object = Get_Wavelength_Flux_File(\n",
    "    '/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170722/product/mbgphH201707220018_u2wm.fits')\n",
    "btime_strings = [(ref_time)]\n",
    "bvels = [(0)]\n",
    "brverrs = [(0)]\n",
    "snr_cutoff = 8\n",
    "\n",
    "for i in b_files:\n",
    "    wav_observed,flux_observed,obs_time,obs_object = Get_Wavelength_Flux_File(i)\n",
    "    if obs_object.startswith(str(ref_object)):\n",
    "        snr = np.median(flux_observed)/(np.std(flux_observed))\n",
    "        #print(snr)\n",
    "        rv_max,sigma = RVCC(wav_reference,flux_reference,wav_observed,flux_observed,window_size)\n",
    "        #bvels.append((rv[np.argmax(cc)]))\n",
    "        #rv_max,sigma = CCF_processing_gaussian(rv,cc)\n",
    "        #popt,pcov = CCF_processing(rv,cc)\n",
    "        #print(popt,pcov)\n",
    "        bvels.append(rv_max)\n",
    "        btime_strings.append(obs_time)\n",
    "        brverrs.append(sigma)\n",
    "    else:\n",
    "        print(\"This is a file for\" + str(obs_object))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to manually check data at the end of each run\n",
    "print(np.mean(bvels))\n",
    "print(bvels)\n",
    "print(brverrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting for the cross_correlation\n",
    "t_list = Time(btime_strings, scale='utc')\n",
    "times = t_list.datetime \n",
    "t0 = times[0]\n",
    "tdays = [(dt - t0).days for dt in times]\n",
    "tarray = np.array(tdays)\n",
    "rvarray = np.array(bvels)\n",
    "rverrsarray = np.array(brverrs)\n",
    "\n",
    "#rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "\"\"\" filtered_indices = np.where((rverrsarray < 100) & (rvarray > 0))[0] #& (rvarray > 10)\n",
    "filtered_rvarray = rvarray[filtered_indices]\n",
    "filtered_tarray = tarray[filtered_indices]\n",
    "filtered_rverrsarray = rverrsarray[filtered_indices]\n",
    "filtered_rvcorrsarray = rvcorrsarray[filtered_indices] \"\"\"\n",
    "\n",
    "bvels= bvels[:len(btime_strings)]\n",
    "\n",
    "unique_times = np.unique(tarray)\n",
    "\n",
    "averages = np.array([[t, np.nanmean(rvarray[tarray == t]), np.nanmean(rverrsarray[tarray == t])] for t in unique_times])\n",
    "#averages = averages[np.isnan(averages[:,1])]\n",
    "\n",
    "#print(averages)\n",
    "file_name = \"/data/wdplanetary/omri/Output/resultfiles/crosscorrRV_bluespec_results.txt\"\n",
    "np.savetxt(file_name, averages, delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radial Velocity variations with time plot\n",
    "\n",
    "t = averages[:, 0]\n",
    "v = averages[:, 1] \n",
    "mean = np.mean(v)\n",
    "#errs = np.full(len(v),0.5)\n",
    "errs = averages[:, 2] \n",
    "stdev = np.std(v)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(facecolor='white',figsize = (14,16))\n",
    "plt.errorbar(t, v, yerr=errs,fmt = '.k',lw = 0.4)\n",
    "plt.fill_between(t, -stdev, stdev, color='gray',label = \"1 sigma\", alpha=0.4)\n",
    "plt.fill_between(t, -3* stdev, 3* stdev, color='gray',label = \"3 sigma\", alpha=0.2)\n",
    "plt.legend()\n",
    "legend_pos = plt.gca().get_legend().get_texts()[0].get_position()\n",
    "plt.text(1250, 10 - 1, f\"RV mean = {mean:.3g}\")\n",
    "plt.text(1250, 10 - 2, f'sigma = {stdev:.3g}')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time - Days\")\n",
    "plt.ylabel(\"Change in Radial Velocity - km/s\")\n",
    "#plt.title(\"WD1929+012 Radial Velocity variations using Voigt fits - SALT data\")\n",
    "plt.text\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(-20,20)\n",
    "#plt.xlim(0,112)\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/DeltaRV_CrossCorr_bluespec.pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "frequencies = np.linspace(0.001,1,1000)\n",
    "#frequencies = np.linspace(1/1000, 50, num=400000)\n",
    "\n",
    "power = LombScargle(t, v,errs).power(frequencies)\n",
    "\n",
    "plt.plot(frequencies, power)\n",
    "plt.title(\"Periodogram of the radial velocity measurements of WD1929+012\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.figure(facecolor='white')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runfile for the cross-correlation with the model spectrum\n",
    "star = \"WD1929+012\"\n",
    "\n",
    "#import the Koester model\n",
    "data_path = \"/home/omn24/Documents/Code/koester2/da21000_800.dk.dat.txt\"\n",
    "wavelengths = []\n",
    "fluxes = []\n",
    "\n",
    "with open(data_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines[6:]:\n",
    "  # Split the line into columns (assuming whitespace as delimiter)\n",
    "    columns = line.split()\n",
    "    \n",
    "    # Convert wavelength and flux to float and append to respective lists\n",
    "    wavelengths.append(columns[0])\n",
    "    fluxes.append(columns[1])\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "m_wav = np.array(wavelengths).astype(float)\n",
    "m_flux = np.array(fluxes).astype(float)\n",
    "model_wav,model_flux = process_data_model(m_wav,m_flux,b_lines)\n",
    "\n",
    "\n",
    "window_size = 100.\n",
    "btime_strings = []\n",
    "bvels = []\n",
    "brverrs = []\n",
    "snr_cutoff = 10\n",
    "\n",
    "#running part\n",
    "for i in b_files:\n",
    "    wav_observed,flux_observed,obs_time,obs_object = Get_Wavelength_Flux_File(i)\n",
    "    \n",
    "    \n",
    "    if obs_object.startswith(star):\n",
    "        #snr = np.median(flux_observed)/(np.std(flux_observed))\n",
    "        #print(snr)\n",
    "        \n",
    "        rv_max,sigma = RVCC_model(model_wav, model_flux,wav_observed,flux_observed,window_size,b_lines)\n",
    "        #bvels.append((rv[np.argmax(cc)]))\n",
    "        #rv_max,sigma = CCF_processing_gaussian(rv,cc)\n",
    "        #popt,pcov = CCF_processing(rv,cc)\n",
    "        #print(popt,pcov)\n",
    "        bvels.append(rv_max)\n",
    "        btime_strings.append(obs_time)\n",
    "        brverrs.append(sigma)\n",
    "    else:\n",
    "        print(\"This is a file for\" + str(obs_object))\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to manually check data at the end of each run\n",
    "print(np.nanmean(bvels))\n",
    "print(bvels)\n",
    "print(brverrs)\n",
    "print(btime_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting for the model_ cross_correlation\n",
    "t_list = Time(btime_strings, scale='utc')\n",
    "times = t_list.datetime \n",
    "t0 = times[0]\n",
    "tdays = [(dt - t0).days for dt in times]\n",
    "tarray = np.array(tdays)\n",
    "rvarray = np.array(bvels)\n",
    "rverrsarray = np.array(brverrs)\n",
    "\n",
    "#rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "\"\"\" filtered_indices = np.where((rverrsarray < 100) & (rvarray > 0))[0] #& (rvarray > 10)\n",
    "filtered_rvarray = rvarray[filtered_indices]\n",
    "filtered_tarray = tarray[filtered_indices]\n",
    "filtered_rverrsarray = rverrsarray[filtered_indices]\n",
    "filtered_rvcorrsarray = rvcorrsarray[filtered_indices] \"\"\"\n",
    "\n",
    "bvels= bvels[:len(btime_strings)]\n",
    "\n",
    "unique_times = np.unique(tarray)\n",
    "\n",
    "averages = np.array([[t, np.nanmean(rvarray[tarray == t]), np.nanmean(rverrsarray[tarray == t])] for t in unique_times])\n",
    "#averages = averages[np.isnan(averages[:,1])]\n",
    "\n",
    "#print(averages)\n",
    "file_name = \"/data/wdplanetary/omri/Output/resultfiles/crosscorrRV_model_mg4481_bothlines_results.txt\"\n",
    "np.savetxt(file_name, averages, delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radial Velocity variations with time plot\n",
    "\n",
    "t = averages[:, 0]\n",
    "v = averages[:, 1] \n",
    "mean = np.mean(v)\n",
    "v = v- np.mean(v)\n",
    "#errs = np.full(len(v),0.2)\n",
    "errs = averages[:, 2] + 0.2\n",
    "stdev = np.std(v)\n",
    "\n",
    "\"\"\" fig, (ax_t, ax_w) = plt.subplots(2, 1,figsize = (10,6), constrained_layout=True)\n",
    "ax_t.errorbar(t, y, yerr=dy, fmt='b+', label='Data with Errors')\n",
    "ax_t.set_xlabel('Time [days]')\n",
    "ax_t.legend()\n",
    "\n",
    "ax_w.plot(frequency, power)\n",
    "ax_w.set_xlabel('Angular frequency [rad/days]')\n",
    "ax_w.set_ylabel('Normalized amplitude')\n",
    "plt.show() \"\"\"\n",
    "\n",
    "plt.figure(facecolor='white',figsize = (10,6))\n",
    "plt.errorbar(t, v, yerr=errs,fmt = '.k',lw = 0.4)\n",
    "plt.fill_between(t, -stdev, stdev, color='gray',label = \"1 sigma\", alpha=0.4)\n",
    "plt.fill_between(t, -3* stdev, 3* stdev, color='gray',label = \"3 sigma\", alpha=0.2)\n",
    "plt.legend()\n",
    "legend_pos = plt.gca().get_legend().get_texts()[0].get_position()\n",
    "plt.text(1200, 3, f\"RV mean = {mean:.3g}\")\n",
    "plt.text(1200, 2.5, f'sigma = {stdev:.3g}')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time - Days\")\n",
    "plt.ylabel(\"Change in Radial Velocity - km/s\")\n",
    "#plt.title(\"WD1929+012 Radial Velocity variations using Voigt fits - SALT data\")\n",
    "plt.text\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(-20,20)\n",
    "#plt.xlim(0,112)\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/DeltaRV_CrossCorr_model_4481_bothlines.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runfile for the Cross-correlation code\n",
    "\n",
    "#for the blue files\n",
    "btime_strings = []\n",
    "bvels = []\n",
    "brverrs = []\n",
    "bwav_reference,bflux_reference,ref_time,ref_object = Get_Wavelength_Flux_File(\n",
    "    \"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "#bwav_reference,bflux_reference = process_data_cc(bwav_reference,bflux_reference)\n",
    "\n",
    "rwav_reference,rflux_reference,ref_time,ref_object = Get_Wavelength_Flux_File(\n",
    "    \"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphR201707060019_u2wm.fits\")\n",
    "#rwav_reference,rflux_reference = process_data_cc(rwav_reference,rflux_reference)\n",
    "\n",
    "\n",
    "#rwbox,rfbox = splitfiles(bwav_reference,bflux_reference)\n",
    "btime_strings.append((ref_time))\n",
    "bvels.append(0)\n",
    "brverrs.append(0)\n",
    "w_size = 20\n",
    "\n",
    "for i,j in zip(b_files,r_files):\n",
    "    \n",
    "    v = []\n",
    "    wavelength_observed, flux_observed,time,obs_object = Get_Wavelength_Flux_File(i)\n",
    "    \n",
    "    if obs_object.startswith(star):\n",
    "\n",
    "        wavelength_observed, flux_observed = process_data_cc(wavelength_observed, flux_observed)\n",
    "        \n",
    "        # Perform cross-correlation for boxes\n",
    "        #owbox,ofbox = splitfiles(wavelength_observed, flux_observed)\n",
    "        #for i in rfbox:\n",
    "        #    print(len(i))\n",
    "        #for i in ofbox:\n",
    "        #    print(len(i))\n",
    "        #for (ofbox, rfbox) in enumerate(zip(ofbox, rfbox)):\n",
    "        #    rv = cross_correlation(rfbox, ofbox)\n",
    "        #    v.append(rv)\n",
    "        #vel = np.mean(v)\n",
    "        #err = np.std(v)\n",
    "    \n",
    "        #Perform cross-correlation for each line and the boxes around it:\n",
    "        \n",
    "        for line in b_lines:\n",
    "            r_wav, r_flux = split_files_by_line(bwav_reference,bflux_reference,line[1],w_size)\n",
    "            o_wav, o_flux = split_files_by_line(wavelength_observed,flux_observed,line[1],w_size)\n",
    "            vel,err = cross_correlation(r_flux,o_flux,o_wav,time)\n",
    "            print(vel,err)\n",
    "            \n",
    "            s_wav = [val * (1 + vel / 299792) for val in o_wav]\n",
    "            plt.plot(s_wav,o_flux,  color = 'blue',linewidth =0.5,label = (\"Shifted Spectrum\" + str(time.datetime)))\n",
    "            plt.plot(r_wav,r_flux, color = 'green',linewidth =0.5,label = \"Reference Spectrum\")\n",
    "            #plt.xlim(4470,4494)\n",
    "            #plt.ylim(0,0.02)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print(vel)\n",
    "        \n",
    "            btime_strings.append(time)\n",
    "            bvels.append(vel)\n",
    "            brverrs.append(err)\n",
    "    else:\n",
    "        print(\"This is a file for \" + str(obs_object) + \" instead of \" + str(star))\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    wavelength_observed, flux_observed,time,obs_object = Get_Wavelength_Flux_File(j)\n",
    "\n",
    "    if obs_object.startswith(star):   \n",
    "        wavelength_observed, flux_observed = process_data_cc(wavelength_observed, flux_observed)\n",
    "\n",
    "        for line in r_lines:\n",
    "            r_wav, r_flux = split_files_by_line(rwav_reference,rflux_reference,line[1],w_size)\n",
    "            o_wav, o_flux = split_files_by_line(wavelength_observed,flux_observed,line[1],w_size)\n",
    "            vel,err = cross_correlation(r_flux,o_flux,o_wav,time)\n",
    "            print(vel,err)\n",
    "            \n",
    "            s_wav = [val * (1 + vel / 299792) for val in o_wav]\n",
    "            plt.plot(s_wav,o_flux,  color = 'blue',linewidth =0.5,label = (\"Shifted Spectrum\" + str(time.datetime)))\n",
    "            plt.plot(r_wav,r_flux, color = 'green',linewidth =0.5,label = \"Reference Spectrum\")\n",
    "            #plt.xlim(4470,4494)\n",
    "            #plt.ylim(0,0.02)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            print(vel)\n",
    "        \n",
    "            btime_strings.append(time)\n",
    "            bvels.append(vel)\n",
    "            brverrs.append(err)\n",
    "        \n",
    "\n",
    "        #vel,err = cross_correlation(rflux_reference,flux_observed,wavelength_observed,time)\n",
    "        #btime_strings.append(time)\n",
    "        #bvels.append(vel)\n",
    "        #brverrs.append(err)\n",
    "    else:\n",
    "        print(\"This is a file for \" + str(obs_object) + \" instead of \" + str(star))\n",
    "        continue\n",
    "        \n",
    "\n",
    "print(np.mean(bvels))\n",
    "print(np.std(bvels))\n",
    "    #do the plotting of the shifted spectra here:\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Runfile for the PyAstronomy CrosscorrRV code\n",
    "\n",
    "#for the blue files\n",
    "btime_strings = []\n",
    "bvels = []\n",
    "#\"/home/omn24/Documents/Documents/product/mbgphH201707060019_u2wm.fits\"\n",
    "wav_reference,flux_reference,ref_time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphH201707060019_u2wm.fits\")\n",
    "btime_strings.append((ref_time))\n",
    "bvels.append(0)\n",
    "for i in b_files:\n",
    "    wavelength_observed, flux_observed,time = Get_Wavelength_Flux_File(i)\n",
    "    # Perform cross-correlation\n",
    "    vel = crosscorrRV(wav_reference,flux_reference,wavelength_observed, flux_observed)\n",
    "    btime_strings.append(time)\n",
    "    bvels.append(vel)\n",
    "\n",
    "\n",
    "\n",
    "#for the red files\n",
    "rtime_strings = []\n",
    "rvels = []\n",
    "wav_reference,flux_reference,ref_time = Get_Wavelength_Flux_File(\"/data/wdplanetary/omri/Data/WD1929+012/2017-1-SCI-031.20170706/product/mbgphR201707060019_u2wm.fits\")\n",
    "rtime_strings.append((ref_time))\n",
    "rvels.append(0)\n",
    "for i in r_files:\n",
    "    wavelength_observed, flux_observed,time = Get_Wavelength_Flux_File(i)\n",
    "    # Perform cross-correlation\n",
    "    vel = crosscorrRV(wav_reference,flux_reference,wavelength_observed, flux_observed)\n",
    "    rtime_strings.append(time)\n",
    "    rvels.append(vel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for Cross Correlation\n",
    "#Combining data points that are the same times\n",
    "t_list = Time(btime_strings, scale='utc')\n",
    "times = t_list.datetime\n",
    "tarray = np.array(times)\n",
    "rvarray = np.array(bvels)\n",
    "rverrsarray = np.array(brverrs)\n",
    "#rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "unique_times = np.unique(tarray)\n",
    "averages = np.array([[t, np.mean(rvarray[tarray == t]), np.mean(rverrsarray[tarray == t])] for t in unique_times])\n",
    "#print(averages)\n",
    "file_name = \"/data/wdplanetary/omri/Output/resultfiles/cross_correlation.txt\"\n",
    "np.savetxt(file_name, averages, delimiter=',', fmt='%f')\n",
    "\n",
    "#d = pd.DataFrame(data=[tarray, rvarray, rverrsarray], columns=[\"Time\", \"RV\", \"Errors\"])\n",
    "\n",
    "t = averages[:, 0]\n",
    "v = averages[:, 1] - np.nanmean(averages[:, 1])\n",
    "#errs = np.full(len(v),0.5)\n",
    "errs = averages[:, 2]\n",
    "t0 = t[0]\n",
    "tdays = [(dt - t0).days for dt in t]\n",
    "stdev = np.std(v)\n",
    "\n",
    "plt.figure(facecolor='white')\n",
    "plt.errorbar(tdays, v, yerr=errs,fmt = '.k')\n",
    "plt.fill_between(tdays, -stdev, stdev, color='gray',label = \"1 sigma\", alpha=0.4)\n",
    "plt.fill_between(tdays, -3* stdev, 3* stdev, color='gray',label = \"3 sigma\", alpha=0.2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time - Days\")\n",
    "plt.ylabel(\"Change in Radial Velocity - km/s\")\n",
    "plt.title(\"WD1929+012 Radial Velocity variations using \\n Cross-Correlation Functions - SALT data\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(30,50)\n",
    "#plt.xlim(0,112)\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/DeltaRVGaussian.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CCF self-correlation for MIKE\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "\n",
    "n = 0\n",
    "mike_blue_plot_dir =  \"/data/wdplanetary/omri/Output/mike/referenceCCF/\"\n",
    "stacked_plot_dir =  \"/data/wdplanetary/omri/Output/stackedplots/\"\n",
    "\n",
    "ref_order,ref_wav,ref_flux,ref_snr,refOBJECT,ref_time = read_mike_spec(\"/data/wdplanetary/laura/MIKE/Data/WD1929+011/blue/galex1931_blue_2010-06-17.fits\")\n",
    "rvs.append(0)\n",
    "time_strings.append(ref_time)\n",
    "rverrs.append(0)\n",
    "\n",
    "star = \"G\"\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(6, 20))\n",
    "\n",
    "for j in mike_b_files:\n",
    "    order,wav,flux,snr,obs_OBJECT,obs_time =read_mike_spec(j)\n",
    "    \n",
    "    xwav_list = np.empty(len(order), dtype=object)\n",
    "    padded_flux_list = np.empty(len(order), dtype=object)\n",
    "    moving_avg_list = np.empty(len(order), dtype=object)\n",
    "    errors_list = np.empty(len(order), dtype=object)\n",
    "    snr_list = np.empty(len(order), dtype=object)\n",
    "\n",
    "    print(\"imported files\")\n",
    "    if obs_OBJECT.startswith(star):\n",
    "        #then call data processing\n",
    "        for o in order:\n",
    "            #xwav,padded_flux,moving_avg = process_data_mike_gaussian(wav[o-1],flux[o-1],b_lines)\n",
    "            obs_wav, obs_flux = wav[o-1],flux[o-1]\n",
    "            r_wav,r_flux = ref_wav[o-1],ref_flux[o-1] \n",
    "\n",
    "            for i in b_lines:\n",
    "                line = float(i[1])\n",
    "                if np.min(obs_wav) <= line <= np.max(obs_wav):\n",
    "                    \n",
    "                    rv,err = RVCC_MIKE(r_wav,r_flux,obs_wav,obs_flux,line,obs_time)\n",
    "                    rvs.append(rv)\n",
    "                    time_strings.append(obs_time)\n",
    "                    rverrs.append(err)\n",
    "                    print(\"The fit has been calculated successfully\")\n",
    "            \n",
    "    else:\n",
    "        print(\"This is a file for \" + str(obs_OBJECT) + \" instead of \" + str(star))\n",
    "        continue\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voigt fitting runfile for MIKE\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "\n",
    "n = 0\n",
    "mike_blue_plot_dir =  \"/data/wdplanetary/omri/Output/mike/Gaussianline/blue/\"\n",
    "stacked_plot_dir =  \"/data/wdplanetary/omri/Output/stackedplots/\"\n",
    "\n",
    "star = \"G\"\n",
    "\n",
    "stacked_plot_dir =  \"/data/wdplanetary/omri/Output/stackedplots/\"\n",
    "#fig, ax = plt.subplots(figsize=(6, 20))\n",
    "\n",
    "for files_list in (mike_b_files, mike_r_files):\n",
    "    for j in files_list:\n",
    "        order,wav,flux,snr,OBJECT,time =read_mike_spec(j)\n",
    "        \n",
    "        xwav_list = np.empty(len(order), dtype=object)\n",
    "        padded_flux_list = np.empty(len(order), dtype=object)\n",
    "        moving_avg_list = np.empty(len(order), dtype=object)\n",
    "        errors_list = np.empty(len(order), dtype=object)\n",
    "        snr_list = np.empty(len(order), dtype=object)\n",
    "\n",
    "        print(\"imported files\")\n",
    "        if OBJECT.startswith(star):\n",
    "            #then call data processing\n",
    "            for o in order:\n",
    "                xwav,padded_flux,moving_avg = process_data_mike_gaussian(wav[o-1],flux[o-1],b_lines)\n",
    "                errors = calculate_error(padded_flux)  \n",
    "\n",
    "                xwav_list[o-1] = xwav\n",
    "                padded_flux_list[o-1] = padded_flux\n",
    "                moving_avg_list[o-1] = moving_avg\n",
    "                errors_list[o-1] = errors\n",
    "                snr_list[o-1] = snr[o-1]\n",
    "            \n",
    "            #print(xwav_list)\n",
    "            #print(snr)\n",
    "\n",
    "            print(\"Smoothed flux and calculated errors\") \n",
    "            #print(errors,padded_flux,moving_avg)      \n",
    "            #then do Gaussian\n",
    "            rv,rv_err,n = Mike_Gaussian(xwav_list,padded_flux_list,moving_avg_list,errors_list,snr_list,b_lines,time,n,mike_blue_plot_dir)\n",
    "            #now need to add to directories of wavelengths, flux_results, best_fits,snrs, line depths\n",
    "            \n",
    "            #Then find the atmospheric correction- don't need to do this as we have the stabilities\n",
    "            #rv_corr,corr_err,ax = Gaussian(xskywav,sky_padded_flux,sky_moving_avg,sky_errors,sky_lines,skytime,n,ax)\n",
    "            if np.isnan(rv) : #or np.isnan(rv_corr):\n",
    "                print(\"Fit could not be calculated\")\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                #rv_corrs.append((rv_corr))\n",
    "                rvs.append((rv)) #-rv_corr\n",
    "                rverrs.append((rv_err )) \n",
    "                time_strings.append((time))\n",
    "                print(\"The fit has been calculated successfully\")\n",
    "        else:\n",
    "            print(\"This is a file for \" + str(OBJECT) + \" instead of \" + str(star))\n",
    "            continue\n",
    "\n",
    "\"\"\" \n",
    "#Now show the stacked plot\n",
    "ax.set_xlabel('Wavelength - Å')\n",
    "ax.set_ylabel('Normalised Flux')\n",
    "#ax.set_xlim(4476.13,4486.13)\n",
    "ax.set_xlim(4851,4871)\n",
    " \n",
    "ax.axvline(x=6371.360, color='black', linestyle='--', label='Laboratory line centre')\n",
    "ax.axvline(x=(6371.360 * (1+ np.mean(rvs)/299792)), color='red', linestyle='--', label='Mean shifted line centre')\n",
    "ax.axvspan(-np.std(rvs),np.std(rvs), color='gray',label = \"1 sigma\", alpha=0.4) \n",
    "#ax.set_title('Stacked Mg II Spectral Lines from ' + str(OBJECT))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/stackedplots/mike_lines.pdf\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "print(np.nanmean(rvs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now need to plot and all for MIKE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian fits runfile for both channels\n",
    "#using code from above and iterating over the filenames\n",
    "#making arrays to plot values after\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "rv_corrs = []\n",
    "rv_corrs_errs = []\n",
    "star = \"WD1929\"\n",
    "\n",
    "n = 0\n",
    "blue_plot_dir =     \"/data/wdplanetary/omri/Output/Gaussianline/blue/\"\n",
    "red_plot_dir =      \"/data/wdplanetary/omri/Output/Gaussianline/red/\"\n",
    "sky_plot_dir =      \"/data/wdplanetary/omri/Output/Gaussianline/sky/\"\n",
    "\n",
    "#stacked_plot_dir =  \"/data/wdplanetary/omri/Output/stackedplots/\"\n",
    "#fig, ax = plt.subplots(figsize=(6, 20))\n",
    "\n",
    "for j,k,s in zip(b_files,r_files,sky_r_files):\n",
    "    wav, flux ,time,bOBJECT =Get_Wavelength_Flux_File(j)\n",
    "    skywav,skyflux,skytime,SKY = Get_Wavelength_Flux_File(s)\n",
    "    rwav,rflux,rtime,rOBJECT = Get_Wavelength_Flux_File(k)\n",
    "    \n",
    "    if bOBJECT.startswith(star):\n",
    "        #then call data processing\n",
    "        xwav,padded_flux,moving_avg = process_data_gaussian(wav,flux)\n",
    "        errors = calculate_error(padded_flux)  \n",
    "        #print(errors,padded_flux,moving_avg)      \n",
    "        #then do Gaussian\n",
    "        rv,rv_err,n = Gaussian(xwav,padded_flux,moving_avg,errors,b_lines,time,n,blue_plot_dir)\n",
    "        #now need to add to directories of wavelengths, flux_results, best_fits,snrs, line depths\n",
    "        \n",
    "        #Then find the atmospheric correction- don't need to do this as we have the stabilities\n",
    "        #rv_corr,corr_err,ax = Gaussian(xskywav,sky_padded_flux,sky_moving_avg,sky_errors,sky_lines,skytime,n,ax)\n",
    "        if np.isnan(rv): #or np.isnan(rv_corr):\n",
    "            print(\"Fit could not be calculated\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            #rv_corrs.append((rv_corr))\n",
    "            rvs.append((rv)) #-rv_corr\n",
    "            rverrs.append((rv_err )) \n",
    "            time_strings.append((time))\n",
    "            print(\"The fit has been calculated successfully\")\n",
    "    else:\n",
    "        print(\"This is a file for \" + str(bOBJECT) + \" instead of \" + str(star))\n",
    "        continue\n",
    "     \n",
    "    #Now do the same for the red wav and the sky spectra - use the sky spectra to correct the red spectra\n",
    "    if rOBJECT.startswith(star):\n",
    "        #then call data processing\n",
    "        xwav,padded_flux,moving_avg = process_data_gaussian(rwav,rflux)\n",
    "        errors = calculate_error(padded_flux)  \n",
    "        #print(errors,padded_flux,moving_avg)      \n",
    "        #then do Gaussian\n",
    "        rv,rv_err,n = Gaussian(xwav,padded_flux,moving_avg,errors,r_lines,time,n,red_plot_dir)\n",
    "        #now need to add to directories of wavelengths, flux_results, best_fits,snrs, line depths\n",
    "        \n",
    "        #Then find the atmospheric correction- don't need to do this as we have the stabilities\n",
    "        #rv_corr,corr_err,ax = Gaussian(xskywav,sky_padded_flux,sky_moving_avg,sky_errors,sky_lines,skytime,n,ax)\n",
    "        if np.isnan(rv): #or np.isnan(rv_corr):\n",
    "            print(\"Fit could not be calculated\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            #rv_corrs.append((rv_corr))\n",
    "            rvs.append((rv)) #-rv_corr\n",
    "            rverrs.append((rv_err )) \n",
    "            time_strings.append((time))\n",
    "            print(\"The fit has been calculated successfully\")\n",
    "    else:\n",
    "        print(\"This is a file for \" + str(bOBJECT) + \" instead of \" + str(star))\n",
    "        continue\n",
    "    \n",
    "\n",
    "print(np.mean(rvs))\n",
    "#print(np.mean(rv_corrs))\n",
    "#rverrs = rverrs + (np.std(rv_corrs))\n",
    "\"\"\" \n",
    "#Now show the stacked plot\n",
    "ax.set_xlabel('Wavelength - Å')\n",
    "ax.set_ylabel('Normalised Flux')\n",
    "#ax.set_xlim(4476.13,4486.13)\n",
    "ax.set_xlim(4851,4871)\n",
    " \n",
    "ax.axvline(x=6371.360, color='black', linestyle='--', label='Laboratory line centre')\n",
    "ax.axvline(x=(6371.360 * (1+ np.mean(rvs)/299792)), color='red', linestyle='--', label='Mean shifted line centre')\n",
    "ax.axvspan(-np.std(rvs),np.std(rvs), color='gray',label = \"1 sigma\", alpha=0.4) \n",
    "#ax.set_title('Stacked Mg II Spectral Lines from ' + str(OBJECT))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/stackedplots/H_4860.pdf\")\n",
    "plt.show()\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Voigt fits for SALT data with a restricted rv shift\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rvs = []\n",
    "time_strings = []\n",
    "rverrs = []\n",
    "rv_corrs = []\n",
    "rv_corrs_errs = []\n",
    "\n",
    "n = 0\n",
    "\n",
    "\n",
    "#stacked_plot_dir =  \"/data/wdplanetary/omri/Output/stackedplots/\"\n",
    "#fig, ax = plt.subplots(figsize=(6, 20))\n",
    "\n",
    "for j,k in zip(b_files,r_files):\n",
    "    wav, flux ,time,bOBJECT =Get_Wavelength_Flux_File(j)\n",
    "    rwav,rflux,rtime,rOBJECT = Get_Wavelength_Flux_File(k)\n",
    "    \n",
    "    if bOBJECT.startswith(star):\n",
    "        #then call data processing\n",
    "        bxwav,bpadded_flux,bmoving_avg = process_data_gaussian(wav,flux)\n",
    "        berrors = calculate_error(bpadded_flux)  \n",
    "        rxwav, rpadded_flux, rmoving_avg = process_data_gaussian(rwav,rflux)\n",
    "        rerrors = calculate_error(rpadded_flux)  \n",
    "        \n",
    "        xwav = np.concatenate((bxwav,rxwav))\n",
    "        padded_flux = np.concatenate((bpadded_flux,rpadded_flux))\n",
    "        moving_avg = np.concatenate((bmoving_avg,rmoving_avg))\n",
    "        errors = np.concatenate((berrors,rerrors))\n",
    "        \n",
    "        #print(errors,padded_flux,moving_avg)      \n",
    "        #then do Gaussian\n",
    "        \n",
    "        rv,rv_err,t = Combined_Voigt(xwav,padded_flux,moving_avg,errors,b_lines,time)\n",
    "        #now need to add to directories of wavelengths, flux_results, best_fits,snrs, line depths\n",
    "        if np.isnan(rv): #or np.isnan(rv_corr):\n",
    "            print(\"Fit could not be calculated\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            #rv_corrs.append((rv_corr))\n",
    "            rvs.append((rv)) #-rv_corr\n",
    "            rverrs.append((rv_err )) \n",
    "            time_strings.append((time))\n",
    "            print(\"The fit has been calculated successfully\")\n",
    "    else:\n",
    "        print(\"This is a file for \" + str(bOBJECT) + \" instead of \" + str(star))\n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining data points that are the same times\n",
    "t_list = Time(time_strings, scale='utc')\n",
    "times = t_list.datetime \n",
    "t0 = times[0]\n",
    "tdays = [(dt - t0).days for dt in times]\n",
    "mean = np.nanmean(rvs)\n",
    "print(mean)\n",
    "\n",
    "\n",
    "tarray = np.array(tdays)\n",
    "rvarray = np.array(rvs)\n",
    "rverrsarray = np.array(rverrs)\n",
    "#rvcorrsarray = np.array(rv_corrs)\n",
    "#rverrsarray = np.full(len(tarray),2)\n",
    "\n",
    "filtered_indices = np.where((rverrsarray < 20) & ( rvarray > 30 ) & ( rvarray < 45 ))[0] #& (rvarray > 10)\n",
    "filtered_rvarray = rvarray[filtered_indices]\n",
    "filtered_tarray = tarray[filtered_indices]\n",
    "filtered_rverrsarray = rverrsarray[filtered_indices]\n",
    "#filtered_rvcorrsarray = rvcorrsarray[filtered_indices]\n",
    "\n",
    "\n",
    "\n",
    "unique_times = np.unique(filtered_tarray)\n",
    "averages = np.array([[t, np.nanmean(filtered_rvarray[filtered_tarray == t]), np.nanmean(filtered_rverrsarray[filtered_tarray == t])] for t in unique_times])\n",
    "#averages = averages[np.isnan(averages[:,1])]\n",
    "\n",
    "print(averages)#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gaussian fits runfile for the red channel\n",
    "#using code from above and iterating over the filenames\n",
    "#rvs = []\n",
    "#time_strings = []\n",
    "#rverrs = []\n",
    "#num_files = len(b_files)\n",
    "\n",
    "n = -1\n",
    "plot_dir = \"/data/wdplanetary/omri/Output/Gaussianline/red/\"\n",
    "for j in r_files:\n",
    "    n=n+1\n",
    "    wav, flux ,time = Get_Wavelength_Flux_File(j)\n",
    "#then call data processing\n",
    "    xwav,padded_flux,moving_avg = process_data_gaussian(wav,flux)\n",
    "    errors = calculate_error(padded_flux)\n",
    "#then do gaussian\n",
    "    rv,rv_err = Gaussian(xwav,padded_flux,moving_avg,errors,r_lines,time,n,plot_dir)\n",
    "    rvs.append((rv))\n",
    "    rverrs.append((rv_err))\n",
    "    time_strings.append((time))\n",
    "    print(time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "output_dir = \"/data/wdplanetary/omri/Output/bigspectraplots/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for line in b_lines:\n",
    "    plot_dir = \"/data/wdplanetary/omri/Output/Gaussianline/red/\"\n",
    "    sub_dir = f\"{line[0]}/\"\n",
    "    sub_dir_path = os.path.join(plot_dir, sub_dir)\n",
    "\n",
    "    # Get list of plot files in the directory\n",
    "    if os.path.isdir(sub_dir_path):\n",
    "        plot_files = [file for file in os.listdir(sub_dir_path) if file.endswith('.png')]\n",
    "    else:\n",
    "        print(\"Subdirectory does not exist\")\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Initialize combined image dimensions\n",
    "    max_width = max_height = 0\n",
    "    num_images = len(plot_files)\n",
    "    print(num_images)\n",
    "    if num_images == 0:\n",
    "        continue\n",
    "    else:\n",
    "        num_rows = int(num_images ** 0.5)\n",
    "        num_cols = (num_images + num_rows - 1) // num_rows  # Round up\n",
    "\n",
    "        # Calculate the maximum width and height among all images\n",
    "        for plot_file in plot_files:\n",
    "            image = Image.open(os.path.join(sub_dir_path, plot_file))\n",
    "            width, height = image.size\n",
    "            max_width = max(max_width, width)\n",
    "            max_height = max(max_height, height)\n",
    "\n",
    "        # Create the combined image\n",
    "        combined_width = max_width * num_cols\n",
    "        combined_height = max_height * num_rows\n",
    "        combined_image = Image.new('RGB', (combined_width, combined_height), color='white')\n",
    "\n",
    "        # Paste each image onto the combined image\n",
    "        current_row = current_col = 0\n",
    "        for plot_file in plot_files:\n",
    "            image = Image.open(os.path.join(sub_dir_path, plot_file))\n",
    "            combined_image.paste(image, (current_col * max_width, current_row * max_height))\n",
    "            current_col += 1\n",
    "            if current_col == num_cols:\n",
    "                current_col = 0\n",
    "                current_row += 1\n",
    "\n",
    "        # Save the combined image\n",
    "        combined_image.save(os.path.join(output_dir, f\"{line[0]}_combined_fits.pdf\"))\n",
    "\n",
    "# Display the combined image if needed\n",
    "png_files = [file for file in os.listdir(output_dir) if file.endswith('.pdf')]\n",
    "# Loop through each PNG file and display it\n",
    "for file in png_files:\n",
    "    # Construct the full path to the PNG file\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    # Open the PNG file using PIL\n",
    "    #image = Image.open(file_path)\n",
    "    # Display the image\n",
    "    #display(image)\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/wdplanetary/omri/Output/bigspectraplots/H_4860_2_combined_fits.pdf'\n",
    "image_file = 'H_4860_2_combined_fits.png'\n",
    "\n",
    "# Convert the PDF file to an image\n",
    "images = convert_from_path(file_path)\n",
    "\n",
    "# Save the first page of the PDF as a PNG image\n",
    "images[0].save(image_file, 'PNG')\n",
    "\n",
    "# Open the PNG image with PIL\n",
    "image = Image.open(image_file)\n",
    "display(image)\n",
    "\n",
    "# Do something with the image object\n",
    "# ...\n",
    "\n",
    "image.close()  # Remove the temporary PNG file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_data = {\n",
    "    'wav': sum(wavlist),  # flatten the list of lists\n",
    "    'data': sum(datalist),\n",
    "    'result': sum(resultlist),\n",
    "    'error': sum(errorlist),\n",
    "    'time': [str(time) for time in timelist], \n",
    "    'rv': sum(rvlist)\n",
    "}\n",
    "df = pd.DataFrame(plot_data)\n",
    "\n",
    "for i in range(num_plots):\n",
    "    epoch_data = df[df['time'] == timelist[i]]  # Filter data for the current epoch\n",
    "    p = (ggplot(epoch_data, aes(x='wav', y='data')) +\n",
    "         geom_line(color='black', size=0.5) +\n",
    "         geom_line(aes(y='result'), color='red') +\n",
    "         geom_ribbon(aes(ymin='result - error', ymax='result + error'), fill='gray', alpha=0.3) +\n",
    "         labs(title=f\"{timelist[i].year}-{timelist[i].month}-{timelist[i].day} : {line}\", x='Wavelength', y='Data') +\n",
    "         theme_minimal() +\n",
    "         theme(text=element_text(size=12))\n",
    "        )\n",
    "    p.save(filename=f'/data/wdplanetary/omri/Output/Gaussianfits_mgII_4481_epoch_{i}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data = pd.DataFrame({'Time':times,'RV':rvs, \"RV errors\":rverrs})\n",
    "#d = data.groupby('Time').mean().reset_index()\n",
    "#f = d[(d['RV'] >= 30) & (d['RV'] <= 50)]\n",
    "#times = [d['Time']]\n",
    "#rvs = [d[\"RV\"]]\n",
    "#rverrs = [d[\"RV errors\"]]\n",
    "#print((times))\n",
    "#print(d['Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Radial Velocity variations with time plot\n",
    "mean = np.nanmean(averages[:, 1])\n",
    "t = averages[:, 0]\n",
    "v = averages[:, 1] - mean\n",
    "#errs = np.full(len(v),0.5)\n",
    "errs = averages[:, 2] \n",
    "\n",
    "variance = np.var(v) + np.mean(errs)**2\n",
    "stdev = np.sqrt(variance)\n",
    "\n",
    "frequencies = np.linspace(0.001,50,30000)\n",
    "#frequencies = np.linspace(1/1000, 50, num=400000)\n",
    "\n",
    "power = LombScargle(t, v,errs).power(frequencies)\n",
    "\n",
    "#save data\n",
    "max_length = max(len(t), len(v), len(errs), len(frequencies), len(power))\n",
    "\n",
    "# Open a file for writing\n",
    "os.makedirs(\"/data/wdplanetary/omri/Output/resultfiles/MIKE_Voigt/\", exist_ok=True)\n",
    "file_name = \"/data/wdplanetary/omri/Output/resultfiles/MIKE_Voigt/all_abs_lines_snr_filter_13_depth_3.txt\"\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    # Write header row\n",
    "    txtfile.write(\"Time\\tDeltaRV\\tErrors\\tfrequencies\\tpower\\n\")\n",
    "\n",
    "    # Write data rows\n",
    "    for i in range(max_length):\n",
    "        row = [t[i] if i < len(t) else \"\", \n",
    "               v[i] if i < len(v) else \"\", \n",
    "               errs[i] if i < len(errs) else \"\", \n",
    "               mean if i < 1 else \"\",\n",
    "               stdev if i < 1 else \"\",\n",
    "               frequencies[i] if i < len(frequencies) else \"\", \n",
    "               power[i] if i < len(power) else \"\"]\n",
    "        txtfile.write(\"\\t\".join(map(str, row)) + \"\\n\")\n",
    "\n",
    "#data = pd.DataFrame({\"Time\":t,\"DeltaRV\": v, \"Errors\": errs, \"STDEV\": [stdev], \"RVMean\": [mean], \"frequencies\": [frequencies], \"power\": [power]}\n",
    "#data.to_csv(file_name, sep='\\t', index=False)\n",
    "\n",
    "fig, (ax_t, ax_w) = plt.subplots(2, 1, facecolor=\"white\", figsize=(12,14), constrained_layout=True)\n",
    "\n",
    "ax_t.errorbar(t, v, yerr=errs,fmt = '.k',lw = 1.5)\n",
    "ax_t.fill_between(t, -stdev, stdev, color='gray',label = \"1 sigma\", alpha=0.4)\n",
    "ax_t.fill_between(t, -3* stdev, 3* stdev, color='gray',label = \"3 sigma\", alpha=0.2)\n",
    "ax_t.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax_t.legend(fontsize = 18)\n",
    "ax_t.text(200, 1.2, f\"RV mean = {mean:.3g} km/s\",fontsize=18)\n",
    "ax_t.text(200, 1, f'sigma = {stdev:.3g} km/s',fontsize=18)\n",
    "ax_t.set_xlabel(\"T - Days\",fontsize=18)\n",
    "ax_t.set_ylabel(\"ΔRV - km/s\",fontsize=18)\n",
    "ax_t.tick_params(axis='x', labelsize=16)\n",
    "ax_t.tick_params(axis='y', labelsize=16)\n",
    "#plt.title(\"WD1929+012 Radial Velocity variations using Voigt fits - SALT data\")\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(-20,20)\n",
    "#plt.xlim(0,112)\n",
    "\n",
    "ax_w.plot(frequencies, power/np.max(np.abs(power)))\n",
    "ax_w.set_xlabel('Angular frequency [rad/days]',fontsize=18)\n",
    "ax_w.set_ylabel('Normalized amplitude',fontsize=18)\n",
    "ax_w.tick_params(axis='x', labelsize=16)\n",
    "ax_w.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "os.makedirs(\"/data/wdplanetary/omri/Output/DeltaRV_files/MIKE/Voigt_profiles/all_abs_lines/\", exist_ok=True)\n",
    "#plt.savefig(\"/data/wdplanetary/omri/Output/DeltaRV_files/MIKE/Voigt_profiles/all_abs_lines/snr_filter_13_depth_3.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from voigt fitting \n",
    "mg4481 = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/voigt_fitting_mg4481.txt\",delimiter=',')\n",
    "si6347 = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/voigt_fitting_si6347.txt\", delimiter=',')\n",
    "si6371 = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/voigt_fitting_si6371.txt\", delimiter=',')\n",
    "H4860 = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/voigt_fitting_H4860.txt\", delimiter=',')\n",
    "all_lines = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/voigt_fitting_all_lines.txt\", delimiter=',')\n",
    "\n",
    "voigt_data = np.array((mg4481[:, 1], si6347[:, 1], si6371[:, 1], H4860[:, 1],all_lines[:, 1]))\n",
    "print(voigt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "\n",
    "\n",
    "def set_axis_style(ax, labels):\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "    ax.set_xlabel('Lines used for Voigt fit')\n",
    "    ax.set_ylabel('Radial Velocity (km/s)')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6), sharey=True)\n",
    "\n",
    "#ax.set_title('Violin plot')\n",
    "parts = ax.violinplot(\n",
    "        voigt_data, showmeans=False, showmedians=False,\n",
    "        showextrema=False)\n",
    "\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('#D43F3A')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(1)\n",
    "\n",
    "# Initialize lists to store quartiles and adjacent values for each column\n",
    "all_quartile1 = []\n",
    "all_medians = []\n",
    "all_quartile3 = []\n",
    "all_whiskers_min = []\n",
    "all_whiskers_max = []\n",
    "\n",
    "# Iterate over each column and calculate quartiles and adjacent values\n",
    "for column in voigt_data:\n",
    "    q1, median, q3 = np.percentile(column, [25, 50, 75])\n",
    "    all_quartile1.append(q1)\n",
    "    all_medians.append(median)\n",
    "    all_quartile3.append(q3)\n",
    "    whiskers = adjacent_values(np.sort(column), q1, q3)\n",
    "    all_whiskers_min.append(whiskers[0])\n",
    "    all_whiskers_max.append(whiskers[1])\n",
    "\n",
    "inds = np.arange(1, len(all_medians) + 1)\n",
    "ax.scatter(inds, all_medians, marker='o', color='white', s=30, zorder=3)\n",
    "ax.vlines(inds, all_quartile1, all_quartile3, color='k', linestyle='-', lw=5)\n",
    "ax.vlines(inds, all_whiskers_min, all_whiskers_max, color='k', linestyle='-', lw=1)\n",
    "\n",
    "# set style for the axes\n",
    "labels = ['Mg 4481', 'Si 6347', 'Si 6371', \"H 4860\", 'All lines']\n",
    "set_axis_style(ax, labels)\n",
    "\n",
    "#plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/violinplots/voigt_comparisons.pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg4481 = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/voigt_fitting_mg4481.txt\",delimiter=',')\n",
    "crosscorr_bluespec = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/crosscorrRV_bluespec_results.txt\",delimiter=',')\n",
    "crosscorr_model = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/crosscorrRV_model_mg4481_results.txt\",delimiter=',')\n",
    "data = np.array((mg4481[:, 1] - np.mean(mg4481[:, 1]), crosscorr_bluespec[:, 1], crosscorr_model[:, 1] - np.mean(crosscorr_model[:, 1])))\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "\n",
    "\n",
    "def set_axis_style(ax, labels):\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "    ax.set_ylabel('Radial Velocity variation (km/s)')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 6), sharey=True)\n",
    "\n",
    "#ax.set_title('Violin plot')\n",
    "parts = ax.violinplot(\n",
    "        data, showmeans=False, showmedians=False,\n",
    "        showextrema=False)\n",
    "\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('#D43F3A')\n",
    "    pc.set_edgecolor('black')\n",
    "    pc.set_alpha(1)\n",
    "\n",
    "# Initialize lists to store quartiles and adjacent values for each column\n",
    "all_quartile1 = []\n",
    "all_medians = []\n",
    "all_quartile3 = []\n",
    "all_whiskers_min = []\n",
    "all_whiskers_max = []\n",
    "\n",
    "# Iterate over each column and calculate quartiles and adjacent values\n",
    "for column in data:\n",
    "    q1, median, q3 = np.percentile(column, [25, 50, 75])\n",
    "    all_quartile1.append(q1)\n",
    "    all_medians.append(median)\n",
    "    all_quartile3.append(q3)\n",
    "    whiskers = adjacent_values(np.sort(column), q1, q3)\n",
    "    all_whiskers_min.append(whiskers[0])\n",
    "    all_whiskers_max.append(whiskers[1])\n",
    "\n",
    "inds = np.arange(1, len(all_medians) + 1)\n",
    "ax.scatter(inds, all_medians, marker='o', color='white', s=30, zorder=3)\n",
    "ax.vlines(inds, all_quartile1, all_quartile3, color='k', linestyle='-', lw=5)\n",
    "ax.vlines(inds, all_whiskers_min, all_whiskers_max, color='k', linestyle='-', lw=1)\n",
    "\n",
    "# set style for the axes\n",
    "labels = ['Voigt fit on Mg 4481 line', 'Self Cross-Correlation', \"Model Cross-Correlation\"]\n",
    "set_axis_style(ax, labels)\n",
    "\n",
    "plt.savefig(\"/data/wdplanetary/omri/Output/violinplots/voigt_vs_crosscorr.pdf\")\n",
    "#plt.subplots_adjust(bottom=0.15, wspace=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin plot for each line:\n",
    "y = filtered_rvarray - np.mean(filtered_rvarray)\n",
    "plt.violinplot(y, showmeans=False)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gaussian fitting plot\n",
    "\n",
    "plt.errorbar(times, rvs, yerr=rverrs,fmt = '.k')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Radial Velocity km/s\")\n",
    "plt.title(\"WD1929+012 Radial Velocity variations using Gaussian fits\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.tight_layout()\n",
    "#plt.ylim(30,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now attempt to make a periodogram of these results\n",
    "\n",
    "data = np.loadtxt(\"/data/wdplanetary/omri/Output/resultfiles/mike_voigt_fitting_ca_mg.txt\",delimiter=',')\n",
    "v = data[:, 1] - np.mean(data[:, 1])\n",
    "t = data[:, 0]\n",
    "errs = data[:, 2]\n",
    "print(data)\n",
    "#errs = np.full(len(v),0.5)\n",
    "#frequency,power = LombScargle(tdays, v, errs).autopower()\n",
    "frequencies = np.linspace(0.001,10,10000)\n",
    "#frequencies = np.linspace(1/1000, 50, num=400000)\n",
    "\n",
    "power = LombScargle(t, v,errs).power(frequencies)\n",
    "\n",
    "fig, (ax_t, ax_w) = plt.subplots(2, 1, constrained_layout=True)\n",
    "ax_t.errorbar(t, v, yerr=errs, fmt='b+', label='Data with Errors')\n",
    "ax_t.set_xlabel('Time [days]')\n",
    "ax_t.legend()\n",
    "\n",
    "ax_w.plot(frequencies, power/np.max(power))\n",
    "ax_w.set_xlabel('Angular frequency [rad/days]')\n",
    "ax_w.set_ylabel('Normalized amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model data periodogram\n",
    "rand = np.random.default_rng(402)\n",
    "t = 1000 * rand.random(30)\n",
    "dy = 0.3\n",
    "y = np.sin(0.2* 2 * np.pi * t) + dy * rand.standard_normal(30)\n",
    "\n",
    "frequency = np.linspace(0.1, 1, 10000)\n",
    "power = LombScargle(t, y, dy).power(frequency)\n",
    "\n",
    "fig, (ax_t, ax_w) = plt.subplots(2, 1, constrained_layout=True)\n",
    "ax_t.errorbar(t, y, yerr=dy, fmt='b+', label='Data with Errors')\n",
    "ax_t.set_xlabel('Time [days]')\n",
    "ax_t.legend()\n",
    "\n",
    "ax_w.plot(frequency, power)\n",
    "ax_w.set_xlabel('Angular frequency [rad/days]')\n",
    "ax_w.set_ylabel('Normalized amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding wavelength resolution of SALT\n",
    "resolving_power = 13000\n",
    "wav = 4800\n",
    "deltawav = (wav)/resolving_power\n",
    "v = (deltawav/(wav)) * 299792\n",
    "print(\"This resolving power leads to a minimum resolved radial velocity change of \" + str(v))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
